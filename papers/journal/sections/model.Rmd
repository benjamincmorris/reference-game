# Model: Communication as planning

In order to model when people should speak, point, or teach, we begin from the problem of what goal people are trying to solve [@marr1982]. Following a long history of work in philosophy of language, we take the goal of communication to be causing an action in the world by transmitting some piece of information to one's conversational partner [e.g., @wittgenstein1953; @austin1975]. If people are near-optimal communicators, they should choose communicative signals that maximize the probability of being understood while minimizing the cost of producing the signal [@grice1975; @clark1996]. In the special case of reference, solving this problem amounts to producing the least costly signal that correctly specifies one's intended target referent in such a way that one's conversational partner can select it from the set of alternative referents.

Recently, @frank2012 developed the Rational Speech Act framework-- a formal instantiation of these ideas. In this model, speakers choose from a set of potential referential expressions in accordance to a utility function that maximizes the probability that a listener will correctly infer their intended meaning while minimizing the number of words produced. This framework has found successful application in a variety of linguistic applications such as scalar implicature, conventional pact formation, and production and interpretation of hyperbole [@goodman2016; see also related work from @franke2013]. These models leverage recursive reasoning--speakers reasoning about listeners who are reasoning about speakers--in order to capture cases in which the literal meaning and the intended meaning of sentences diverge.

To date, this framework has been applied primarily in cases where both communicative partners share the same linguistic repertoire, and thus communicators know their probability of communicating successfully having chosen a particular signal. This is a reasonable assumption for pairs of adults in contexts with shared common ground. But what if partners do not share the same linguistic repertoire, and in fact do not know the places where their knowledge diverges? In this case, communicators must solve two problems jointly: (1) Figure out what their communicative partner knows, and (2) produce the best communicative signal they can given their estimates of their partner's knowledge. If communicative partners interact repeatedly, these problems become deeply intertwined: Communicators can learn about each-other's knowledge by observing whether their attempts to communicate succeed. For instance, if a communicator produces a word that they believe identifies their intended referent, but their partner fails to select that referent, the communicator can infer that their partner must not share their understanding of that word. They might then choose not to use language to refer to this object in the future, but choose to point to it instead.

Critically, communicators can also change each-other's knowledge. When a communicator both points to an object and produces a linguistic label, they are in effect teaching their partner the word that they use to refer to this object. While this this behavior is costly in the moment, and no more referentially effective than pointing alone, it can lead to more efficient communication in the future--instead of pointing to this referent forever more, communicators can now use the linguistic label they both know they share. This behavior naturally emerges from a conception of communication as planning: Communicators' goal is to choose a communicative signal today that will lead to efficient communication not just in the present moment, but in future communications as well. If they are likely to need to refer to this object frequently, it is worth it to be inefficient in this one exchange in order to be more efficient future. In this way, pedagogically supportive behavior can emerge naturally from a model with no separate pedagogical goal. In the following section, we present a formal instantiation of this intuitive description of communication as planning and show that it accounts for the behavior we observed in our experiments.

Alternatively, pedagogically-supportive input could emerge from an explicit pedagogical goal. @shafto2014 have developed an framework of rational pedagogy built on the same recursive reasoning principles as in the Rational Speech Act Framework: Teachers aim to teach a concept by choosing a set of examples that would maximize learning for students who reason about the teachers choices as attempting to maximize their learning. @rafferty2016 et al. expanded this framework to sequential teaching, in which teachers use students in order to infer what they have learned and choose the subsequent example. In this case, teaching can be seen as a kind of planning where teachers should choose a series of examples that will maximize students learning but can change plans if an example they thought would be too hard turns out too easy--or vice-versa. In the case of our reference game, this model is indistinguishable from a communicator who seeks to maximize communicative success but is indifferent to communicative cost. A cost-indifferent model makes poor predictions about parents' behavior in our corpus, and also adults' behavior in our experiments, but we return to it in the subsequent section to consider how differences in parents' goals and differences in children's learning contribute to changes in the rate of language acquisition.

## Formal Model

```{r model-data}
all_data_model <- read_csv(here("model_output/predicted_data.csv")) %>%
  mutate(appearanceNumeric = as.numeric(as.factor(appearance)),
         partnersExposure = factor(partnersExposureDescriptive, 
                                   levels = c("None", "Same", "Perfect")),
         partnersExposureNumeric = as.numeric(partnersExposure) - 1,
         method = factor(method, levels = c("click", "label", "label_click"),
                         labels = MODALITY_LABELS),
         action = factor(action, levels = c("point", "speak", "teach"),
                         labels = MODALITY_LABELS),
         condition = factor(condition, levels = c("100_30", "80_50"),
                            labels = CONDITION_LABELS))
```

We take as inspiration the idea that communication is a kind of action--e.g., talking is a speech act [@austin1975]. Consequently, we can understand the choice of *which communicative act* a speaker should take as a question of which act would maximize their utility: achieving successful communication while minimizing their cost [@frank2012]. In this game, speakers can take three actions: talking, pointing, or teaching. The Utilities ($U$) are given directly by the rules of this game. Because communication is a repeated game, people should take actions that maximize their Expected Utility ($EU$)  not just for the current round, but for all future communicative acts with the same conversational partner. We can think of communication, then as a case of recursive planning. However, people do not have perfect knowledge of each-other's vocabularies ($v$). Instead, they only have uncertain beliefs ($b$) about these vocabularies that combine their expectations about what kinds of words people with as much linguistic experience as their partner are likely to know with their observations of their partner's behavior in past communicative interactions. This makes communication a kind of planning under uncertainty well modeled as a Partially Observable Markov Decision Process [POMDP, @kaelbling1998].

Optimal planning in a Partially Observable Markov Decision Process involves a cycle of three phases: (1) Plan, (2) Act, and (3) Update beliefs. We describe those in turn and finally define how people form initial beliefs about their partner's language now.

### Plan 

When people plan, they compute the expected utility of each possible action ($a$) by combining the expected utility of that action now with the Discounted Expected Utility they will get in all future actions. The amount of discounting ($\gamma$) reflects how much people care about success now compared to success in the future. Because utilities depend on the communicative partner's vocabulary, people should integrate over all possible vocabularies in proportion to the probability that their belief assigns to that vocabulary ($\mathbb{E}_{v \sim b}$).
$$
EU\left[a\right | b] = \mathbb{E}_{v \sim b} \left(U(a|v) + \gamma \,\mathbb{E}_{v',o',a'} \,\left( EU\left[a' | b'\right]\right)\right)
$$

### Act 

Next, people take an action as a function of its expected utility. Following other models in the Rational Speech Act framework, we use the Luce Choice Axiom, in which each choice is taken in probability proportional to its exponentiated utility [@frank2012; @luce1959]. This choice rule has a single parameter $\alpha$ that controls the noise in this choice--as $\alpha$ approaches 0, choice is random and as $\alpha$ approaches infinity, choice is optimal.
$$
P\left(a|b\right) \propto \alpha \, e^{EU[a|b]}
$$

### Update beliefs

After taking an action, people observe ($o$) their partner's choice--sometimes they correctly select the intended object, and sometimes they do not. People then update their beliefs about the partner's vocabulary based on this observation. For simplicity, we assume that people think their partner should always select the correct target if they point to it, or if they teach, and similarly should always select the correct target if they produce its label and the label is in their partner's vocabulary. Otherwise, they assume that their partner will select the wrong object. People could of course have more complex inferential rules, e.g., assuming that if their partner does know a word they will choose among the set of objects whose labels they do not know [mutual exclusivity, @markman1988]. Empirically, however, our simple model appears to accord well with people's behavior.
$$
b'(v') \propto P\left(o|v',a\right) \sum_{v \in V}P\left(v'|v,a\right)b\left(v\right)
$$

The critical feature of a repeated communication game is that people can change their partner’s vocabulary. In teaching, people pay the cost of both talking and pointing together, but can leverage their partner’s new knowledge on future trials. Note here that teaching has an upfront cost and the only benefit to be gained comes from using less costly communication modes later. There is no pedagogical goal--the model treats speakers as selfish agents aiming to maximize their own utilities by communicating successfully. We assume for simplicity that teaching is always successful in this very short game, that communicative partners do not forget words once they have learned them, and that no learning happens by inference from mutual exclusivity.

$$
P\left(v'|v,a\right)= \begin{cases} 
1 & \text{if } v_{w} \in v \& v' \;\; | \;\; a = \text{point+talk}\\ 
0 & otherwise\end{cases}
$$

### Initial Beliefs

The final detail is to specify how people estimate their partner's learning rate ($p$) and initial vocabulary ($v$). We propose that people begin by estimating their own learning rate by reasoning about the words they learned at the start of the task: Their learning rate ($p$) is the rate that maximizes the probability of them having learned their initial vocabularies from the trials they observed. People can then expect their partner to have a similar $p$ [per the "like me" hypothesis, @meltzoff2005]. Having an estimate of their partner's $p$, they can estimate their vocabulary by simulating their learning from the amount of prior exposure to language their partner had before the game. In our experiments, we explicitly manipulated this expectation by telling participants how much exposure their partner had relative to their own exposure. 

```{r model-fit, fig.height = 3, fig.width = 4.25, set.cap.width=T, num.cols.cap=1, fig.cap = "Speaker communicative method choice as a function of exposure and the utility manipulation."}
###wrangling data for plots by exposure
model_prop_methods_exposures <- all_data_model %>%
  group_by(condition, ldf_num, partnersExposure, exposures, action) %>%
  summarise(n = n()) %>%
  mutate(n = n / sum(n)) %>%
  ungroup() %>%
  mutate(action = as.factor(action)) %>%
  tidyr::complete(nesting(condition, ldf_num, partnersExposure), 
                  exposures, action, fill = list(n = 0)) %>%
  group_by(condition, partnersExposure, action) %>%
  tidyboot_mean(n)

label_data <- tibble(partnersExposure = c(2.6, 2.6, 2.6),
                     empirical_stat = c(.45, .6, .15),
                     action = MODALITY_LABELS) %>%
  mutate(condition = first(CONDITION_LABELS))


model_plot1 <- ggplot(model_prop_methods_exposures,
       aes(x = partnersExposure, y = empirical_stat, color = action, 
           label = action, group = interaction(action, condition))) +
  facet_wrap(~ condition) +
  geom_line(size=.9, position = position_dodge(.25)) +
  geom_pointrange(aes(ymax = ci_upper, ymin = ci_lower),
                  position = position_dodge(.25)) +
  labs(y="Proportion of Trials", x = "Exposure Rate During Training") +
  scale_color_manual(values=c(modality.colors), name = "Method") +
  geom_text(data = label_data)

model_plot1
```

## Method

We implemented the planning model using the WebPPL-- a programming language designed for specifying probabilistic models [@goodman2014]. To derive predictions from the model, we exposed it to the same trial-by-trial stimuli as the participants in our experiment, and used the probabilistic equations defined above to determine the likelihood of choosing each behavior (e.g., "speak", "point", or "teach") on every trial. Separate predictions were made for each trial for each participant on the basis of all of the information available to each participant at that point in time (e.g., how many words they had learned, their partner's observed behavior previously, etc).

The model's behavior is contingent on two parameters--discounting ($\gamma$), and it's rationality ($\alpha$). In order to determine the values of these parameters that best characterize human participants, we used Bayesian inference to estimate the posterior means of both. Using estimates rather than the maximum likelihood estimates naturally penalizes the models for their ability to predict patterns of data that were not observed, applying a kind of Bayesian Ockham's razor [@mackay1992]. Because of we found substantial variability in the best parameter estimates across individual participants, we estimated parameters hierarchically, with group-level parameters forming the priors for individual participants' parameters. This hierarchical estimation process achieves the same partial pooling as as subject-level random effects in mixed-effects models, giving estimates of the group-level parameters [@gelman2006]. Details of the estimation procedure can be found in the Supplemental Materials.

## Model Results

```{r model-parameters}
hyper_parameters <- read_csv(here("model_output/hyper_parameters.csv"))

hyper_means <- hyper_parameters %>%
  filter(Parameter != "score") %>%
  group_by(Parameter) %>%
  summarise(mean = mean(value), ci_lower = quantile(value, .025), 
            ci_upper = quantile(value, .975))
```

In line with previous work on rational speech act models, and decision making, we expected rationality ($\alpha$) to be around 1 or 2 [@frank2012, @frank2014]. We estimated the posterior mean rationality ($\alpha$) to be `r pull(filter(hyper_means, Parameter == "hyperAlpha"), mean)` with 95% credible intervals of [`r pull(filter(hyper_means, Parameter == "hyperAlpha"), ci_lower)`, `r pull(filter(hyper_means, Parameter == "hyperAlpha"), ci_upper)`]. We did not have strong expectations for the value of the discounting parameter ($\gamma$), but estimated it to be `r pull(filter(hyper_means, Parameter == "hyperDiscount"), ci_lower)` [`r pull(filter(hyper_means, Parameter == "hyperDiscount"), ci_lower)`, `r pull(filter(hyper_means, Parameter== "hyperDiscount"), ci_upper)`], suggesting that on average participants weighed the next occurrence of a referent as slightly less than half as important as the current occurrence.

<!-- The fit between our model’s predictions and our empirical data from our reference game study on Amazon Turk can be seen in Figure \ref{fig:model_fit}. The model outputs trial-level action predictions (e.g., “speak”) for every speaker in our empirical data. These model outputs were aggregated across the same factors as the empirical data: modality, appearance, partner’s exposure, and utility condition. We see a significant correlation of our model predictions and our empirical data (*r = `r #model_corr`, p<0.0001*). Our model provides a strong fit for these data, supporting our conclusion that richly-structured language input could emerge from in-the-moment pressure to communicate, without a goal to teach. -->

```{r model_fit, fig.env = "figure", fig.pos = "H", fig.height=3,  fig.width=4, fig.cap = "Fit between model predictions and empirical data.", eval = FALSE}

#plot of model predictions vs empirical game data
model_fit %>%
  ggplot(aes(x=mean_pred_plot, y=mean_emp, color=method,
             shape=as.factor(partnersExposure),group=appearance,
             alpha=condition,
             label = appearance)) +
  geom_point(aes(size=as.factor(appearance))) +
  geom_abline(intercept =0, slope=1) +
  geom_linerange(aes(ymax = mean_emp + se_emp,
                     ymin = mean_emp - se_emp),
                 alpha=.4) +
  geom_errorbarh(aes(xmin = mean_pred_plot - se_pred_plot,
                     xmax = mean_pred_plot + se_pred_plot),
                 alpha = .4)+
  # geom_text(aes(label=appearance),hjust=0, vjust=0) + 
  # facet_grid(.~condition)+
  coord_cartesian(xlim=c(0,.8), ylim=c(0,.8)) + 
  labs(y="Empirical Proportion of Trials", x="Model Predictions") +
  scale_color_manual(values=modality.colors, name='Modality', labels=c("Gesture", "Speech", "Teach")) +
  scale_shape_manual(values=c(15,17,16),name="Partner's Exposure", labels=c("None", "Same Amount", "Twice as Much")) + 
  scale_alpha_manual(values=c(.4, 1), name="Utility Condition", labels=c("Low Relative Cost", "Higher Relative Cost")) +
  scale_size_manual(values=c(1,2,3), name="Appearance", labels=c("First", "Second", "Third"))+
  theme(legend.direction = "vertical", 
        legend.position = "right",
        legend.key.size = unit(.15, 'lines'),
        legend.title=element_text(size=7),
        legend.text=element_text(size=6),
        legend.spacing = unit(0, "in"),
        legend.box.spacing = unit(0, "in"))
```
