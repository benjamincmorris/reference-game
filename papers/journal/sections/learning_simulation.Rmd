# Consequences for Learning

In the model and experiments above, we asked whether the pressure to communicate successfully with a linguistically-naive partner would lead to pedagogically supportive input. These results confirmed its sufficiency: As long as linguistic communication is less costly than deictic gesture, speakers should be motivated to teach in order to reduce future communicative costs. Further, the strength of this motivation is modulated by predictable factors (speaker's linguistic knowledge, listener's linguistic knowledge, relative cost of speech and gesture, learning rate, etc.), and the strength of this modulation is well predicted by a rational model of planning under uncertainty about a listener's vocabulary.

In this final section, we take up the consequences of communicatively-motivated linguistic input for a child learning language. To do this, we adapt a framework used by @blythe2010 to estimate the learning times for an idealized child learning language under a variety of models of both the child and their parent. We derive estimates by simulating exposure to successive communicative events, and measuring the probability that successful learning happens after each event. The question of how different models of the parent impact the learner can then be formalized as a question of how much more quickly learning happens in the context of one parent model than another.

We consider three parents that have three possible goals:

1. *Communication* - The parent's goal in each interaction with their child is to maximize their communicative success while minimizing their communicative cost. This the model described in the Model section above.

2. *Teaching* - The parents' goal in each interaction is to maximize their child's learning (by teaching on every trial). This goal is equivalent to a model in which the goal is to maximize communicative success without minimizing communicative cost.

3. *Talking* - The parents' goal on each interaction is to refer to their intended referent so that an knowledgeable listener would understand them, without accounting for the child's language knowledge.  This goal is equivalent to minimizing communicative cost without maximizing communicative success.

Under all of these models, we consider the child's goal to be to learn the correct word-referent mappings that explain the parent's communications. If a communicative event is unambiguous--i.e. the parent is teaching--the child is limited only by their ability to encode this correct mapping. If the event is instead ambiguous, the child needs to both encode potential word-object mappings, and to track their statistical consistency. That is, the child needs to solve the cross-situational learning problem [@yu2007]. Across models, we vary both the fidelity of the child's encoding ability, and their capacity for cross-situational learning.

One important point to note is that we are modeling the learning of a single word rather than the entirety of a multi-word lexicon [as in @blythe2010]. Although learning times for each word could be independent, an important feature of many models of word learning is that they are not [@yurovsky2014; @yu2008; @frank2009; although c.f. @mcmurray2007]. Indeed, positive synergies across words are predicted by the majority of models and the impact of these synergies can be quite large under some assumptions about the frequency with which different words are encountered [@reisenauer2013]. We assume independence primarily for pragmatic reasons here--it makes the simulations significantly more tractable (although it is what our experimental participants appear to assume about learners). Nonetheless, it is an important issue for future consideration. Of course, synergies that support learning under a cross-situational scheme must also support learning from communicators and teachers [@markman1988; @frank2009; @yurovsky2013]. Thus, the ordering across conditions should remain unchanged. However, the magnitude of the difference across teacher conditions could potentially increase or decrease.

## Method

In each fo the sections below, we describe the join models of parents' communication and child's learning that predict learning times under each of the three models of parents' goals.

### Teaching. 

Because the teaching model is indifferent to communicative cost, it engages in ostensive labeling (pointing + speaking) on each communicative event. Consequently, learning on each trial occurs with a probability that depends entirely on the learner's learning rate ($P_{k}=p$). Because we assume that the learner does not forget, the probability that a learner has failed to successfully learn after $n$ trials is equal to the probability that they have failed to learn on each of $n$ successive independent trials (The probability of zero successes on $n$ trials of a Binomial random variable with parameter $p$). The probability of learning after $n$ trials is thus: 

$$ P_k(n) = 1 - \left(1-p\right)^{n}  $$

The expected probability of learning after $n$ trials was thus defined analytically and required no simulation. For comparison to the other models, we computed $P_{k}$ for values of $p$ that ranged from $.1$ to $1$ in increments of $.1$.

### Communication.

To test learner under the communication model, we implemented the same model described in the paper above. However, because our interest was in understanding the relationship between parameter values and learning outcomes rather than inferring the parameters that best describe people's behavior, we made a few simplifying assumptions to allow many runs of the model to complete in a more practical amount of time. First, in the full model above, speakers begin by inferring their own learning parameters ($P_{s}$) from their observations of their own learning, and subsequently use their maximum likelihood estimate as a stand-in for their listener's learning parameter ($P_{l}$). Because this estimate will converge to the true value in expectation, we omit these steps and simply stipulate that the speaker correctly estimates the listener's learning parameter. 

Second, unless the speaker knows a priori how many times they will need to refer to a particular referent, the planning process is an infinite recursion. However, each future step in the plan is less impactful than the previous step (because of exponential discounting). This infinite process is in practice well approximated by a relatively small number of recursive steps. In our explorations we found that predictions made from models which planned over 3 future events were indistinguishable from models that planned over four or more, so we simulated 3 steps of recursion[^1]. Finally, to increase the speed of the simulations we re-implemented them in the R programming language. All other aspects of the model were identical.

[^1]: It is an interesting empirical question to determine how the level of depth to which that people plan in this and similar games [see e.g. bounded rationality in @simon1991; resource-rationality in @griffiths2015]. This future work is outside the scope of the current project.

### Talking.

The literature on cross-situational learning is rich with a variety of models that could broadly be considered to be "hypothesis testers." In an eliminative hypothesis testing model, the learner begins with all possible mappings between words and objects and prunes potential mappings when they are inconsistent with the data according to some principle. A maximal version of this model relies on the principle that every time a word is heard its referent must be present, and thus prunes any word-object mappings that do not appear on the current trial. This model converges when only one hypothesis remains and is probably the fastest learner when the assumption it relies on is correct [@smith2011].

A positive hypothesis tester begins with no hypotheses, and on each trial stores one ore more hypotheses that are consistent with the data, or alternatively strengthens one or more hypotheses that it has already stored that are consistent with the new data. A number of such models have appeared in the literature, with different assumptions about (1) how many hypotheses a learner can store, (2) existing hypotheses are strengthened, (3) how existing hypotheses are pruned, and (4) when the model converges [@siskind1996; @smith2011; @stevens2017; @trueswell2013; @yu2012].

Finally, Bayesian models have been proposed that leverage some of the strengths of both of these different kinds of model, both increasing their confidence in hypotheses consistent with the data on a given learning event and decreasing their confidence in hypotheses inconsistent with the event [@frank2009]. 

Because of its more natural alignment with the learning models we use Teaching and Communication simulations, we implemented a positive hypothesis testing model[^2]. In this model, learners begin with no hypotheses and add new ones to their store as they encounter data. Upon first encountering a word and a set of objects, the model encodes up to $h$ hypothesized word-object pairs each with probability $p$. On subsequent trials, the model checks whether any of the existing hypotheses are consistent with the current data, and prunes any that are not. If no current hypotheses are consistent, it adds up to $h$ new hypotheses each with probability $p$. The model has converged when it has pruned all but the one correct hypothesis for the meaning of a word. This model is most similar to the Propose but Verify model proposed in @trueswell2013, with the exception that it allows for multiple hypotheses. Because of the data generating process, storing prior disconfirmed hypotheses [as in @stevens2017], or incrementing hypotheses consistent with some but not all of the data [as in @yu2012] has no impact on learner and so we do not implement it here. We note also that, as described in @yu2012, hypothesis testing models can mimic the behavior of associative learning models given the right parameter settings [@townsend1990]. 

In contrast to the Teaching and Communication simulations, the behavior of the Talking model depends on which particular non-target objects are present on each naming event. We thus began each simulation by generating a corpus of 100 naming events. On each event, we sampled the correct target as well as ($C$-1) competitors from a total set of $M$ objects. We then simulated learning over this set of events as described above, and recorded the first trial on which the learner converged (having only the single correct hypothesized mapping between the target word and target object). We repeated this process 1000 times for each simulated combination of $M = (8, 16, 32, 64, 128)$  total objects, $C = (1, 2 , 4, 8)$ objects per trial, $h = (1, 2, 3, 4)$ concurrent hypotheses, as the learning rate $p$ varied from $.1$ to $1$ in increments of $.1$. 
 
[^2]: Our choice to focus on hypothesis testing rather than other learning frameworks is purely a pragmatic choice--the learning parameter $p$ in this models maps cleanly onto the learning parameter in our other models. We encourage other researchers to adapt the code we have provided to estimate the long-term learning for other models.

## Results

```{r read-sim-data}
teach_props <- read_csv(here("data/teach_props.csv"))
com_props <- read_csv(here("data/com_props_newmodel.csv"))
pbv_props <- read_csv(here("data/pbv_props.csv"))
```

In order to understand how learning rates vary with model parameters, we first discuss the dependence of each of the three test models on it's parameters, and then discuss relationships between the models. In addition the results reported here, we have made the full set of simulated results available in an interactive web application at dyurovsky.shinyapps.io/ref-sims. We encourage readers to fully explore the relationships among the models beyond the summary we provide. 

### Teaching.

Because the Teaching model behaves identically on each trial regardless of the learner, the rate of learning under this model depends entirely on the learner's learning rate $p$. If the learning rate was high (e.g. .8), more than 90% of learners acquired the word after 2 learning instances. If the learning rate was medium, closer to the range we estimated for adult learners (.6), more than 90% of learners acquired the word after only 3 instances. Finally, if the learning rate was very low (.2), the same threshold was reached after 10 instances. Thus, the model is predictably sensitive to learning rate, but even very slow learners are expected to acquire words after a small number of communicative events

### Communication

```{r com-tradeoffs, fig.height = 3, fig.width = 8, set.cap.width=T, num.cols.cap=1, fig.cap = ""}
THRESHOLD <- .9

time_to_90 <- com_props %>%
  group_by(alpha, lambda, p, P, S) %>%
  filter(prob >= THRESHOLD) %>%
  slice(1)

ggplot(time_to_90 %>%
         filter(alpha %in% c(.5,  2),
             #   p %in% c(.2, .5, .8),
                lambda %in% c(.2, .8)),
       aes(x = p, y = trial, color = as.factor(alpha), shape = as.factor(lambda),
           linetype = as.factor(lambda))) + 
  geom_point()+
  scale_color_brewer(palette = "Set1") +
  geom_line(aes(group = interaction(alpha, lambda, P))) +
  facet_grid(S ~ P) +
  theme_few()
```

The communication model's behavior depends on parameters of both the child learner and the parent communicator. In general, parameters of both participants had predictable effects on learning: Children learned faster when they had higher learning rates, when parents were more rational, and when parents gave greater weight to the future. Further, the effects of parents' parameters were more pronounced at the lowest learning rates. However, as the cost of speaking increased relative to pointing, the effects of parents' parameters changed. In particular, highly rational parents who heavily discounted the lead to significantly slower learning. At these parameter settings, the parent becomes very likely to point on any given trial in order to maximize the local utility at the expense of discounted future utility gained from teaching. Figure ~\ref{fig:com-tradeoffs} shows the number of trials required for 90% of learners to acquire the word as a function of parameters in the Communication model.

### Talking.

Finally, when parents spoke on each trial and children had to learn from cross-situational statistics, learning was controlled by the the child's learning rate, the number of hypotheses the child could entertain, the number of objects per event, and to a small extent the total vocabulary size. In general, children learned faster when they had a higher learning rate, and could entertain more hypotheses. Learning was also predictably slower when there were more objects on each event and thus ambiguity was higher. Finally, as the total vocabulary size increased, the rate of learning increased slightly, as it does with human cross-situational learners [@yu2007]. This counter-intuitive outcome occurs because the rate of spurious co-occurrences, in which the target word consistently co-occurs with an object that is not its referent, decreases as the set of potential foils expands. 

```{r talk-tradeoffs, fig.height = 3, fig.width = 8, set.cap.width=T, num.cols.cap=1, fig.cap = ""}
xsit_time_to_90 <- pbv_props %>%
  group_by(nguesses, M, C, p) %>%
  filter(prob >= THRESHOLD) %>%
  slice(1)


ggplot(xsit_time_to_90 %>%
          filter(C %in% c(4,  8),
                 M %in% c(16, 128),
                 nguesses %in% c(1, 4)),
       aes(x = p, y = trial, color = as.factor(C), shape = as.factor(nguesses),
           linetype = as.factor(nguesses))) + 
  geom_point()+
  scale_color_brewer(palette = "Set1") +
  geom_line(aes(group = interaction(nguesses, C, M))) +
  facet_grid(. ~ M) +
  theme_few()
```