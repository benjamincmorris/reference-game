# Experiment

```{r text_labels}
MODALITY_LABELS <- c("point", "speak", "teach")
CONDITION_LABELS <- c("Higher speech efficiency", "Lower speech efficiency")
```

```{r exp-screenshot, fig.width = 4.5, fig.cap = "Screenshot showing the participant view during gameplay."}
img <- png::readPNG(here("papers/journal/figs/exp_screenshot.png"))
grid::grid.raster(img)
```


To study the emergence of pedagogically supportive input from communicative pressure, we developed a simple reference game in which participants would be motivated to communicate successfully. After giving people varying amounts of training on novel names for nine novel objects, we asked them to play a communicative game in which they were given one of the objects as their referential goal, and they were rewarded if their partner successfully selected this referent from among the set of competitors (Figure \ref{fig:exp-screenshot}). Although we told participants that their partner would be played by another human, all partners were actually pre-programmed bots. 

Participants could choose to refer either using the novel labels they had been exposed to, or they could use a deictic gesture (i.e. point) to indicate the referent to their partner. The point was unambiguous, and thus would always succeed. However, in order for language to be effective, the participant and their partner would have to know the correct novel label for the referent. 

In choosing a communicative signal, participants should try to maximize their likelihood of success while minimizing their communicative cost [see e.g. @frank2012]. This cost should be related to the energy required to produce the signal. Though it need not be the case for all gestures and contexts, our task compares simple lexical labeling and unambiguous deictic gestures, which are likely are slower and more effortful to produce [e.g., see @yurovsky2018children]. We thus assumed that pointing should be more costly than speaking. Nonetheless, because we do not have a way of estimating the costs of pointing and speaking from the observational data, we manipulated them experimentally across conditions to understand how they impact peoples' behavior.

Critically, participants were told that they would play this game repeatedly with their partner. In these repeated interactions, participants are able to learn about an interlocutor and potentially influence their learning. Thus, there is a third type of signal participants could send: using both pointing and speech within a single trial to effectively teach the listener through ostensive labeling. This strategy necessitates making inferences about their partner's knowledge state, so we induced knowledge asymmetries between the participant and their partner. To do so, we manipulated how much training they thought their partner had received. 

Our communicative game was designed to reward in-the-moment communication, and thus teaching required the participant pay a high cost upfront. However, rational communicators may understand that if one is accounting for future trials, paying the cost upfront to teach their partner allows them to use a less costly message strategy on subsequent trials (namely, speech). Manipulating the partner's knowledge and the utility of communicative strategies, we aimed to experimentally determine the circumstances under which richly-structured input emerges, without an explicit pedagogical goal.

While our reference game setting has limited ecological validity, this setup allows us to explicitly manipulate the crucial features of the communicative setting (e.g., communicative cost, strategy, and partner knowledge). In this controlled task, we can look for the emergence of structure that parallels the naturalistic input described in our corpus evidence, while also experimentally testing for possible drivers of such structure. This experimental setup further allows us to straightforwardly test and compare key predictions using a formal model to explain participant behavior. 

## Method

In this experiment, participants were recruited to play our reference game via Amazon Mechanical Turk, an online platform that allows workers to complete surveys and short tasks for payment. In this study, all participants were placed in the role of speaker and listener responses were programmed. 

### Transparency and Openness

This sample size, experimental design, and analysis plan were pre-registered at https://osf.io/63qdg. Sample size was determined based on prior pilot experiments. All data, analysis code, and research materials are available at https://osf.io/d9gkw/.

```{r e2-data}
all_data <- read_csv(here("data/1.30_turk_exp.csv"),
                     show_col_types = FALSE) %>%
  select(-`...1`, -X1)

n_Ps <- all_data %>% 
  summarise(n = n_distinct(ldf_num)) %>%
  pull()

n_manipFail <- all_data %>% 
  filter(manipFail == 1) %>% 
  summarise(n = n_distinct(ldf_num)) %>%
  pull()

n_engLabels <- all_data %>% 
  filter(manipFail == 0 & usedEnglishLabels == 1) %>% 
  summarise(n = n_distinct(ldf_num)) %>%
  pull()
```

### Participants

`r n_Ps` participants were recruited though Amazon Mechanical Turk and received \$1 for their participation. Data from `r n_manipFail` participants were excluded from subsequent analysis for failing the critical manipulation check (accurately reporting their partners knowledge prior to gameplay) and a further `r n_engLabels` for producing pseudo-English labels (e.g., "pricklyyone"). The analyses reported here exclude the data from those participants, but all analyses were also conducted without excluding any participants and all patterns hold ($ps < 0.05$).

### Design and Procedure

Participants were told they would be introduced to novel object-label pairs and then asked to play a communication game with a partner wherein they would have to refer to a particular target object. Participants were exposed to nine novel objects, each with a randomly assigned pseudo-word label. We manipulated the exposure rate within-subjects: during training participants saw three of the nine object-label mappings four times, three of them two times, and three of them just one time, yielding a total of 21 training trials. Participants were then given a simple recall task to establish their knowledge of the novel lexicon (pretest). 
  
During gameplay, participants saw the target object in addition to an array of all nine objects. Participants had the option of either directly selecting the target object from the array (pointing)--a higher cost, but unambiguous cue--or typing a label for the object (speech)--a lower cost cue contingent on their partner's knowledge. After sending the message, participants were shown which object their partner selected.  

We also manipulated participants' expectations about their partner's knowledge to explore the role of knowledge asymmetries. Prior to beginning the game, participants were told how much exposure their partner had to the lexicon. Across three between-subjects conditions, participants were told that their partner had either no experience with the lexicon, had the same experience as them, or had twice their experience. As a manipulation check, participants were then asked to report their partner's level of exposure, and were corrected if they answered incorrectly. Participants were then told that they would be asked to refer to each object three times during the game.

Partners were programmed with starting knowledge states initialized according to the partner knowledge condition. Partners with no exposure began the game with knowledge of 0 object-label pairs. Partners with the same exposure as the participant began with knowledge of five object-label pairs (three high-frequency, on mid-frequency, one low-frequency), based on the learning we observed from participants in a pilot experiment. Lastly, partners with twice as much exposure as the participant began with knowledge of all nine object-label pairs. 

To simulate knowledgeable behavior, when the participant typed an object label, the partner was programmed to consult their own knowledge. Messages were evaluated by taking the Levenshtein distance (LD) between the typed label and each possible label in the partner's vocabulary. Partners then selected the candidate with the smallest edit distance (e.g., if a participant typed the message "tomi", the programmed partner would select the referent corresponding to "toma", provided toma was found in its vocabulary). If the participant's message was more than two edits away from all of the words in the partner's vocabulary, the partner selected an object whose label they did not know. If the participant clicked on an object (pointing), the partner was programmed to always select that referent. 

Participants could win up to 100 points per trial if their partner correctly selected the target referent based on their message. If the partner failed to identify the target object, participants received no points. We manipulated the relative utility of the speech cue between subjects across two conditions: Higher Speech Efficiency and Lower Speech Efficiency. In the *Higher Speech Efficiency* condition, participants received 30 points for gesturing and 100 points for labeling, and thus speech had very little cost relative to pointing and participants should be highly incentivized to speak. In the *Lower Speech Efficiency* condition, participants received 50 points for gesturing and 80 points for labeling, and thus gesturing is still costly relative to speech, but the difference between them is smaller lowering the incentivize to speak.

Participants were told about a third type of possible message: using both pointing and speech within a single trial to effectively teach their partner an object-label mapping. This action directly mirrors the ostensive labeling behavior parents produced in the corpus data--it yields an information-rich, pedagogically-supportive learning moment. In order to produce this teaching behavior, participants had to pay the cost of producing both cues (i.e. both pointing and speech). Note that, in all utility conditions, teaching yielded participants 30 points (compared with the much more beneficial strategy of speaking which yielded 100 points or 80 points across our two utility manipulations). Partners were programmed to integrate new taught words into their knowledge of the lexicon, and check those taught labels on subsequent trials when evaluating participants' messages.

Crossing our 2 between-subjects manipulations yielded 6 conditions (2 utility manipulations: Higher Speech Efficiency and Lower Speech Efficiency; and 3 levels of partner’s exposure: None, Same, Twice), with 80 participants in each condition. We expected to find results that mirrored our corpus findings such that rates of teaching would be higher when there was an asymmetry in knowledge where the participant knew more (None manipulation) compared with when there was equal knowledge (Same manipulation) or when the partner was more familiar with the language (Twice manipulation). We expected that participants would also be sensitive to our utility manipulation, such that rates of labeling and teaching would be higher in the Higher Speech Efficiency conditions than the other conditions.

## Results

In each trial, participants could choose one of 3 communicative strategies: pointing, speech, or teaching. We expected participants to flexibly use communicative strategies in response to their relative utilities, their partner's knowledge of the lexicon, and participants' own lexical knowledge. To test our predictions about each communicative behavior (pointing, speech, and teaching), we fit separate logistic mixed effects models for each behavior, reported below. It should be noted that these three behaviors are mutually exhaustive. First, we report how well participants learned our novel lexicon during training.

```{r experiment-learning-descriptives}
filtered_data <- all_data %>%
  filter(toBeDropped != 1) %>%
  mutate(base = logit(1/3),
         appearanceNumeric = as.numeric(as.factor(appearance)),
         partnersExposure = factor(partnersExposure, 
                                   levels = c("None", "Same", "Perfect"),
                                   labels = c("None", "Same", "Twice")),
         partnersExposureNumeric = as.numeric(partnersExposure) - 1,
         method = factor(method, levels = c("click", "label", "label_click"),
                         labels = MODALITY_LABELS),
         condition = factor(condition, levels = c("100_30", "80_50"),
                            labels = CONDITION_LABELS))

learningByExposure <- filtered_data %>% 
  group_by(ldf_num, condition, partnersExposure, realLabel, exposureRate) %>%
  summarize(testCorrect = first(testCorrect))

descriptives <- learningByExposure %>%
  group_by(ldf_num) %>%
  summarize(sum = sum(testCorrect)) %>%
  summarize(meanK = mean(sum), sdK = sd(sum))
```

```{r experiment-experiment-learning-glm}
gm_known <- glmer(testCorrect ~ exposureRate + condition + 
                    (exposureRate | ldf_num) +
                    (1|realLabel),
            data = learningByExposure,
      family=binomial) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = printp(p.value))

make_text_vars(gm_known, "learning_exposure", "exposureRate")
```

```{r experiment-learning-anova}
learning_anova <- learningByExposure %>%
  group_by(condition, partnersExposure, ldf_num) %>%
  summarise(testCorrect = mean(testCorrect)) %>%
  aov(testCorrect ~ partnersExposure * condition, data = .) %>%
  tidy()

condition_learning_esposure_f_num <- learning_anova %>%
  filter(term == "partnersExposure")
```

### Learning

As an initial check of our exposure manipulation, we first fit a logistic regression predicting accuracy at test from a fixed effect of exposure rate and random intercepts and slopes of exposure rate by participant as well as random intercepts by item. We found a reliable effect of exposure rate, indicating that participants were better able to learn items that appeared more frequently in training ($\beta =$ `r learning_exposure_estimate`, $p$ `r learning_exposure_p.value`, see Figure \ref{fig:learning-and-speaking-plot}). On average, participants knew approximately 6 of the 9 words in the lexicon ($M(sd)$ = `r descriptives$meanK` (`r descriptives$sdK`)). An analysis of variance confirmed that learning did not differ systematically across participants by partner's exposure, utility manipulation, or their interaction ($ps$ > 0.05).


```{r learning-and-speaking-plot, fig.height = 3, fig.width = 3, set.cap.width=T, num.cols.cap=1, fig.cap = "Participants' performance on the baseline recall task for the lexicon, as function of amount of exposure during training (grey bars). The red line shows the propotion of trials during gameplay in which participants used the learned labels, excluding teaching behaviors. Error bars show 95\\% confidence intervals computed by non-parametric bootstrapping."}
expanded_data <- filtered_data %>%
  group_by(condition, ldf_num,partnersExposure, exposureRate, method) %>%
  summarise(n = n()) %>%
  mutate(n = n / sum(n)) %>%
  ungroup() %>%
  mutate(method = as.factor(method)) %>%
  complete(nesting(condition, ldf_num, partnersExposure), 
                  exposureRate, method, fill = list(n = 0))

acc_data <- all_data %>%
  filter(toBeDropped != 1) %>%
  distinct(exposureRate, realLabel, ldf_num, testCorrect) %>%
  group_by(exposureRate, ldf_num) %>%
  summarise(testCorrect = mean(testCorrect)) %>%
  tidyboot_mean(testCorrect) %>%
  mutate(measure = "accuracy")

speak_data <- expanded_data %>%
  filter(method == "speak") %>%
  group_by(exposureRate) %>%
  tidyboot_mean(n) %>%
  mutate(measure = "speak")

plot_1_data <- bind_rows(acc_data, speak_data)

ggplot(acc_data, aes(x = as.factor(exposureRate), y = empirical_stat, 
                     ymin = ci_lower, ymax = ci_upper)) +
  geom_col(fill = "grey") + 
  geom_linerange(group = 1) + 
  geom_pointrange(data = speak_data, color = "#e8250b") + 
  geom_line(data = speak_data, color = "#e8250b", group = 1) +
  xlab("Partner's training exposure") +
  ylab("Learning at baseline")

```

### Pointing

```{r modality-plot-function}
modality_colors <- c(speak = "#e8250b", point = "#1f11e0", teach ="#54a832")

modality_plot <- function(df, method_column, label_data) {
  method_column <- enquo(method_column)
  
  prop_methods_exposures <- df %>%
    group_by(condition, ldf_num, partnersExposure, exposureRate, !!method_column) %>%
    summarise(n = n()) %>%
    mutate(n = n / sum(n)) %>%
    ungroup() %>%
    complete(nesting(condition, ldf_num, partnersExposure),
             exposureRate, !!method_column, fill = list(n = 0)) %>%
    group_by(condition, partnersExposure, !!method_column) %>%
    tidyboot_mean(n)
  
  ggplot(prop_methods_exposures,
       aes(x = partnersExposure, y = empirical_stat, color = !!method_column, 
           label = !!method_column, group = interaction(!!method_column, condition))) +
    facet_wrap(~ condition) +
    geom_line(size=.9, position = position_dodge(.25)) +
    geom_pointrange(aes(ymax = ci_upper, ymin = ci_lower),
                  position = position_dodge(.25)) +
    labs(y = "Proportion of trials", x = "Partner's training exposure rate") +
    scale_color_manual(values = modality_colors, name = "Method") +
    geom_text(data = label_data)
}
```

```{r modalities-empirical, fig.height = 3, fig.width = 4.25, set.cap.width=T, num.cols.cap=1, fig.cap = "Participants' communicative method choice as a function of exposure and the utility manipulation. Error bars indicate 95\\% confidence intervals computed by non-parameteric bootstrapping"}

empirical_label_data <- tibble(partnersExposure = c(2.6, 2.6, 2.6),
                     empirical_stat = c(.45, .7, .15),
                     method = MODALITY_LABELS) %>%
  mutate(condition = first(CONDITION_LABELS))

modality_plot(filtered_data, method, empirical_label_data)

```

```{r e1-gesture-model}
glm_point <- glmer((method == 'point') ~ partnersExposureNumeric * exposureRate + 
                    partnersExposureNumeric * appearanceNumeric +
                    condition + 
                    (1| ldf_num) + (1|realLabel),
            data = filtered_data,
            family = binomial, offset = base) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = printp(p.value))

walk2(c("point_appearance", "point_exposure", "point_partners_exposure", 
        "point_condition"),
      c("appearanceNumeric", "exposureRate", "partnersExposureNumeric", 
        glue("condition{last(CONDITION_LABELS)}")), 
      ~make_text_vars(glm_point, .x, .y))

make_text_vars(glm_point, "point", "appearanceNumeric")
```

When should we expect participants to rely on pointing? Pointing has the highest utility for words you failed to learn during training, words you think your partner is unlikely to know (i.e., for lower partner knowledge conditions), and when the utility scheme is relatively biased toward pointing (i.e., the Lower Speech Efficiency condition). To test these predictions, we ran a mixed effects logistic regression to predict whether participants chose to point during a given trial as a function of the target object's exposure rate during training, object instance in the game (first, second, or third), utility manipulation, and partner manipulation. Random effects terms for subject and object were included in the model.

Consistent with our predictions, exposure rate during training was a significant negative predictor of pointing during the game, such that participants were less likely to rely on pointing for well trained (and thus well learned) objects ($\beta =$ `r point_exposure_estimate`, $p$ `r point_exposure_p.value`). Additionally, participants were significantly more likely to point in the Lower Speech Efficiency condition where pointing is relatively less costly, compared with the Higher Speech Efficiency condition ($\beta =$ `r point_condition_estimate`, $p$ `r point_condition_p.value`; see Figure \ref{fig:modalities-empirical}). We also found a significant negative effect of partner's knowledge, such that participants pointed more for partners with less knowledge of the lexicon ($\beta =$ `r point_partners_exposure_estimate`, $p$ `r point_partners_exposure_p.value`).

Note that these effects cannot be explained by solely participants' knowledge; all patterns above hold when looking *only* at words known by the participant at pretest (*ps < 0.01*). Further, these patterns mirror previous corpus analyses demonstrating parents' use of pointing in naturalistic parental communicative behaviors, and parents likely have lexical knowledge of even the least frequent referent [see @yurovsky2018children].

### Speech

```{r e1-speech-model}
glm_label <- glmer((method == "speak") ~ partnersExposureNumeric * exposureRate + 
                    partnersExposureNumeric * appearanceNumeric +
                    condition + 
                    (1| ldf_num) + (1|realLabel),
            data = filtered_data,
            family = binomial, offset = base) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = printp(p.value))

walk2(c("label_appearance", "label_exposure", "label_partners_exposure", 
        "label_condition"),
      c("appearanceNumeric", "exposureRate", "partnersExposureNumeric", 
        glue("condition{last(CONDITION_LABELS)}")), 
      ~make_text_vars(glm_label, .x, .y))
```

When should we expect participants to use speech? Speech has the highest utility for words you learned during training, words you think your partner is likely to know (i.e., for higher partner knowledge conditions), and when utility scheme is relatively biased toward speech (i.e., the Higher Speech Efficiency condition). To test these predictions, we ran a mixed effects logistic regression to predict whether participants chose to speak during a given trial as a function of the target object's exposure rate during training, object instance in the game (first, second, or third), utility manipulation, and partner manipulation. Random effects terms for subjects and object were included in the model.

Consistent with our predictions, speech seemed to largely trade off with gesture. Exposure rate during training was a significant positive predictor of speaking during the game, such that participants were more likely to use speech for well trained (and thus well learned) objects ($\beta =$ `r label_exposure_estimate`, $p$ `r label_exposure_p.value`). Additionally, participants were significantly less likely to speak in the Lower Speech Efficiency condition where speech is relatively more costly, compared with the Higher Speech Efficiency condition ($\beta =$ `r label_condition_estimate`, $p =$ `r label_condition_p.value`). We also found a significant positive effect of partner's knowledge, such that participants used speech more for partners with more knowledge of the lexicon ($\beta =$ `r label_partners_exposure_estimate`, $p$ `r label_partners_exposure_p.value`). Unlike for gesture, there was a significant effect of object instance in the game (i.e., first, second, or third trial with this target object) on the rate of speaking, such that later trials were more likely to elicit speech ($\beta =$ `r label_appearance_estimate`, $p$ `r label_appearance_p.value`). This effect of order likely stems from a trade-off with the effects we see in teaching (described below); after a participant teaches a word on the first or second trial, the utility of speech is much higher on subsequent trials.

### Emergence of Teaching.

Thus far, we have focused on relatively straightforward scenarios to demonstrate that a pressure to communicate successfully in the moment can lead participants to trade off between gesture and speech sensibly. Next, we turn to the emergence of teaching behavior.

```{r teaching-rates}
glm_teach <- glmer((method == MODALITY_LABELS[3]) ~ partnersExposureNumeric * exposureRate + 
                    partnersExposureNumeric * appearanceNumeric +
                    condition + 
                    (1| ldf_num) + (1|realLabel),
            data = filtered_data,
            family = binomial, offset = base) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = printp(p.value))

walk2(c("teach_appearance", "teach_exposure", "teach_partners_exposure", 
        "teach_condition"),
      c("appearanceNumeric", "exposureRate", "partnersExposureNumeric", 
        glue("condition{last(CONDITION_LABELS)}")), 
      ~make_text_vars(glm_teach, .x, .y))
```

When should we expect participants to teach? Teaching has the highest utility for words you learned during training, words you think your partner is unlikely to know (i.e., for lower partner knowledge conditions), and when utility scheme is relatively biased toward speech (i.e., the Higher Speech Efficiency condition). In this utility scheme, it is more valuable to pay the cost of teaching early because of the increased benefit of using speech later. To test these predictions, we ran a mixed effects logistic regression to predict whether participants chose to teach during a given trial as a function of the target object's exposure rate during training, object instance in the game (first, second, or third), utility manipulation, and partner manipulation. Random effects terms for subjects and object were included in the model.


```{r teach-plot-function}
teach_colors <- c(None = "#87d868", Same = "#54a832", Twice = "#2c6d12")

teach_plot <- function(df, method_column, label_data) {
  method_column <- enquo(method_column)
  
  prop_methods_teaching <- df %>%
    group_by(condition, ldf_num, partnersExposure, appearance, !!method_column) %>%
    summarise(n = n()) %>%
    group_by(condition, ldf_num, partnersExposure, appearance) %>%
    mutate(n = n / sum(n)) %>%
    ungroup() %>%
    complete(nesting(condition, ldf_num, partnersExposure),
             appearance, !!method_column, fill = list(n = 0)) %>%
    filter(!!method_column == MODALITY_LABELS[3]) %>%
    group_by(condition, partnersExposure, appearance, !!method_column) %>%
    tidyboot_mean(n)

  ggplot(prop_methods_teaching,
       aes(x = appearance, y = empirical_stat, label = partnersExposure,
           color = partnersExposure, group = partnersExposure)) +
    geom_line(size = 1, position = position_dodge(.25)) +
    geom_pointrange(aes(ymax = ci_upper, ymin = ci_lower),
                    position = position_dodge(.25)) +
    facet_grid(. ~ condition) +
    labs(y="Proportion of Teaching Trials", x = "Object instance during game") +
   # coord_cartesian(ylim = c(0,.4)) +
    scale_color_manual(values = teach_colors, name = "Partner's Exposure") +
    geom_text(data = label_data) +
    theme(legend.position = "none",
          legend.key.size = unit(.8, 'lines'),
          legend.text=element_text(size=7),
          legend.title=element_text(size=8),
          axis.text.x = element_text(angle = 35, hjust = 1))
}
```

```{r teach-empirical, fig.height = 3, fig.width = 4.25, fig.cap = "Rates of teaching across the six utility and partner knowledge conditions as a function of how many times the current target referent  object had previously been the target. Error bars show 95\\% confidence intervals computed by non-parametric bootstrapping."}

teach_label_data <- tibble(appearance = c(1.5, 1, 1), condition = CONDITION_LABELS[1], 
                     partnersExposure = c("None", "Same", "Twice"),
                     empirical_stat = c(.3, .165, .005))

teach_plot(filtered_data, method, teach_label_data) 
```


Consistent with our predictions, rates of teaching were higher for more highly trained words, less knowledgeable partners, and when speech had the highest utility. Exposure rate during training was a significant positive predictor of teaching during the game, such that participants were more likely to teach for well trained (and thus well learned) objects ($\beta =$ `r teach_exposure_estimate`, $p=$ `r teach_exposure_p.value`). While costly in the moment, teaching can be a beneficial strategy in our reference game because it subsequently allows for lower cost strategy (i.e. speaking), thus when speaking has a lower cost, participants should be more incentivized to teach. Indeed, participants were significantly less likely to teach in the Lower Speech Efficiency condition where speech is relatively more costly, compared with the Higher Speech Efficiency condition ($\beta =$ `r teach_condition_estimate`, $p=$ `r teach_condition_p.value`). We also found a significant negative effect of partner's knowledge, such that participants taught more with partners that had less knowledge of the lexicon ($\beta =$ `r teach_partners_exposure_estimate`, $p$ `r teach_partners_exposure_p.value`). There was also a significant effect of object instance in the game (i.e., whether this is the first, second, or third trial with this target object) on the rate of teaching. The planned utility of teaching comes from using another, cheaper strategy (speech) on later trials, thus the expected utility of teaching should decrease when there are fewer subsequent trials for that object, predicting that teaching rates should drop dramatically across trials for a given object. Participants were significantly less likely to teach on the later appearances of the target object ($\beta =$ `r teach_appearance_estimate`, $p$ `r teach_appearance_p.value`). 

## Discussion 

As predicted, the results of this experiment corroborate our findings from the corpus analysis, demonstrating that pedagogically supportive behavior emerges despite the initial cost when there is an asymmetry in knowledge and when speech is less costly than other modes of communication. While this paradigm has stripped away much of the interactive environment of the naturalistic corpus data, it provides important proof of concept that the structured and tuned language input we see in those data could arise from a pressure to communicate. The paradigm’s clear, quantitative trends also allow us to build a formal model to predict our empirical results.

The results from this experiment are qualitatively consistent with a model in which participants make their communicative choices to maximize their expected utility from the reference game. We next formalize this model to determine if these results are predicted quantitatively as well.