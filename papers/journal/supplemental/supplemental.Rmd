---
title: "\\LARGE Communicative pressure on caregivers leads to language input that supports children's word learning"
author: "\\large \\emph{Ben Morris and Daniel Yurovsky}"
header-includes:
  - \usepackage[section]{placeins}
  - \usepackage{float}
  - \floatplacement{figure}{h!} # make every figure with caption = t
  - \raggedbottom
output: 
  pdf_document:
    latex_engine: xelatex
    number_sections: true
documentclass: article
bibliography: ../reference-game.bib
fontsize: 11pt
geometry: margin=1in
csl: "../apa7.csl"
---

```{r load-libraries, message=FALSE, warning=FALSE, include = F}
library(readxl)
library(janitor)
library(here)
library(latex2exp)
library(data.table)
library(knitr)
library(papaja)
library(kableExtra)
library(tidyverse)
library(tidyboot)
library(feather)
library(lme4)
library(lmerTest)
library(broom)
library(broom.mixed)
library(boot)
library(ggthemes)

knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE, 
               tidy = FALSE, echo = FALSE)

theme_set(theme_few(base_size = 12))

options(digits = 3, dplyr.summarise.inform = FALSE)
set.seed(42)
```

\renewcommand\thesection{S\arabic{section}}
\renewcommand{\thetable}{S\arabic{table}}  
\renewcommand{\thefigure}{S\arabic{figure}}

\section{Corpus Referential Talk and Gesture Coding Supplemental}

Using the session transcripts, we coded all the concrete referents (in either speech or gesture) throughout the session that were produced by the parent or child. We further coded whether a referent was likely to be physically present, based primarily on transcript context. Coding was led by Madeline Meyers. A full coding manual can be found on the Open Science Foundation project page: https://osf.io/ums45/.

\subsection{Coding A Referent}

In our coding, we aimed to capture any referential communication about concrete in gesture or speech. A concrete noun can roughly be thought of as any noun that could be pointed to if it was present. Note that a single utterance may have multiple referents. For gesture, any deictic gesture toward an identifiable referent definitionally refers to a concrete noun. For references in spoken language, note that the referent sometimes differs from the word in the utterance--for example, using a nickname like "bidiba" to consistently refer to a blanket would be coded as "blanket."

Conversely, non-concrete nouns can roughly be thought of as any noun that cannot be pointed to, such as abstract concepts. These nouns were not included. Gestures that could not be readily identified or that gestured to vague locations (e.g., "over there" while gesturing) also did not receive a referent code.

\subsection{Coding for Presentness}

Not all concrete referents that are mentioned in speech are actually present in the environment. Using contextual information in the transcripts, we thus also coded whether each referent was likely to be present or not. Key contextual features included whether the same referent had been recently gestured to, question forms (e.g., "Where is the X?"), and semantic information about presentness ("Look at that?"). Every referent in an utterance is coded as "1" if present or "0" if not present. A subset of these codes were also done using video data to establish the reliability of our presentness coding. Further information can be found in the full coding manual on the Open Science Foundation project page: https://osf.io/ums45/.

\subsection{Coding Example}
\begin{tabular} {c c c c}
	\hline person & chat & spoken obj & ref\_pres\_predicted \\
    \hline parent &	want a bite of banana ?	 & banana &  1\\
    child & orange . & orange & 0\\
    parent	& that's all the orange we have . & orange & 0	\\
    parent & that's it . & & \\
\end{tabular}

\section{Experimental Results}

```{r}
all_data <- read_csv(here("data/1.30_turk_exp.csv"),
                     show_col_types = FALSE) %>%
  select(-`...1`, -X1)

filtered_data <- all_data %>%
  filter(toBeDropped != 1) %>%
  mutate(base = logit(1/3),
         appearanceNumeric = as.numeric(as.factor(appearance)),
         partnersExposure = factor(partnersExposure, 
                                   levels = c("None", "Same", "Perfect"),
                                   labels = c("None", "Same", "Twice")),
         partnersExposureNumeric = as.numeric(partnersExposure) - 1)
```

For readability, the main text includes only the key effects for each statistical model rather than a full specification. We include those here. Each model included at least a random intercept for each subject and item.

\subsection{Learning}

To confirm that we successfully manipulated participants' learning, we asked whether items with more exposure during training were better learned at pretest. To do this we fit a logistics mixed-effects model to analyze learning at baseline (i.e. prior to gameplay). We see the predicted significant effect of exposure rate on learning, confirming that object-label mappings that were presented more in training were better learned. 

Additionally, we tested our critical between-subjects manipulations to ensure that learning of the lexicon did not differ significantly at pretest (prior to the manipulations). Neither utility condition or partner's exposure significantly predicted performance at pretest. This provides a simple check that participants in each condition learned the lexicon similarly. The full results of this model are presented in Table \ref{tab:learningTable}.

```{r learning}
learningByExposure <- filtered_data %>% 
  group_by(ldf_num, condition, partnersExposureNumeric, realLabel, exposureRate) %>%
  summarize(testCorrect = first(testCorrect))

gm_known <- glmer(testCorrect ~ exposureRate + condition + 
                    partnersExposureNumeric + (exposureRate|ldf_num) + 
                    (1|realLabel),
            data = learningByExposure,
      family=binomial) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = printp(p.value)) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  mutate(term = c("intercept", "exposure rate", "utility condition", "partner's exposure"))
```

```{r learningTable}
apa_table(gm_known, format.args = list(na_string = ""), font_size = "footnotesize",
            caption = "Pariticpant learning at baseline, specified as \\texttt{testCorrect $\\sim$ exposureRate + condition + (exposureRate | subj) + (1 | realLabel)}.", escape = FALSE, placement = "h")
```

\subsection{Communicative Strategy}

Our key analyses concerned participants choice of communicative strategy. In each trial, participants were able to choose one of 3 communicative strategies: point, speak, or teach We expected flexible trade-off between the use of each strategy given their relative utilities, participants' knowledge of the lexicon, and the partners' knowledge of the lexicon. To test our predictions about each communicative behavior (gesture, speech, and teaching), we conducted separate logistic mixed-effects models for each behavior, reported below. The mixed effects model for each communicative behavior has an identical effect structure for comparability. Each model included a random effect of subject and item. It should be noted that these three behaviors are mutually exhaustive.

\subsubsection{Pointing}

To examine pointing, we ran a mixed effects logistic regression to predict whether speakers chose to point during a given trial as a function of the target object's exposure rate during training, object instance in the game (first, second, or third), utility manipulation, and partner manipulation. Random effects terms for subject and object were included in the model. Consistent with our predictions, participants gestured more for words that received less training, with partners who had less knowledge, and in the condition where the utility of pointing was higher. The full results of this model are presented in Table \ref{tab:clickTable}.

```{r click, results="asis", cache = T}
gm_click <- glmer((method == 'click') ~  exposureRate * partnersExposureNumeric + 
                    partnersExposureNumeric * appearanceNumeric +
                    condition + 
                    (1| ldf_num) + (1|realLabel),
            data = filtered_data,
            family = binomial, offset = base) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = printp(p.value)) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  mutate(term = c("intercept", "exposure rate", "partner's exposure", "instance",
                  "lower speech efficiency condition", "partner's exposure * exposure rate",
                  "partner's exposure * instance"))
```

```{r clickTable}
apa_table(gm_click, format.args = list(na_string = ""), font_size = "footnotesize",
            caption = "Propensity to point, specified as \\texttt{point $\\sim$ exposureRate * partnersExposure + appearanceNum * partnersExposure + utilityCondition + (1 | subj) + (1 | realLabel)}.", escape = FALSE, placement = "h")
```


\subsubsection{Speaking}

To examine speaking, we ran a mixed-effects logistic regression to predict whether speakers chose to speak during a given trial as a function of the target object's exposure rate during training, object instance in the game (first, second, or third), utility manipulation, and partner manipulation. Random effects terms for subject and object were included in the model. Consistent with our predictions, participants used labels more for words that received more training, with partners who had more knowledge, and in the condition where the utility of speech was higher. The full results of this model are presented in Table \ref{tab:labelTable}.


```{r label, results="asis", cache = T}
gm_label <- glmer((method == 'label') ~ exposureRate * partnersExposureNumeric + 
                    partnersExposureNumeric * appearanceNumeric +
                    condition + 
                    (1| ldf_num) + (1|realLabel),
            data = filtered_data,
            family = binomial, offset = base) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = printp(p.value)) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  mutate(term = c("intercept", "exposure rate", "partner's exposure", "instance",
                  "lower speech efficiency condition", "partner's exposure * exposure rate",
                  "partner's exposure * instance"))
```

```{r labelTable}
apa_table(gm_label, format.args = list(na_string = ""), font_size = "footnotesize",
            caption = "Propensity to use labeling as a strategy, specified as \\texttt{speak $\\sim$ exposureRate * partnersExposure + appearanceNum * partnersExposure + utilityCondition + (1 | subj) + (1 | realLabel)}.", escape = FALSE)
```

\subsubsection{Teaching}

To examine teaching, we ran a mixed-effects logistic regression to predict whether speakers chose to teach during a given trial as a function of the target object's exposure rate during training, object instance in the game (first, second, or third), utility manipulation, and partner manipulation. Random effects terms for subject and object were included in the model. Consistent with our predictions, participants taught labels more often (i.e. used both the pointing and speaking strategy simultaneously) for words that received more training, with partners who had less knowledge, and in the condition where the utility of speech was higher. The full results of this model are presented in Table \ref{tab:teachTable}.

```{r teach, results="asis"}
gm_teach <- glmer((method == 'label_click') ~ exposureRate * partnersExposureNumeric  + 
                    partnersExposureNumeric * appearanceNumeric +
                    condition + 
                    (1| ldf_num) + (1|realLabel),
            data = filtered_data,
            family = binomial, offset = base) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = printp(p.value)) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  mutate(term = case_when(term == "condition80_50" ~ "High Relative Cost",
                          TRUE ~ term)) %>%
  mutate(term = c("intercept", "exposure rate", "partner's exposure", "instance",
                  "lower speech efficiency condition", "partner's exposure * exposure rate",
                  "partner's exposure * instance"))
```

```{r teachTable}
apa_table(gm_teach, format.args = list(na_string = ""), font_size = "footnotesize",
            caption = "Propensity to use teaching as a strategy, specified as \\texttt{teach $\\sim$ exposureRate * partnersExposure + appearanceNum * partnersExposure + utilityCondition + (1 | subj) + (1 | realLabel)}.", escape = FALSE)
```


\section{Estimating model parameters}

In order to explain how people choose their communicative modality in our reference game, we developed a model of communication as planning under uncertainty. This model has two parameters that need to be specified in order to get quantitative predictions out of it: (1) A rationality parameter $\alpha$ that controls how sensitive the model is to differences in utility, and (2) a discounting parameter $\gamma$ that controls how much less the model cares about the utility it would get on its next trial than on the current trial when it makes its plan.

One way of determining the values for these parameters would be to use a Maximum Likelihood estimator to find the unique values that make the data most likely (provide the best fit). However, these point estimates would not give us a way of quantifying our uncertainty about the right parameters, nor would they correctly penalize the model correctly if only a very narrow range of the possible parameter values gave a good account for the data [see e.g. @pitt2002]. Instead, we used Empirical Bayesian methods to estimate the full posterior probabilities of model parameters conditioned on both parameter priors and the observed data. The values reported in the paper are estimated means for these posterior distributions.

Ideally, we would like to condition on all of the data from all of the participants simultaneously. However, because of variability in the parameter values that gave good fits for different participants, the estimation algorithm did not converge in a tractable number of samples when all of the data points were considered simultaneously. To address this, we instead chose to estimate the model parameters hierarchically. We defined the generative process as one in which participants have their own individual $\alpha$ and $\gamma$ parameters drawn from a single population $\alpha$ and $\gamma$ distribution respectively as in the equation below (like random effects in a mixed-effects model; \ref{eq:hierarchical-sampling}). These group-level level rationality and discounting parameters are reported in the main text.

\begin{align*}
P\left(\alpha,\gamma \vert D\right) &
\propto P\left(D \vert \alpha,\gamma \right)P\left(\alpha\right)P\left(\gamma\right)\\
& \propto \left(\prod_{i}P\left(D_i \vert \alpha_i,\gamma_i \right) P\left(\alpha_i \vert \alpha\right)P\left(\gamma_i \vert \gamma\right)\right)P\left(\alpha\right)P\left(\gamma\right)
\end{align*}

```{r load-individual-fits}
files <- list.files(here("model/samples/full_model"), pattern = "*.csv", 
                    full.names = TRUE)


read_file <- function(filename) {
  ldf_num <- str_split(filename, "/") %>%
    unlist() %>%
    last() %>%
    str_extract(.,"[0-9]+") %>%
    as.numeric()
  
  fread(filename) %>%
    as_tibble() %>%
    mutate(ldf_num = ldf_num)
}

nested_data <- map_dfr(files, read_file)

mean_data <- nested_data %>%
  filter(Parameter != "score") %>%
  mutate(Parameter = factor(Parameter, levels = c("alpha", "discount"),
                            labels = c(TeX("rationality ($\\alpha$)", 
                                           output = "character"),
                                       TeX("discount ($\\gamma$)", 
                                           output = "character")))) %>%
  group_by(Parameter, ldf_num) %>%
  summarise(value = mean(value))

n_participants <- mean_data %>%
  ungroup() %>%
  distinct(ldf_num) %>%
  pull() %>%
  length()
```

Unfortunately, estimating this full hierarchical model also proved intractable. Consequently, we first estimated each participant's individual parameters from their data using 200 steps of Metropolis-Hastings sampling after 100 burn-in samples. We used a $\text{Uniform}\left(0,20\right)$ prior for $\alpha$ and a $\text{Uniform}\left(0,1\right)$ prior for $\gamma$. The goal of these Uniform priors was to minimally impact the inferred parameter values as the true priors for them should be the population level distributions. Figure \ref{fig:participant-means} shows the distribution of posterior means $\alpha$ and $\gamma$ estimated for for each of the `r n_participants` participants.

```{r participant-means, fig.width = 4.5, fig.height = 2.5, fig.cap = "\\label{fig:participant-means}Estimated rationality and discount parameters for individual participants."}

ggplot(mean_data, 
       aes(x = value)) + 
  geom_histogram() + 
  facet_wrap( ~ Parameter, scales = "free", labeller = label_parsed) +
  labs(y = "# participants")
```

We used the posterior mean of each participant's parameter distributions, as well as the likelihood of observing each participant's data given these parameters to estimate the population-level parameters. To do this we used 10,000 steps of Metropolis-Hastings sampling after 2,000 burn-in samples. To help the sampler converge, instead of using the prior distribution as the proposal function, we used a $\text{Normal}$ distribution centered at the last sample with a standard deviation if .1. 

At this group-level, we used theoretically-motivated priors. The rationality parameter $\alpha$ controls the sensitivity of the model to differences in utility and can range over the non-negative real numbers $\left[0,\infty\right)$. A rationality value of 1 corresponds to probability matching--in which actions are chosen according to the @luce1959 Choice rule, and larger values correspond to increasingly great moves towards maximizing. For this parameter, we chose a distribution of the prior with a mean of 1, but an Exponential shape with a long right tail. This distribution makes very high levels of rationality unlikely but not impossible (\ref{eq:alpha}). 

\begin{equation}
P\left(\alpha\right) \sim \text{Gamma}\left(1,1\right)
\end{equation}

The discounting parameter $\gamma$ controls how much the model values utility it will gain on the next step in its plan relative to the current step. This parameter can take all values between 0 and 1. As a mathematical convenience, we modeled the log odds ratio between the next trial and the previous trial with the $\text{logit}$ function rather than assigning this parameter a prior directly. Like the squashing function in neural network models, or the linking function in logistic regression, this allows us to instead sample from all real-number values and then transform to the $\left(0,1\right)$ range. We thus used a $\text{Normal}$ distribution with a mean of 0 in logit space, equivalent to a $\gamma$ of .5. 
\begin{equation}
P\left(logit\left(\gamma\right)\right) \sim \text{Normal}\left(0,1\right)
\end{equation}

Importantly, because the model's parameters were estimated from many trials of behavior from hundreds of participants, the priors are dwarfed by the likelihoods in estimating the parameters' posterior distributions. The estimates reported in the paper are thus robust to many other prior choices. This sampling procedure was identical for all models described in the main paper. All code is available in the project page: https://osf.io/d9gkw/, as are all the samples generated in these sampling procedures.

\section*{References}

\begingroup
\setlength{\parindent}{-0.5in}

\noindent

<div id = "refs"></div>
\endgroup