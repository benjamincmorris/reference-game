---
title             : "A communicative framework for early word learning"
shorttitle        : "Communicative Word Learning"

author: 
  - name          : "Benjamin C. Morris"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Department of Psychology, University of Chicago, 5848 S University Ave, Chicago, IL 60637"
    email         : "benmorris@uchicago.edu"
  - name          : "Daniel Yurovsky"
    affiliation   : "1,2"
    corresponding : no    # Define only one corresponding author

affiliation:
  - id            : "1"
    institution   : "University of Chicago"
  - id            : "2"
    institution   : "Carnegie Mellon University"

author_note: >

abstract: >
  Children do not learn language from passive observation of the world, but from interaction with caregivers who want to communicate with them. These communicative exchanges are structured at multiple levels in ways that support support language learning. We argue this pedagogically supportive structure can result from pressure to communicate successfully with a linguistically immature partner. We first characterize one kind of pedagogically supportive structure in a corpus analysis: caregivers provide more information-rich referential communication, using both gesture and speech to refer to a single object, when that object is rare and when their child is young. Then, in an iterated reference game experiment on Mechanical Turk (n = 480), we show how this behavior can arise from pressure to communicate successfully with a less knowledgeable partner. Lastly, we show that speaker behavior in our experiment can be explained by a rational planning model, without any explicit teaching goal. We suggest that caregivers’ desire to communicate successfully may play a powerful role in structuring children’s input in order to support language learning.
  
keywords          : "language learning; communication; computational modeling"
wordcount         : "X"

bibliography      : ["reference-game.bib"]

header-includes:
  - \usepackage{xcolor}

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
---

```{r load-packages}
library(papaja)
library(feather)
library(here)
library(lme4)
library(lmerTest)
library(png)
library(boot)
library(broom)
library(broom.mixed)
library(ggthemes)
library(tidyverse)

knitr::opts_chunk$set(fig.pos = 'tb', echo = FALSE, cache = TRUE, 
                      warning = FALSE, message = FALSE, 
                      sanitize = TRUE, fig.path='figs/', fig.width = 3,
                      fig.height = 3)

theme_set(theme_few(base_size = 12))
options(digits = 3)
set.seed(42)
```

One of the most striking aspects of children’s language learning is just how quickly they master the complex system of their natural language (Bloom, 2000). In just a few short years, children go from complete ignorance to conversational fluency in a way that is the envy of second-language learners attempting the same feat later in life (Newport, 1990). What accounts for this remarkable transition?

Distributional learning presents a unifying account of early language learning: Infants come to language acquisition with a powerful ability to learn the latent structure of language from the statistical properties of speech in their ambient environment (Saffran, 2003). Distributional learning mechanisms can be seen in accounts across language including phonemic discrminitation (Maye, Werker, & Gerken, 2002), word segmentation (Saffran, 2003), learning the meanings of both nouns (Smith & Yu, 2008) and verbs (Scott & Fisher, 2012), learning the meanings of words at multiple semantic levels (Xu & Tenenbaum, 2007), and perhaps even the grammatical categories to which a word belongs (Mintz, 2003). A number of experiments clearly demonstrate both the early availability of distributional learning mechanisms and their potential utility across these diverse language phenomena (DeCasper & Fifer, 1980; DeCasper & Spence, 1986; Gomez & Gerken, 1999; Graf Estes, Evans, Alibali, & Saffran, 2007; Maye, Werker, & Gerken, 2002; Saffran, Newport, & Aslin, 1996; Smith & Yu, 2008; Xu & Tenenbaum, 2007). 

However, there is reason to be suspicious about just how precocious statistical learning abilities are in early development. Although these abilities are available early, they are highly constrained by limits on other developing cognitive capacities. For example, infants’ ability to track the co-occurrence information connecting words to their referents is constrained significantly by their developing memory and attention systems (Smith & Yu, 2013; Vlach & Johnson, 2013). Computational models of these processes show that the rate of acquisition is highly sensitive to variation in environmental statistics (e.g., Vogt, 2012). Models of cross-situational learning have demonstrated that the Zipfian distribution of word frequencies and word meanings yields a learning problem that cross-situational learning alone cannot explain over a reasonable time frame (Vogt, 2012). Further, a great deal of empirical work demonstrates that cross-situational learning even in adults drops off rapidly when participants are asked to track more referents, and also when the number of intervening trials is increased (e.g., Yurovsky & Frank, 2015). Thus, precocious unsupervised statistical learning appears to fall short of a complete explanation for rapid early language learning.

Even relatively constrained statistical learning could be rescued, however, if caregivers structured their language in a way that simplified the learning problem and promoted learning. For example, in phoneme learning, infant-directed speech provides examples that seem to facilitate the acquisition of phonemic categories (Eaves et al., 2016). In word segmentation tasks, infant-directed speech facilitates infant learning more than matched adult-directed speech (Thiessen, Hill, & Saffran, 2005). In word learning scenarios, caregivers produce more speech during episodes of joint attention with young infants, which uniquely predicts later vocabulary (Tomasello & Farrar, 1986).  Child-directed speech even seems to support learning at multiple levels in parallel-- e.g., simultaneous speech segmentation and word learning (Yurovsky et al., 2012). For each of these language problems faced by the developing learner, caregiver speech exhibits structure that seems uniquely beneficial for learning. 

Under distributional learning accounts, the existence of this kind of structure is a theory-external feature of the world that does not have an independently motivated explanation. Such accounts view the generative process of structure in the language environment as a problem separate from language learning. However, across a number of language phenomena, the language environment is not merely supportive, but seems calibrated to children’s changing learning mechanisms. For example, across development, caregivers engage in more multimodal naming of novel objects than familiar objects, and rely on this synchrony most with young children (Gogate, Bahrick, & Watson, 2000). The role of synchrony in child-directed speech parallels infant learning mechanisms: young infants appear to rely more on synchrony as a cue for word learning than older infants, and language input mirrors this developmental shift (Gogate, Bahrick, & Watson, 2000). Beyond age-related changes, caregiver speech may also support learning through more local calibration to a child’s knowledge; caregivers have been shown to provide more language to refer to referents that are unknown to their child, and show sensitivity to the knowledge their child displays during a referential communication game (Leung et al., 2019).  The calibration of parents production to the child’s learning suggests a co-evolution such that these processes should not be considered in isolation.  

What then gives rise to structure in early language input that mirrors child learning mechanisms? Because of widespread agreement that parental speech is not usually motivated by explicit pedagogical goals (Newport et al., 1977), the calibration of speech to learning mechanisms seems a happy accident; parental speech just happens to be calibrated to children’s learning needs. Indeed, if parental speech was pedagogically-motivated, we would have a framework for deriving predictions and expectations (e.g., Shafto, Goodman, & Griffiths, 2014). Models of optimal teaching have been successfully generalized to phenomena as broad as phoneme discrimination (Eaves et al., 2016) to active learning (Yang et al., 2019). These models take the goal to be to teach some concept to a learner and attempt to optimize that learner’s outcomes. While these optimal pedagogy accounts have proven impressively useful, such models are theoretically unsuited to explaining parent language production where there is widespread agreement that caregiver goals are not pedagogical (e.g., Newport et al., 1977).

Instead, the recent outpouring of work exploring optimal communication (the Rational Speech Act model, see Frank & Goodman, 2012) provides another framework for understanding parent production. Under optimal communication accounts, speakers and listeners engage in recursive reasoning to produce and interpret speech cues by making inferences over one another’s intentions (Frank & Goodman, 2012). These accounts have made room for advances in our understanding of a range of language phenomena previously uncaptured by formal modeling, notably a range of pragmatic inferences (e.g., Frank & Goodman, 2012; other RSA papers). In this work, we consider the communicative structure that emerges from an optimal communication system across a series of interactions where one partner has immature linguistic knowledge. This perspective offers the first steps toward a unifying account of both the child’s learning and the parents’ production: Both are driven by a pressure to communicate successfully (Brown, 1977). 

Early, influential functionalist accounts of language learning focused on the importance of communicative goals (e.g., Brown, 1977). Our goal in this work is to formalize the intuitions in these accounts in a computational model, and to test this model against experimental data. We take as the caregiver’s goal the desire to communicate with the child, not about language itself, but instead about the world in front of them. To succeed, the caregiver must produce the kinds of communicative signals that the child can understand and respond contingently, potentially leading caregivers to tune the complexity of their speech as a byproduct of in-the-moment pressure to communicate successfully (Yurovsky, 2017).

To examine this hypothesis, we focus on ostensive labeling (i.e. using both gesture and speech in the same referential expression) as a case-study phenomenon of information-rich structure in the language learning environment.  We first analyze naturalistic parent communicative behavior in a longitudinal corpus of parent-child interaction in the home (Goldin-Meadow et al., 2014). We investigate the extent to which parents tune their ostensive labeling across their child’s development to align to their child’s developing linguistic knowledge (Yurovsky, Doyle, & Frank, 2016). 

We then experimentally induce this form of structured language input in a simple model system: an iterated reference game in which two players earn points for communicating successfully with each other. Modeled after our corpus data, participants are asked to make choices about which communicative strategy to use (akin to modality choice). In an experiment on Mechanical Turk using this model system, we show that tuned, structured language input can arise from a pressure to communicate. We then show that participants’ behavior in our game conforms to a model of communication as rational planning: People seek to maximize their communicative success while minimizing their communicative cost over expected future interactions. Lastly, we demonstrate potential benefits for the learner through a series of simulations to show that communicative pressure facilitates learning compared with various distributional learning accounts. 

# Corpus Analysis
```{r corpus-data}
# Plot of Referential Communication from LDP Corpus Data
corpus_data <- read_feather(here("data/coded_responses.feather"))

modality_data <- corpus_data %>%
  group_by(person, age, subj, freq_cut, rank, freq, referent, modality, chat) %>%
  summarise(n = n()) %>%
  spread(modality, n, fill = 0)


plot_data <- modality_data %>%
  group_by(person, age, subj, freq, referent) %>%
  gather(modality, n, both, gesture, speech) %>%
  mutate(prop = n/sum(n)) %>%
  filter(modality == "both") %>%
  ungroup()

#set gradient
cc <- scales::seq_gradient_pal("#87d868", "#124200", "Lab")(seq(0,1,length.out=6))
cc[[6]] <- "#000000"
teaching_ribbons <- scales::seq_gradient_pal("#cbffb7", "#124200", "Lab")(seq(0,1,length.out=6))
teaching_ribbons[[6]] <- "#000000"
```

We first investigate parent referential communication in a longitudinal corpus of parent-child interaction. We analyze the production of multi-modal cues (i.e. using both gesture and speech) to refer to the same object, in the same instance. While many aspects of CDS support learning, multi-modal cues (e.g., speaking while pointing or looking) are particularly powerful sources of data for young children [e.g., @baldwin2000; @gogate2000]. We take multi-modal cues to be a case-study pheonmenon of pedagogically supportive language input. While our account should hold for other language phenomena, by focusing on one phenomenon we attempt to specify the dynamics involved in the production of such input.

In this analysis of naturalistic communication, we examine the prevelance of multi-modal cues in children's language environment, to demonstrate that it is a viable, pedagogically supportive form of input. Beyond being a prevelant form of communication, multi-modal reference may be especially pedagogically supportive if usage patterns reflect adaptive linguistic tuning, with caregivers using this information-rich cue more for young children and infrequent objects. The amount of multi-modal reference should be sensitive to the child’s age, such that caregivers will be more likely to provide richer communicative information when their child is younger (and has less linguistic knowledge) than as she gets older [@yurovsky2016]. 


## Methods

We used data from the Language Development Project-- a large-scale, longitudinal corpus of naturalistic parent child-interaction in the home [@goldin-meadow2014]. The Language Development Project corpus contains transcription of all speech and communicative gestures produced by children and their caregivers over the course of the 90-minute home recordings. An independent coder analyzed each of these communicative instances and identified each time a concrete noun was referenced using speech, gesture, or both in the same referential expression (so called ostenstive labeling). In these analyses, we focus only caregiver's productions of ostenstive labeling.


### Participants

The Language Development Project aimed to recruit a sample of families who are representative of the Chicago community in socio-economic and racial diversity [@goldin-meadow2014]. These data are drawn from a subsample of 10 families from the larger corpus. Our subsample contains data taken in the home every 4-months from when the child was 14-months-old until they were 34-months-old, resulting in 6 timepoints (missing one family at the 30-month timepoint). Recordings were 90 minute sessions, and participants were given no instructions.

Of the 10 target children, 5 were girls, 3 were Black and 2 were Mixed-Race. Families spanned a broad range of incomes, with 2 families earning \$15,000 to \$34,999 and 1 family earning greater than \$100,000. The median family income was \$50,000 to \$74,999.


### Procedure

From the extant transcription and gesture coding, we specifically coded all concrete noun referents produced in either the spoken or gestural modality (or both). Spoken reference was coded only when a specific noun form was used (e.g., "ball"), to exlcude pronouns and anaphoric usages (e.g., "it"). Gesture reference was coded only for deitic gestures (e.g., pointing to or holding an object) to minimize ambiguity in determining the intended referent. In order to fairly compare rates of communication across modalities, we need to examine concepts that can be referred to in either gesture or speech (or both) with similar ease. Because abstract entites are difficult to gesture about using deitic gestures, we coded only on references to concrete nouns. 

### Reliability

To establish the reliability of the referent coding, 25% of the transcripts were double-coded. Inter-rater reliability was sufficently high (Cohen’s $\kappa$ = 0.76). Disagreements in coding decisions were discussed and resolved by hand.

To ensure that our each referent could potentially be refered to in gesture or speech, we focused on concrete nouns. We further wanted to ensure that the referents were physically present in the scene (and thus accessible to deitic gestures). Using the transcripts, a human rater judged whether the referent was likely to be present, primarily relying on discourse context (e.g., a referent was coded as present if the deitic gesture is used or used at another timepoint for the reference, or if the utterance included demonstratives such as "This is an X"). A full description of the coding criterea can be found in the Supporting Materials. \textcolor{red}{MAKE SURE WE MAKE THIS}.

To ensure our transcript-based coding of presentness was sufficiently accurate, a subset of the transcripts (5%) were directly compared to corresponding video data observation. Reliability across the video data and the transcript coding was sufficiently high ($\kappa$ = 0.72). Based on transcript coding of all the referential communication about concrete nouns, 90% of the references were judged to be about referents that were likely present. All references are included in our dataset for further analysis.

## Results

```{r corpus-plot-lm}
mid_age <- modality_data %>% 
  pull(age) %>%
  unique() %>% 
  mean() %>%
  log()

mid_freq <- modality_data %>% 
  pull(freq) %>%
  unique() %>% 
  mean() %>%
  log()

corpus_model_data <- modality_data %>%
  ungroup() %>%
  filter(person == "parent") %>%
  mutate(age = log(age) - mid_age,
         freq = log(freq) - mid_freq)


parent_both_raw <- glmer(cbind(both, speech + gesture) ~ age * freq + 
                    (1 | referent) + ( freq | subj), 
      data = corpus_model_data, 
      family = "binomial") 


parent_both_lm <- parent_both_lm_raw %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group)


corpus_age_estimate <- parent_both_lm %>% 
  filter(term == "age") %>% 
  pull(estimate)

corpus_age_statistic <- parent_both_lm %>%
  filter(term == "age") %>% 
  pull(statistic)

corpus_age_p <- parent_both_lm %>% 
  filter(term == "age") %>%
  pull(p.value) %>% 
  printp()

corpus_freq_estimate <- parent_both_lm %>% 
  filter(term == "freq") %>% 
  pull(estimate)

corpus_freq_statistic <- parent_both_lm %>%
  filter(term == "freq") %>% 
  pull(statistic)

corpus_freq_p <- parent_both_lm %>% 
  filter(term == "freq") %>%
  pull(p.value) %>% 
  printp()

corpus_interaction_estimate <- parent_both_lm %>% 
  filter(term == "age:freq") %>% 
  pull(estimate)

corpus_interaction_statistic <- parent_both_lm %>%
  filter(term == "age:freq") %>% 
  pull(statistic)

corpus_interaction_p <- parent_both_lm %>% 
  filter(term == "age:freq") %>%
  pull(p.value) %>% 
  printp()

```

These corpus data were analyzed using a mixed effects regression to predict parent use of multi-modal reference for a given referent. The model included fixed effects of age in months, frequency of the referent, and the interaction between the two. The model included a random intercept and random slope of frequency by subject and a random intercept for each unique referent. Frequency and age were both log-scaled and then centered both because age and frequency tend to have log-linear effects and to help with model convergence. The model showed that parents teach less to older children ($\beta =$  `r corpus_age_estimate`, $t =$  `r corpus_age_statistic`, $p$ `r corpus_age_p`), marginally less for more frequent targets ($\beta =$  `r corpus_freq_estimate`, $t =$  `r corpus_freq_statistic`, $p =$ `r corpus_freq_p`), and that parents teach their younger children more often for equally frequent referents ($\beta =$  `r corpus_interaction_estimate`, $t =$  `r corpus_interaction_statistic`, $p =$ `r corpus_interaction_p`). Thus, in these data, we see early evidence that parents are providing richer, structured input about rarer things in the world for their younger children (Figure \ref{fig:corpus-plot).


```{r corpus-plot, fig.height=4, fig.width = 6, fig.cap = "Proportion of parent multi-modal referential talk across development. The log of a referent's frequency is given on the x-axis, with less frequent items closer to zero."}

plot_data %>% 
  ungroup() %>%
  filter(person == "parent") %>%
  mutate(plot_age = factor(age, labels = c('14 months', '18 months','22 months', 
                                      '26 months', "30 months",
                                      "34 months"))) %>%
  ggplot(aes(x = log(freq), y = prop, color = plot_age, 
             fill = plot_age, label = plot_age)) +
  geom_smooth(method = "loess") +
  labs(y="References\nusing gesture + speech (proportion)", x='Referent frequency (log)') +
  coord_cartesian(ylim=c(0,.38), xlim=c(0,6)) +
  scale_fill_manual(values = teaching_ribbons, name='Child\'s age') +
  scale_color_manual(values = teaching_ribbons, name='Child\'s age') +
  theme(legend.position = c(.8,.67),
      legend.key.size = unit(.8, 'lines'),
      legend.text=element_text(size=7),
      legend.title=element_text(size=8),
      aspect.ratio = 1/1.66)
```

## Discussion

Caregivers are not indiscriminate in their use of multi-modal reference; in these data, they provided more of this support when their child was younger and when discussing less familiar objects. These longitudinal corpus findings are consistent with an account of parental alignment: parents are sensitive to their child’s linguistic knowledge and adjust their communication accordingly (Yurovsky et al., 2016). Ostensive labeling is perhaps the most explicit form of pedagogical support, so we chose to focus on it for our first case study. We argue that these data could be explained by a simple, potentially-selfish pressure: to communicate successfully. The influence of communicative pressure is difficult to draw in naturalistic data, so we developed a paradigm to try to experimentally induce richly-structured, aligned input from a pressure to communicate in the moment.

# Experimental Framework

To study the emergence of pedagogically supportive input from communicative pressure, we developed a simple reference game in which participants would be motivated to communicate successfully. After giving people varying amounts of training on novel names for 9 novel objects, we asked them to play a communicative game in which they were given one of the objects as their referential goal, and they were rewarded if their partner successfully selected this referent from among the set of competitors (Figure \ref{fig:exp_screenshot}). 

Participants could choose to refer either using the novel labels they had been exposed to, or they could use a deictic gesture to indicate the referent to their partner. Deixis was unambiguous, and thus would always succeed. However, in order for language to be effective, the participant and their partner would have to know the correct novel label for the referent. 

Across conditions, we manipulated the relative costs of these two communicative methods (gesture and speech), as we did not have a direct way of assessing these costs in our naturalistic data, and they likely vary across communicative contexts. In all cases, we assumed that gesture was more costly than speech.  Though this need not be the case for all gestures and contexts, our framework compares simple lexical labeling and unambiguous deictic gestures, which likely are more costly and slower to produce [see @yurovsky2018]. We set the relative costs by explicitly implementing strategy utility, assigning point values to each communicative method.

If people are motivated to communicate successfully, their choice of referential modality should reflect the tradeoff between the cost of producing the communicative signal with the likelihood that the communication would succeed. We thus predicted that peoples' choice of referential modality would reflect this tradeoff: People should be more likely to use language if they have had more exposures to the novel object's correct label, and they should be more likely to use language as gesture becomes relatively more costly. 

Across two experiments, XXX participants were recruited to play our reference games via Amazon Mechanical Turk, an online platform that allows workers to complete surveys and short tasks for payment. In these studies, all participants were placed in the role of speaker and listener responses were programmed. 

# Experiment 1   

In this experiment, we provide proof of concept demonstrating that participants were sensitive to our manipulations and that we could induce speech-gesture tradeoff with this paradigm. 

## Method

### Participants

80 participants were recruited though Amazon Mechanical Turk and received a small payment for their participation. Data from XXX participants was excluded from subsequent analysis for failing the manipulation check or for producing illegal pseudo-English labels (e.g., 'pricklyyone').

### Design and Procedure

Participants were told they would be introduced to novel object-label pairs and then asked to play a communication game with a partner wherein they would have to refer to a particular target object. Participants were exposed to nine novel objects, each with a randomly assigned pseudo-word label. We manipulated the exposure rate within-subjects: during training participants saw three of the nine object-label mappings four times, two times, or just one time, yielding a total of 21 training trials. Participants were then given a simple recall task to establish their knowledge of the novel lexicon (pretest). 
  
After being introduced to the rules of the game, participants are screened to ensure they understand the rules of the game (manipulation check). During gameplay, speakers saw the target object in addition to an array of all six objects. Speakers had the option of either directly selecting the target object from the array (deictic gesture)- a higher cost cue but without ambiguity- or typing a label for the object (speech)- a lower cost cue but contingent on the listener's knowledge.  After sending the message, speakers are shown which object the listener selected.  

If the speaker clicked on object (gesture message), the listener was programmed to simply make the same selection. To simulate knowledgable listener behavior when the speaker typed an object label in Experiment 1, the listener evaluated the Levenshtein distance (LD) between the typed label and each of the nine possible labels and selected the candidate with the smallest edit distance (e.g., if a speaker entered the message "tomi", the programmed listener would select the referent corresponding to "toma"). If the speaker message had an LD greater than two with each of the nine words in the novel lexicon, the listener always selected an incorrect object. 

Speakers could win up to 100 points per trial if the listener correctly selected the target referent based on their message.  If the listener failed to identify the target object, the speaker received no points.  We manipulated the relative utilities of each of the strategies between-subjects. In the 'Higher Relative Cost' condition, speakers received 30 points for gesturing and 100 points for labeling, and thus gesturing was very costly relative to speech and pariticpants should be highly incentivized to speak. In the 'Low Relative Cost' condition speakers received 50 points for gesturing and 80 points for labeling, and thus gesturing is still costly relative to speech but much less so and pariticpants should be less incentivized to speak. 40 participants were run in each of the two conditions.

Speakers could win up to 100 points per trial if the listener correctly selected the target referent.  We manipulated the relative utility of the speech cue between-subjects across two conditions: low relative cost for speech (‘Low Relative Cost’) and higher relative cost for speech (‘Higher Relative Cost’). In the ‘Low Relative Cost’ condition, speakers were charged 70 points for gesturing and 0 points for labeling, yielding 30 points and 100 points respectively if the listener selected the target object. In the ‘Higher Relative Cost’ condition, speakers were charged 50 points for gesturing and 20 points for labeling, yielding up to 50 points and 80 points respectively. If the listener failed to identify the target object, the speaker nevertheless paid the relevant cost for that message in that condition. As a result of this manipulation, there was a higher relative expected utility for labeling in the ‘Low Relative Cost’ condition than the ‘Higher Relative Cost’ condition. 

### Results



# Experiment 2

Thus far, we have focused on relatively straightforward scenarios to demonstrate that a pressure to communicate successfully in the moment can lead speakers to trade-off between gesture and speech sensibly. However, critical to these repeated interactions is the ability to learn about an interlocutor and potentially influence their learning. In Experiment 2, participants were told about a third type of message: using both gesture and speech within a single trial to effectively teach the listener an object-label mapping. This strategy necessitates making inferences about the listener's knowledge state, so in Experiment 2 we induced knowledge asymmetries between speaker and listner. As in Experiment 1, pariticpants are trained and tested on 9 novel objects to establish their own knowledge, but in Experiment 2 we also manipulated how much training they thought their partner received. Using these manipulations, we aimed to experimentally determine the circumstances under which richly-structured input emerges, without an explicit pedagogical goal.

## Method
```{r e2-data}
all_data <- read_csv(here("data/1.30_turk_exp.csv"))

n_Ps <- all_data %>% 
  summarise(n = n_distinct(ldf_num)) %>%
  pull()

n_manipFail <- all_data %>% 
  filter(manipFail == 1) %>% 
  summarise(n = n_distinct(ldf_num)) %>%
  pull()

n_engLabels <- all_data %>% 
  filter(manipFail == 0 & usedEnglishLabels == 1) %>% 
  summarise(n = n_distinct(ldf_num)) %>%
  pull()
```

### Participants
`r n_Ps` participants were recruited though Amazon Mechanical Turk and received $1 for their participation. Data from `r n_manipFail` participants were excluded from subsequent analysis for failing the critical manipulation check and a further `r n_engLabels` for producing pseudo-English labels (e.g., 'pricklyyone'). The analyses reported exclude the data from those participants, but all analyses were also conducted without excluding any participants and all patterns hold (*ps < 0.05*).

### Design and Procedure

In Experiment 2, training and pretest were identical to Experiment 1 protocols. Participants were again randomly assigned to one of two utlity conditions: 'Higher Relative Cost' and 'Low Relative Cost.' These conditions were matched to our Experiment 1 design, outlined above.

```{r exp_screenshot,set.cap.width=T, num.cols.cap=1, fig.cap = "Screenshot of speaker view during gameplay."}
img <- png::readPNG("figs/exp_screenshot.png")
# img <- png::readPNG(here("figs/exp_screenshot.png"))
grid::grid.raster(img)
```

In Experiment 2, we also manipulated participants' expectations about their partner's knowledge to explore the role of knowledge asymmetries. Prior to beginning the game, participants were told how much exposure their partner had to the lexicon and also that they would be asked to discuss each object three times. Across 3 between subjects conditions, participants were told that their partner had either no experience with the lexicon, had the same experience as the speaker, or had twice the experience of the speaker. As a manipulation check, participants were then asked to report their partner's level of exposure, and were corrected if they answer incorrectly. Gameplay then proceeded as in Experiment 1, though listener behavior was programmed to match their supposed knowledge state. 

Listeners were programmed with starting knowledge states initialized accordingly. Listeners with no exposure began the game with knowledge of 0 object-label pairs. Listeners with the same exposure of the speaker began with knowledge of five object-label pairs (3 high frequency, 1 mid frequency, 1 low frequency), based the average retention rates found previously. Lastly, the listener with twice as much exposure as the speaker began with knowledge of all nine object-label pairs. If the speaker produced a label, the listener was programmed to consult their own knowledge of the lexicon and check for similar labels (selecting a known label with a Levenshtein edit distance of two or fewer from the speaker's production), or select among unknown objects if no similar labels are found.

Critically, participants were told about a third type of possible message using both gesture and speech within a single trial to effectively teach the listener an object-label mapping. This action directly mirrors the multi-modal reference behavior from our corpus data-- it presents the listener with an information-rich, potentially pedagogical learning moment. In order to produce this teaching behavior, speakers had to pay the cost of producing both cues (i.e. both gesture and speech). Note that, in all utility conditions, teaching yielded participants 30 points (compared with the much more beneficial strategy of speaking which yielded 100 points or 80 points across our two utility manipulations). Listeners could integrate new words into their knowledge of the lexicon if taught, and check taught labels on subsequent trials. 

<!-- Our communicative game was designed to reward in-the-moment communication, and thus teaching required the speaker pay a high cost upfront. However, rational communicators may understand that if one is accounting for future trials, paying the cost upfront to teach the listener allows a speaker to use a less costly message strategy on subsequent trials (namely, speech). -->
<!-- Incorporating teaching means that our speaker must also reason about their interlocutor’s knowledge state more explicitly, in order to make rational decisions about what to teach and when. To address this added dimension, -->


<!-- If the speaker produced a label, the listener was programmed to consult their own knowledge of the lexicon and select the closest known reference (selecting a label with a levenshtein edit distance of two or fewer from the speaker's production). If there was not a similar known label (i.e. edit distance > 2), the lsitener was programmed to select randomly among unknown objects. If the speaker gestured (or taught by producing gesture and a label), the listener was programmed to always select the gestured object. After a teaching trial, listeners integrated the taught object-label mapping into their set of candidate word-meanings when evaluating subesquent messages.  -->

Crossing our 2 between-subjects manipulations yielded 6 conditions (2 utility manipulations: ‘Low Relative Cost’ and ‘Higher Relative Cost’; and 3 levels of partner’s exposure: None, Same, Double), with 80 participants in each condition. We expected to find results that mirrored our corpus findings such that rates of teaching would be higher when there was an asymmetry in knowledge where the speaker knew more (None manipulation) compared with when there was equal knowledge (Same manipulation) or when the listener was more familiar with the language (Double manipulation). We expected that participants would also be sensitive to our utility manipulation, such that rates of labeling and teaching would be higher in the ‘Low Relative Cost’ conditions than the other conditions.

## Results
```{r e2-results}
filtered_data <- all_data %>%
  filter(toBeDropped != 1) %>%
  mutate(base = logit(1/3),
         appearanceNumeric = as.numeric(as.factor(appearance)),
         partnersExposure = factor(partnersExposure, 
                                   levels = c("None", "Same", "Perfect")),
         partnersExposureNumeric = as.numeric(partnersExposure) - 1)

learningByExposure <- filtered_data %>% 
  group_by(ldf_num, condition, partnersExposure, realLabel, exposureRate) %>%
  summarize(testCorrect = first(testCorrect))

descriptives <- learningByExposure %>%
  group_by(ldf_num) %>%
  summarize(sum = sum(testCorrect)) %>%
  summarize(meanK = mean(sum), sdK = sd(sum))

gm_known <- glmer(testCorrect ~ exposureRate + (exposureRate|ldf_num) +
                    (1|realLabel),
            data = learningByExposure,
      family=binomial) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = printp(p.value))

exposure_estimate <- gm_known %>%
  filter(term == "exposureRate") %>%
  pull(estimate)

exposure_statistic <- gm_known %>%
  filter(term == "exposureRate") %>%
  pull(statistic)

exposure_p <- gm_known %>%
  filter(term == "exposureRate") %>%
  pull(p.value)


```

As an initial check of our exposure manipulation, we fist a logistic regression predicting accuracy at test from a fixed effect of exposure rate and random intercepts and slopes of exposureRate by participant as well as random intercepts by item. We found a reliable effect of exposure rate, indicating that participants were better able to learn items that appear more frequently in training ($\beta =$  `r exposure_estimate`, $t =$ `r exposure_statistic`, $p$ `r exposure_p`). On average, participants knew at least 6 of the 9 words in the lexicon (mean = `r descriptives$meanK`, sd = `r descriptives$sdK`).

### Gesture-Speech Tradeoff.

```{r e2-models, eval = FALSE}
gm_label <- glmer((method == 'label') ~ partnersExposureNumeric * exposureRate + 
                    partnersExposureNumeric * appearanceNumeric +
                    condition + 
                    (1| ldf_num) + (1|realLabel),
            data = filtered_data,
            family = binomial, offset = base) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = printp(p.value))

label_2nd_beta <- round(fixef(gm_label)["appearanceSecond"], digits=2)
label_3rd_beta <- round(fixef(gm_label)["appearanceThird"], digits=2)
label_pExpPerf_beta <- round(fixef(gm_label)["partnersExposurePerfect"], digits=2)
label_pExpSame_beta <- round(fixef(gm_label)["partnersExposureSame"], digits=2)
label_freq2_beta <- round(fixef(gm_label)["exposureRate2"], digits=2)
label_freq4_beta <- round(fixef(gm_label)["exposureRate4"], digits=2)
label_cond80_beta <- round(fixef(gm_label)["condition80_50"], digits=2)


gm_click <- glmer((method == 'click') ~ partnersExposureNumeric * exposureRate + 
                    partnersExposureNumeric * appearanceNumeric +
                    condition + 
                    (1| ldf_num) + (1|realLabel),
            data = filtered_data,
            family = binomial, offset = base) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = printp(p.value))

click_2nd_beta <- round(fixef(gm_click)["appearanceSecond"], digits=2)
click_3rd_beta <- round(fixef(gm_click)["appearanceThird"], digits=2)
click_pExpPerf_beta <- round(fixef(gm_click)["partnersExposurePerfect"], digits=2)
click_pExpSame_beta <- round(fixef(gm_click)["partnersExposureSame"], digits=2)
click_freq2_beta <- round(fixef(gm_click)["exposureRate2"], digits=2)
click_freq4_beta <- round(fixef(gm_click)["exposureRate4"], digits=2)
click_cond80_beta <- round(fixef(gm_click)["condition80_50"], digits=2)

# <!-- To determine how gesture and speech are trading off across conditions, we  looked at a mixed effects logistic regression to predict whether speakers chose to produce a label during a given trial as a function of the exposure rate, object instance in the game (first, second, or third), utility manipulation, and partner manipulation. A random subjects effects term was included in the model. There was a significant effect of exposure rate such that there was more labeling for objects with two exposures (*B =* `r label_freq2_beta`, *p < 0.0001*) or with four exposures (*B =* `r label_freq4_beta`, *p < 0.0001*), compared with objects seen only once at training. Compared with the first instance of an object, speakers were significantly more likely to produce a label on the second appearance (*B =* `r label_2nd_beta`, *p < 0.01*) or third instance of a given object (*B =* `r label_3rd_beta`, *p < 0.0001*). Participants also modulated their communicative behavior on the basis of the utility manipulation and our partner exposure manipulation.  Speakers in the Low Relative Cost condition produced significantly more labels than participants in the Higher Relative Cost condition (*B =* `r label_cond80_beta`, *p < 0.001*). Speakers did more labeling with more knowledgeable partners; compared with the listener with no exposure, there were significantly higher rates of labeling in the same exposure  (*B =* `r label_pExpSame_beta`, *p < 0.0001*) and double exposure conditions (*B =* `r label_pExpPerf_beta`, *p < 0.001*).   -->

```

```{r speech_gesture, fig.height = 3, fig.width = 4.25, set.cap.width=T, num.cols.cap=1, fig.cap = "Speaker communicative method choice as a function of exposure and the utility manipulation."}
# Speaker communicative method choice as a function of exposure and the utility manipulation. Data are taken from the Double Exposure manipulation. Rates of teaching were minimal and are not shown.

modality.colors <- c(speak = "#e8250b", point = "#1f11e0", teach ="#54a832", speech = "#e8250b", gesture = "#1f11e0", both ="#54a832", label = "#e8250b", click = "#1f11e0", label_click ="#54a832")

seprop <- function(props) {
  mean_prop = mean(props)
  sqrt( (mean_prop*(1 - mean_prop)) / length(props))
}

###wrangling data for plots by exposure
prop_methods_exposures <- all_data %>%
  filter(toBeDropped != 1) %>%
  ungroup() %>%
  select(condition,ldf_num, partnersExposure, exposureRate,method) %>%
  group_by(condition, ldf_num,partnersExposure, exposureRate, method) %>%
  summarise(n = n()) %>%
  group_by(condition, ldf_num,partnersExposure, exposureRate) %>%
  mutate(n = n/sum(n)) %>%
  ungroup() %>%
  mutate(method = as.factor(method)) %>%
  tidyr::complete(nesting(condition, ldf_num, partnersExposure), exposureRate, method, fill = list(n = 0)) %>%
  group_by(condition, partnersExposure, method) %>%
  summarise(mean = mean(n), se = seprop(n)) %>%
  ungroup()


label_data <- tibble(exposureRate = 1.25, condition = "Low Relative Cost",
                     method = c("gesture", "speech"), mean = c(.2, .8))

label_data <- label_data %>% mutate(condition = factor(condition, 
                            levels=c("Low Relative Cost","Higher Relative Cost"), ordered=TRUE))

#Exp., Plot 1., gesture speech tradeoff
plot_prop_methods_exposures <- prop_methods_exposures %>% 
  mutate(condition = ifelse(condition=="100_30", "Low Relative Cost", "Higher Relative Cost"),
      partnersExposure = factor(partnersExposure, levels = c("None", "Same", "Perfect")),
      condition = factor(condition, 
                            levels=c("Low Relative Cost","Higher Relative Cost"), ordered=TRUE),
       #  exposureRate = as.factor(exposureRate),
         method = factor(method, levels = c("click", "label", "label_click"),
                         labels = c("gesture", "speech", "teach")))# %>%
  #filter(method != "label_click") 

plot_prop_methods_exposures %>%
  ggplot(aes(x=partnersExposure, y=mean, color = method, label = method,
             group=interaction(method,condition))) +
  facet_wrap(~ condition) +
  #geom_point(size=1.5,position=position_dodge(.25)) +
  geom_line(  size=.9,     
            position=position_dodge(.25)) +
  geom_pointrange(aes(ymax = mean + se,
    ymin = mean - se),position=position_dodge(.25)) +
  coord_cartesian(ylim=c(0,1)) +
  #facet_grid(method ~ .) +
  labs(y="Proportion of Trials", x="Exposure Rate During Training") +
  scale_color_manual(values=c(modality.colors),  name = "Method")+
    theme(legend.position = "none",
        legend.key.size = unit(.8, 'lines'),
        legend.text=element_text(size=7),
        legend.title=element_text(size=8)) #+
  #  geom_text(data = label_data)
```

Figure \ref{fig:exp_speech_gesture} illustrates the gesture-speech tradeoff pattern in the Double Exposure condition (as there was minimal teaching in that condition, so the speech-gesture trade-off is most interpretable). The effects on gesture mirror those found for labeling and are thus not included for brevity (*ps < 0.01*). Note that these effects cannot be explained by participant knowledge; all patterns above hold when looking *only* at words known by the speaker at pretest (*ps < 0.01*). Further, these patterns directly mirror previous corpus analyses demonstrating the gesture-speech tradeoff in naturalistic parental communicative behaviors, where lexical knowledge is likely for even the least frequent referent [see @yurovsky2018].  

### Emergence of Teaching.

Thus far, we have focused on relatively straightforward scenarios to demonstrate that a pressure to communicate successfully in the moment can lead speakers to trade-off between gesture and speech sensibly. Next, we turn to the emergence of teaching behavior.

```{r}

gm_teach <- glmer((method=='label_click') ~ appearance +
                partnersExposure +
                condition +
                exposureRate +
               (1|ldf_num),
            data=all_data %>% filter(toBeDropped==0),
      family=binomial)

teach_pExpPerf_beta <- round(fixef(gm_teach)["partnersExposurePerfect"], digits=2)
teach_pExpSame_beta <- round(fixef(gm_teach)["partnersExposureSame"], digits=2)
teach_freq2_beta <- round(fixef(gm_teach)["exposureRate2"], digits=2)
teach_freq4_beta <- round(fixef(gm_teach)["exposureRate4"], digits=2)
teach_cond80_beta <- round(fixef(gm_teach)["condition80_50"], digits=2)
teach_2nd_beta <- round(fixef(gm_teach)["appearanceSecond"], digits=2)
teach_3rd_beta <- round(fixef(gm_teach)["appearanceThird"], digits=2)

```

<!-- In line with our hypotheses, a mixed effects logistic regression predicting whether or not teaching occurred on a given trial revealed that teaching rates across conditions depend on all of the same factors that predict speech and gesture (see Figure \ref{fig:exp_teach}). There was a significant positive effect of initial training on the rates of teaching, such that participants were more likely to teach words with two exposures (*B =* `r teach_freq2_beta`, *p < 0.05*) and four exposures (*B =* `r teach_freq4_beta`, *p < 0.05*), compared with words seen only once at training. There was also a significant effect of the utility manipulation such that being in the Low Relative Cost condition predicted higher rates of teaching than being in the Higher Relative Cost condition (*B =* `r teach_cond80_beta`, *p < 0.001*), a rational response considering teaching allows one to use a less costly strategy in the future and that strategy is especially superior in the Low Relative Cost condition.  -->

<!-- We found an effect of partner exposure on rates of teaching as well: participants were significantly more likely to teach a partner with no prior exposure to the language than a partner with the same amount of exposure as the speaker (*B =* `r teach_pExpSame_beta`, *p < 0.0001*) or double their exposure (*B =* `r teach_pExpPerf_beta`, *p < 0.0001*). The planned utility of teaching comes from using another, cheaper strategy (speech) on later trials, thus the expected utility of teaching should decrease when there are fewer subsequent trials for that object, predicting that teaching rates should drop dramatically across trials for a given object. Compared with the first trial for an object, speakers were significantly less likely to teach on the second trial (*B =* `r teach_2nd_beta`, *p < 0.0001*) or third trial (*B =* `r teach_3rd_beta`, *p < 0.0001*). -->

## Discussion 

As predicted, the data from our paradigm corroborate our findings from the corpus analysis, demonstrating that pedagogically supportive behavior emerges despite the initial cost when there is an asymmetry in knowledge and when speech is less costly than other modes of communication. While this paradigm has stripped away much of the interactive environment of the naturalistic corpus data, it provides important proof of concept that the structured and tuned language input we see in those data could arise from a pressure to communicate. The paradigm’s clear, quantitative predictions also allow us to build a formal model to predict our empirical results.

```{r exp_teach, cache=TRUE, fig.height = 3, fig.width = 4.25, set.cap.width=T, num.cols.cap=1, fig.cap = "Rates of teaching across the 6 conditions, plotted by how many times an object had been the target object."}

prop_methods_teaching <- all_data %>%
  filter(toBeDropped != 1) %>%
  select(condition,ldf_num, partnersExposure, appearance,method) %>%
  group_by(condition, ldf_num,partnersExposure, appearance, method) %>%
  summarise(n = n()) %>%
  group_by(condition, ldf_num,partnersExposure, appearance) %>%
  mutate(n = n/sum(n)) %>%
  ungroup() %>%
  mutate(method = as.factor(method)) %>%
  tidyr::complete(nesting(condition, ldf_num, partnersExposure), appearance, method, fill = list(n = 0)) %>%
  group_by(condition, partnersExposure, appearance, method) %>%
  summarise(mean = mean(n), se = seprop(n)) 

plot_prop_methods_teaching <- prop_methods_teaching %>%
  ungroup() %>%
  mutate(condition = factor(condition, labels=c('Low Relative Cost', 'Higher Relative Cost')),
         partnersExposure = factor(partnersExposure, levels = c("None", "Same", "Perfect"),
                                   labels = c("None", "Same Amt", "Twice Amt"))) %>%
  filter(method=="label_click") 



teach_label_data <- tibble(appearance = c(1.5, 1, 1), condition = "Low Relative Cost", 
                     partnersExposure = c("None", "Same Amt", "Twice Amt"),
                     mean = c(.3, .165, .005))

teach_label_data <- teach_label_data %>% mutate(condition = factor(condition, 
                            levels=c("Low Relative Cost","Higher Relative Cost"), ordered=TRUE))

plot_prop_methods_teaching %>%
  ggplot(aes(x=appearance, y=mean, label=partnersExposure, color = partnersExposure)) +
  geom_point(aes(y=mean), size=2.25, position=position_dodge(.25)) +
  geom_line(aes(x=appearance, y=mean, group=partnersExposure, color=partnersExposure), 
            size=1, position=position_dodge(.25)) +
  geom_linerange(aes(ymax = mean + se,
    ymin = mean - se, color=partnersExposure), position=position_dodge(.25)) +
  facet_grid(. ~ condition) +
  labs(y="Proportion of Teaching Trials", x="Object Instance during Game") +
  coord_cartesian(ylim=c(0,.4)) +
  scale_color_manual(values=c("#87d868", "#54a832", "#2c6d12"), name = "Partner's Exposure") +
  theme(legend.position = "none",
        legend.key.size = unit(.8, 'lines'),
        legend.text=element_text(size=7),
        legend.title=element_text(size=8),
        axis.text.x = element_text(angle = 35, hjust = 1)) + 
  geom_text(data = teach_label_data, size = 2.5)
```

The results from this experiment are qualitatively consistent with a model in which participants make their communicative choices to maximize their expected utility from the reference game. We next formalize this model to determine if these results are predicted quantitatively as well.

\newcommand{\E}[1]{\mathbb{E}\left[ #1 \right]}

# Model: Communication as planning

In order to model when people should speak, point, or teach, we begin from the problem of what goal people are trying to solve [@marr1982]. Following a long history of work in philosophy of language, we take the goal of communication to be causing an action in the world by transmitting some piece of information to one's conversational partner [e.g. @wittgenstein1953; @austin1975]. If people are near-optimal communicators, they should choose communicative signals that maximize the probability of being understood while minimizing the cost of producing the signal [@grice1975; @clark1996]. In the special case of reference, solving this problem amounts to producing the least costly signal that correctly specifies one's intended target referent in such a way that one's conversational partner can select it from the set of alternative referents.

Recently, @frank2012 developed the Rational Speech Act framework--a formal instantiation of these ideas. In this model, speakers choose from a set of potential referential expressions in accordance to a utility function that maximizes the probability that a listener will correctly infer their intended meaning while minimizing the number of words produced. This framework has found successful application in a variety of linguistic applications such as scalar implicature, conventional pact formation, and production and interpretation of hyperbole [@goodman2016; see also related work from @franke2013]. These models leverage recursive reasoning--speakers reasoning about listeners who are reasoning about speakers--in order to capture cases in which the literal meaning and the intended meaning of sentences diverge.

To date, this framework has been applied primarily in cases where both communicative partners share the same linguistic repertoire, and thus communicators know their probability of communicating successfully having chosen a particular signal. This is a reasonable assumption for pairs of adults in contexts with shared common ground. But what if partners do not share the same lingusitic repertoire, and in fact do not know the places where their knowledge diverges? In this case, communicators must solve two problems jointly: (1) Figure out what their communicative partner knows, and (2) produce the best communicative signal they can given their estimates of their partner's knowledge. If communicative partners interact repeatedly, these problems become deeply intertwined: Communicators can learn about each-other's knowledge by observing whether their attempts to communicate succeed. For instance, if a communicator produces a word that identifies their intended referent, but their partner fails to select that referent from among the set of objects, they can infer that their partner must not share their understanding of this word. They might then choose not to use language to refer to this object in the future, but choose to point to it instead.

Critically, communicators can also change each-other's knowledge. When a communicator both points to an object and produces a linguistic label, they are in effect teaching their partner the word that they use to refer to this object. While this this behavior is costly in the moment, and no more referentially effective than pointing alone, it can lead to more efficient communication in the future--instead of pointing to this referent forever more, communicators can now use the linguistic label they both know they share. This behavior naturally emerges from a conception of communication as planning: Communicators' goal is to choose a communicative signal today that will lead to efficient communication not just in the present moment, but in future communications as well. If they are likely to need to refer to this object frequently, it is worth it to be inefficient in this one exchange in order to be more efficient future. In this way, pedagogically supportive behavior can emerge naturally from a model with no explicit pedagogical goal. In the following section, we present a formal instantiation of this intuitive description of communication as planning and show that it accounts for the behavior we observed in our experiments.

Alternatively, pedogically-supportive input could emerge from an explicit pedagogical goal. @shafto2014 have developed an framework of rational pedagogy built on the same recursive reasoning principles as in the Rational Speech Act Framework: Teachers aim to teach a concept by choosing a set of examples that would maximize learning for students who reason about the teachers choices as attempting to maximize their learning. @rafferty2016 et al expanded framework to sequential teaching, in which teachers use students in order to infer what they have learned and choose the subsequent example. In this case, teaching can be seen as a kind of planning where teachers should choose a series of examples that will maximize students learning but can change plans if an example they thought would be too hard turns out too easy--or vice-versa. In the case of our reference game, this model is indistinguishable form a communicator seeks to maximize communicative success but is indifferent to communicative cost. This model makes poor predictions about parents' behavior in our corpus, and also adults' behavior in our experiments, but we return to it in the subsequent section to consider how differences in parents' goals and differences in children's learning contribute to changes in the rate of language acquisition.

## Formal Model


```{r model-data, eval = FALSE}
# empiricalVocabs_1.30 <- all_data %>%
#   group_by(ldf_num, condition, partnersExposure, realLabel, exposureRate, trueClickPoints, trueLabelPoints) %>%
#   summarize(testCorrect=first(testCorrect)) %>%
#   rename(exposures=exposureRate,
#          label= realLabel) %>%
#   ungroup() %>%
#   mutate(known = ifelse(testCorrect == 1, TRUE, FALSE),
#          partnersExposure = ifelse(partnersExposure=='Perfect', 2,
#                                    ifelse(partnersExposure=='None', 0, 1)),
#          condition=as.factor(condition))  %>%
#   mutate(speechCost = 100-trueLabelPoints,
#           pointCost = 100-trueClickPoints)%>%
#   as.data.frame()

#lines below will run the model and clean its outputted predictions
  #for ease+efficiency, we will load a stored copy of the models predictions instead.
#library(rwebppl)
#outcomes_exp_1.30<-webppl(program_file = "wppl/speaker.wppl", data=empiricalVocabs_1.30, data_var = "empiricalVocabs")

# gamePredictions_1.30 <-outcomes_exp_1.30 %>%
#   unnest(predictions,  gameTrials) %>%
#     cbind(appearance= rep.int(c(1,2,3), nrow(.)/3))
# all_data_model <- all_data %>%
#   filter(ldf_num %in% gamePredictions_1.30$me) %>%
#   select(-partnersExposure) %>%
#   filter(toBeDropped != 1) %>%
#   group_by(ldf_num, exposureRate, targetObjectName) %>%
#   mutate(appearance = if_else(trialnum == min(trialnum), 1,
#                               if_else(trialnum == max(trialnum), 3, 2))) %>%
#   left_join(gamePredictions_1.30, by=c('ldf_num'='me', 'realLabel'='gameTrials', 'appearance')) %>%
#   mutate(method=ifelse(method=='click', "point",
#                         ifelse(method=="label","speak", "teach"))) %>%
#   as.data.frame(.)

all_data_model <- read_csv(here("wppl/1.30.2019_explicit_model.csv")) %>%
  select(-X1, -X1_1)

```

We take as inspiration the idea that communication is a kind of action--e.g. talking is a speech act [@austin1975]. Consequently, we can understand the choice of *which communicative act* a speaker should take as a question of which act would maximize their utility: achieving successful communication while minimizing their cost [@frank2012]. In this game, speakers can take three actions: talking, pointing, or teaching. In this reference game, these Utilities ($U$) are given directly by the rules. Because communication is a repeated game, people should take actions that maximize their Expected Utility ($EU$) over the course of not just this act, but all future communicative acts with the same conversational partner. We can think of communication, then as a case of recursive planning. However, people do not have perfect knowledge of each-other's vocabularies ($v$). Instead, they only have uncertain beliefs ($b$) about these vocabularies that combine their expectations about what kinds of words people with as much linguistic experience as their partner are likely to know with their observations of their partner's behavior in past communicative interactions. This makes communication a kind of planning under uncertainty well modeled as a Partially Observable Markov Decision Process [POMDP, @kaelbling1998].

Optimal planning in a Partially Observable Markov Decision Process involves a cycle of four phases: (1) Plan, (2) Act, (3) Observe, (4) Update beliefs. When people plan, they compute the Expected Utility of each possible action ($a$) by combining the Expected Utility of that action now with the Discounted Expected Utility they will get in all future actions. The amount of discounting ($\gamma$) reflects how people care about success now compared to success in the future. In our simulations, we set $\gamma=.5$ in line with prior work. Because Utilities depend on the communicative partner's vocabulary, people should integrate over all possible vocabularies in proportion to the probability that their belief assigns to that ($\mathbb{E}_{v \sim b}$).
$$
EU\left[a\right | b] = \mathbb{E}_{v \sim b} \left(U(a|v) + \gamma \,\mathbb{E}_{v',o',a'} \,\left( EU\left[a' | b'\right]\right)\right)
$$
Next, people take an action as a function of its Expected Utility. Following other models in the Rational Speech Act framework, we use the Luce Choice Axiom, in which each choice is taken in probability proportional to its exponentiated utility [@frank2012; @luce1959]. This choice rule has a single parameter $\alpha$ that controls the noise in this choice--as $\alpha$ approaches 0, choice is random and as $\alpha$ approaches infinity, choice is optimal. For the results reported here, we set $\alpha = 2$ based on hand-tuning, but other values produce similar results.
$$
P\left(a|b\right) \propto \alpha \, e^{EU[a|b]}
$$

After taking an action, people observe ($o$) their partner's choice--sometimes they pick the intended object, and sometimes they do not. They then update their beliefs about the partner's vocabulary based on this observation. For simplicity, we assume that people think their partner should always select the correct target if they point to it, or if they teach, and similarly should always select the correct target if they produce its label and the label is in their partner's vocabulary. Otherwise, they assume that their partner will select the wrong object. People could of course have more complex inferential rules, e.g. assuming that if their partner does know a word they will choose among the set of objects whose labels they do not know [mutual exclusivity, @markman1988]. Empirically, however, our simple model appears to accord well with people's behavior.
$$
b'(v') \propto P\left(o|v',a\right) \sum_{v \in V}P\left(v'|v,a\right)b\left(v\right)
$$

The critical feature of a repeated communication game is that people can change their partner’s vocabulary. In teaching, people pay the cost of both talking and pointing together, but can leverage their partner’s new knowledge on future trials. Note here that teaching has an upfront cost and the only benefit to be gained comes from using less costly communication modes later. There is no pedagogical goal-- the model treats speakers as selfish agents aiming to maximize their own utilities by communicating successfully.  We assume for simplicity that learning is approximated by a simple Binomial learning model. If someone encounters a word $w$ in an unambiguous context (e.g. teaching), they add it to their vocabulary with probability $p$. We also assume that over the course of this short game that people do not forget--words that enter the vocabulary never leave, and that no learning happens by inference from mutual exclusivity.

$$
P\left(v'|v,a\right)= \begin{cases} 
1 & \text{if } v_{w} \in v \& v'\\ 
p & \text{if } v_{w} \notin v \& a = \text{point+talk}\\ 
0 & otherwise\end{cases}
$$

The final detail is to specify how people estimate their partner's learning rate ($p$) and initial vocabulary ($v$). We propose that people begin by estimating their own learning rate by reasoning about the words they learned at the start of the task: Their $p$ is the rate that maximizes the probability of them having learned their initial vocabularies from the trials they observed. People can then expect their partner to have a similar $p$ [per the "like me" hypothesis, @meltzoff2005]. Having an estimate of their partner's $p$, they can estimate their vocabulary by simulating their learning from the amount of training we told them their partner had before the start of the game. 

```{r model-fit, eval = FALSE}
prop_methods <- all_data_model %>%
  filter(toBeDropped != 1) %>%
  ungroup() %>%
  select(condition,ldf_num, partnersExposure, appearance,method,predictions) %>%
  # select(condition,ldf_num, partnersExposure, appearance,method) %>%
  gather(isEmpirical, method, -condition, -partnersExposure, -ldf_num, -appearance) %>%
  group_by(condition, ldf_num,partnersExposure, appearance,isEmpirical, method) %>%
  summarise(n = n()) %>%
  group_by(condition, ldf_num,partnersExposure, isEmpirical, appearance) %>%
  mutate(n = n/sum(n)) %>%
  ungroup() %>%
  mutate(method = as.factor(method)) %>%
  tidyr::complete(nesting(condition, ldf_num, partnersExposure), isEmpirical, appearance, method, fill = list(n = 0)) %>%
  group_by(condition, partnersExposure, method, isEmpirical, appearance) %>%
  summarise(mean = mean(n), se = seprop(n))


###direct plot of model fit
model_fit <- prop_methods %>%
  ungroup() %>%
  mutate(isEmpirical= ifelse(isEmpirical=='method', 'empirical', isEmpirical)) %>%
  unite(mean_new, mean, se) %>%
  spread(isEmpirical, mean_new) %>% 
  separate(empirical, c('mean_emp', 'se_emp'), '_') %>%
  separate(predictions, c('mean_pred', 'se_pred'), '_') %>%
  mutate_if(grepl('_', names(.)), as.numeric) %>%
  rename(mean_pred_plot=mean_pred,
         se_pred_plot=se_pred)


model_corr <- model_fit %>%
  summarise(cor = round(cor(mean_emp, mean_pred_plot), digits=2))

model_corr_test <- cor.test(model_fit$mean_emp, model_fit$mean_pred_plot)
```

## Model Results

The fit between our model’s predictions and our empirical data from our reference game study on Amazon Turk can be seen in Figure \ref{fig:model_fit}. The model outputs trial-level action predictions (e.g., “speak”) for every speaker in our empirical data. These model outputs were aggregated across the same factors as the empirical data: modality, appearance, partner’s exposure, and utility condition. We see a significant correlation of our model predictions and our empirical data (*r = `r #model_corr`, p<0.0001*). Our model provides a strong fit for these data, supporting our conclusion that richly-structured language input could emerge from in-the-moment pressure to communicate, without a goal to teach.

```{r model_fit, fig.env = "figure", fig.pos = "H", fig.height=3,  fig.width=4, fig.cap = "Fit between model predictions and empirical data.", eval = FALSE}

#plot of model predictions vs empirical game data
model_fit %>%
  ggplot(aes(x=mean_pred_plot, y=mean_emp, color=method,
             shape=as.factor(partnersExposure),group=appearance,
             alpha=condition,
             label = appearance)) +
  geom_point(aes(size=as.factor(appearance))) +
  geom_abline(intercept =0, slope=1) +
  geom_linerange(aes(ymax = mean_emp + se_emp,
                     ymin = mean_emp - se_emp),
                 alpha=.4) +
  geom_errorbarh(aes(xmin = mean_pred_plot - se_pred_plot,
                     xmax = mean_pred_plot + se_pred_plot),
                 alpha = .4)+
  # geom_text(aes(label=appearance),hjust=0, vjust=0) + 
  # facet_grid(.~condition)+
  coord_cartesian(xlim=c(0,.8), ylim=c(0,.8)) + 
  labs(y="Empirical Proportion of Trials", x="Model Predictions") +
  scale_color_manual(values=modality.colors, name='Modality', labels=c("Gesture", "Speech", "Teach")) +
  scale_shape_manual(values=c(15,17,16),name="Partner's Exposure", labels=c("None", "Same Amount", "Twice as Much")) + 
  scale_alpha_manual(values=c(.4, 1), name="Utility Condition", labels=c("Low Relative Cost", "Higher Relative Cost")) +
  scale_size_manual(values=c(1,2,3), name="Appearance", labels=c("First", "Second", "Third"))+
  theme(legend.direction = "vertical", 
        legend.position = "right",
        legend.key.size = unit(.15, 'lines'),
        legend.title=element_text(size=7),
        legend.text=element_text(size=6),
        legend.spacing = unit(0, "in"),
        legend.box.spacing = unit(0, "in"))
```


# Consequences for Learning

In the model and experiments above, we asked whether the pressure to communicate successfully with a linguistically-naive partner would lead to pedagogically supportive input. These results confirmed its' sufficiency: As long as linguistic communication is less costly than deictic gesture, speakers should be motivated to teach in order to reduce future communicative costs. Further, the strength of this motivation is modulated by predictable factors (speaker's linguistic knowledge, listener's linguistic knowledge, relative cost of speech and gesture, learning rate, etc.), and the strength of this modulation is well predicted by a rational model of planning under uncertainty about listner's vocabulary.

In this final section, we take up the consequences of communicatively-motivated teaching for the listener. To do this, we adapt a framework used by @blythe2010 and colleagues to estimate the learning times for an idealized child learning language under a variety of models of both the child and their parent. We come to these estimates by simulating exposure to successive communicative events, and measuring the probability that successful learning happens after each event. The question of how different models of the parent impact the learner can then be formalized as a question of how much more quickly learning happens in the context of one model than another.

We consider three parent models:

1. *Teacher* - under this model, we take the parents' goal to be maximizing the child's linguistic development. Each communicative event in this model consists of an ostensive labelling event (Note: this model is equivalent to a *Communicator* that ignores communicative cost).

2. *Communicator* - under this model, we take the parents' goal to be maximizing communicative success while minimizing communicative cost. This is the model we explored in the previous section.

3. *Indifferent* - under this model, the parent produces a linguistic label in each communicative event regardless of the child's vocabulary state. (Note: this model is equivalent to a *Communicator* who ignores communicative success).

SOME STUFF ABOUT CROSS SITUATIONAL LEARNING

One important point to note is that we are modeling the learning of a single word rather than the entirety of a multi-word lexicon [as in @blythe2010]. Although learning times for each word could be independent, an important feature of many models of word learning is that they are not [@yurovsky2014; @yu2008; @frank2009; although c.f. @mcmurray2007]. Indeed, positive synergies across words are predicted by the majority of models and the impact of these synergies can be quite large under some assumptions about the frequency with which different words are encountered [@reisenauer2013]. We assume independence primarily for pragmatic reasons here--it makes the simulations significantly more tractable (although it is what our experimental participants appear to assume about learners). Nonetheless, it is an important issue for future consideration. Of course, synergies that support learning under a cross-situational scheme must also support learning from communcators and teachers [@markman1988; @frank2009; @yurovsky2013]. Thus, the ordering across conditions should remain unchanged. However, the magnitude of the difference sacross teacher conditions could potentially increase or decrease.

## Method

### Teaching. 
Because the teaching model is indifferent to communicative cost, it engages in ostensive an ostensive labeling (pointing + speaking) on each communicative event. Consequently, learning on each trial occurs with a probability that depends entirely on the learner's learning rate ($P_{k}=p$). Because we do not allow forgetting, the probability that a learner has failed to successfully learn after $n$ trials is equal to the probability that they have failed to learn on each of $n$ successive independent trials (The probabiliy of zero successess on $n$ trials of a Binomial random variable with parameter $p$). The probability of learning after $n$ trials is thus: 

$$ P_k(n) = 1 - \left(1-p\right)^{n}  $$

The expected probability of learning after $n$ trials was thus defined analytically and required no simulation. For comparison to the other models, we computed $P_{k}$ for values of $p$ that ranged from $.1$ to $1$ in increments of $.1$.

### Communication.

To test learner under the communication model, we implemented the same model described in the paper above. However, because our interest was in understanding the relationship between parameter values and learning outcomes rather than inferring the parameters that best describe people's behavior, we made a few simplifying assumptions to allow many runs of the model to complete in a more practical amount of time. First, in the full model above, speakers begin by inferring their own learning parameters ($P_{s}$) from their observations of their own learning, and subsequently use their maximum likelihood estimate as a standin for their listener's learning parameter ($P_{l}$). Because this estimate will converge to the true value in expectation, we omit these steps and simply stipulate that the speaker correctly estimates the listener's learning parameter. 

Second, unless the speaker knows apriori how many times they will need to refer to a particular referent, the planning process is an infinite recursion. However, each future step in the plan is less impactful than the previous step (because of exponential discounting), this infinite process is in practice well approximated by a relatively small number of recursive steps. In our explorations we found that predictions made from models which planned over 3 future events were indistinguishable from models that planned over four or more, so we simulated 3 steps of recursion[^1]. Finally, to increase the speed of the simulations we re-implemented them in the R programming language. All other aspects of the model were identical.

[^1]: It is an intersting empirical question to determine how the level of depth to which that people plan in this and similar games [see e.g. bounded rationality in @simon1991; resource-rationality in @griffiths2015]. This future work is outside the scope of the current project.


### Hypothesis Testing.

The literature on cross-situational learning is rich with a variety of models that could broadly be considered to be "hypothesis testers." In an eliminative hypothesis testing model, the learner begins with all possible mappings between words and objects and prunes potential mappings when they are inconsistent with the data according to some principe. A maximal version of this model relies on the principle that every time a word is heard its referent must be present, and thus prunes any word-object mappings that do not appear on the current trial. This model converges when only one hypothesis remains and is probably the fastest learner when its assumed principle is a correct assumption [@smith2011].

A positive hypothesis tester begins with no hypotheses, and on each trial stores one ore more hypotheses that are consistent with the data, or alternatively strengthens one or more hypotheses that it has already stored that are consistent with the new data. A number of such models have appeared in the literature, with different assumptions about (1) how many hypotheses a learner can store, (2) existing hypotheses are strengthened, (3) how existing hypotheses are pruned, and (4) when the model converges [@siskind1996; @smith2011; @stevens2017; @trueswell2013; @yu2012].

Finally, Bayesian models have been proposed that leverage some of the strengths of both of these different kinds of model, both increasing their confidence in hypotheses consisten with the data on a given learning event and decreasing their confidence in hypotheses inconsistent with the event [@frank2009]. 

Because of its more natural alignment with the learning models we use Teaching and Communication simulations, we implemented a positive hypothesis testing model[^2]. In this model, learners begin with no hypotheses and add new ones to their store as they encounter data. Upon first encountering a word and a set of objects, the model encodes up to $h$ hypothesized word-object pairs each with probability $p$. On subsequent trials, the model checks whether any of the existing hypotheses are consistent with the current data, and prunes any that are not. If no current hypotheses are consistent, it adds up to $h$ new hypotheses each with probability $p$. The model has converged when it has pruned all but the one correct hypothesis for the meaning of a word. This model is most similar to the Propose but Verify model proposed in @trueswell2013, with the exception that it allows for multiple hypotheses. Because of the data generating process, storing prior disconfirmed hypotheses [as in @stevens2017], or incrementing hypotheses consistent with some but not all of the data [as in @yu2012] has no impact on learner and so we do not implement it here. We note also that, as described in @yu2012, hypothesis testing models can mimic the behavior of associative learning models given the right parameter settings [@townsend1990]. 

In contrast to the Teaching and Communication simulations, the behavior of the Hypothesis Testing model depends on which particular non-target objects are present on each naming event. We thus began each simulation by generating a copus of 100 naming events, on each sampling the correct target as well as ($C$-1) competitors from a total set of $M$ objects. We then simulated a hypothesis tester learning over this set of events as described above, and recorded the first trial on which the learner converged (having only the single correct hypothesized mapping between the target word and target object). We repeated this process 1000 times for each simulated combination of $M = (16, 32, 64, 128)$  total objects, $C = (1,2,4,8)$ objects per trial, $h = (1, 2, 3, 4)$ concurrent hypotheses, as the learning rate $p$ varied from $.1$ to $1$ in increments of $.1$. 
 
[^2]: Our choice to focus on hypothesis testing rather than other learning frameworks is purely a pragmatic choice--the learning parameter $p$ in this models maps cleanly onto the learning parameter in our other models. We encourage other researchers to adapt the code we have provided to estimate the long-term learning for other models.


# General Discussion

Across naturalistic corpus data, experimental data, and model predictions, we see evidence that pressure to communicate successfully with a linguistically immature partner could fundamentally structure parent production. In our experiment, we showed that people tune their communicative choices to varying cost and reward structures, and also critically to their partner’s linguistic knowledge–providing richer cues when partners are unlikely to know the language and many more rounds remain. These data are consistent with the patterns shown in our corpus analysis of parent referential communication and demonstrate that such pedagogically supportive input could arise from a motivation to maximize communicative success while minimizing communicative cost– no additional motivation to teach is necessary.  In simulation, we demonstrate that such structure could have profound implications for child language learning, simplifying the learning problem posed by most distributional accounts of language learning.

Accounts of language learning often aim to explain its striking speed in light of the sheer complexity of the language learning problem itself. Many such accounts argue that simple (associative) learning mechanisms alone seem insufficient to explain the rapid growth of language skills and appeal instead to additional explanatory factors, such as the so-called language acquisition device, working memory limitations, word learning biases, etc. (e.g., Chomsky, 1965; Goldowsky & Newport, 1993; Markman, 1990). While some have argued for the simplifying role of language distributions (e.g., McMurray, 2007), these accounts largely focus on learner-internal explanations. For example, Elman (1993) simulates language learning under two possible explanations to intractability of the language learning problem: one environmental, and one internal. He first demonstrates that learning is significantly improved if the language input data is given incrementally, rather than all-at-once (Elman, 1993). He then demonstrates that similar benefits can arise from learning under limited working memory, consistent with the “less-is-more” proposal (Elman, 1993; Goldowsky & Newport, 1993). Elman dismisses the first account arguing that ordered input is implausible, while shifts in cognitive maturation are well-documented in the learner (Elman, 1993); however, our account’s emphasis on changing calibration to such learning mechanisms suggests the role of ordered or incremental input from the environment may be crucial. 

This account is consonant with work in other areas of development, such as recent demonstrations that the infant’s visual learning environment has surprising consistency and incrementality, which could be a powerful tool for visual learning. Notably, research using head mounted cameras has found that infant’s visual perspective privileges certain scenes and that these scenes change across development (Fausey, Jayaraman, & Smith, 2016). In early infancy, the child’s egocentric visual environment is dominated by faces, but shifts across infancy to become more hand and hand-object oriented in later infancy (Fausey et al., 2016). This observed shift in environmental statistics mirrors learning problems solved by infants at those ages, namely face recognition and object-related goal attribution respectively (Fausey et al., 2016). These changing environmental statistics have clear implications for learning and demonstrate that the environment itself is a key element to be captured by formal efforts to evaluate statistical learning (Smith et al., 2018). Frameworks of visual learning must incorporate both the relevant learning abilities and this motivated, contingent structure in the environment (Smith et al., 2018).

By analogy, the work we have presented here aims to draw a similar argument for the language environment, which is also demonstrably beneficial for learning and changes across development. In the case of language, the contingencies between learner and environment are even clearer than visual learning. Functional pressures to communicate and be understood make successful caregiver speech highly dependent on the learner. Any structure in the language environment that is continually suited to changing learning mechanisms must come in large part from caregivers themselves. Thus, a comprehensive account of language learning that can successfully grapple with the infant curriculum (Smith et al., 2018) must explain parent production, as well as learning itself. In this work, we have taken first steps toward providing such an account.

Explaining parental modification is a necessary condition for building a complete theory of language learning, but modification is certainly not a sufficient condition for language learning. No matter how callibrated the language input, non-human primates are unable to acquire language. Indeed, parental modification need not even be a necessary condition for language learning. Young children are able to learn novel words from (unmodified) overheard speech between adults (Foushee & Xu, 2016), although there is reason to think that overheard sources may have limited impact on language learning broadly (e.g., Schniedman & Goldin-Meadow, 2012).  Our argument is that the rate and ultimate attainment of language learners will vary substantially as a function of parental modification, and that describing the cause of this variability is a necessary feature of models of language learning.

### Generalizability and Limitations

Our account aims to think about parent production and child learning in the same system, putting these processes into explicit dialogue. While we have focused on ostensive labeling as a case-study phenomenon, our account should reasonably extend to the changing structure found in other aspects of child-directed speech-- though see below for important limitations to this extension. Some such phenomena will be easily accounted for: aspects of language that shape communicative efficiency should shift in predictable patterns across development.  

While these language phenomena can be captured by our proposed framework, incorporating them will likely require altering aspects of our account and decisions about which alterations are most appropriate. For example, the exaggerated pitch contours seen in infant-directed speech could be explained by our account if we expand the definition of communicative success to include a goal like maintaining attention. Alternatively, one could likely accomplish the same goal by altering the cost and utility structure to penalize loss of engagement. Thus, while this account should generalize to other modifications found in child-directed speech, such generalizations will likely require non-trivial alterations to the extant structure of the framework. 

Of course, not all aspects of language should be calibrated to the child’s language development. Our account also provides an initial framework for explaining aspects of communication that would not be modified in child-directed speech: namely, aspects of communication that minimally effect communicative efficiency. In other words, communication goals and learning goals are not always aligned. For example, children frequently overregularize past and plural forms, producing incorrect forms such as "runn-ed" (rather than the irregular verb "ran") or "foots" (rather than the irregular plural "feet") (citation on overregularization). Mastering the proper tense endings (i.e. the learning goal) might be aided by feedback from parent; however, adults rarely provide corrective feedback for these errors (citation for lack of correction), perhaps because incorrect grammatical forms are often sufficient to allow for successful communication (i.e. the communicative goal). The degree of alignment between communication and learning goals should predict the extent to which a linguistic phenomenon is modified in child-directed speech. Fully establishing the degree to which modification is expected for a given language phenomena will likely require working through a number of limitations in the generalizability of the framework as it stands.

Some aspects of parent production are likely entirely unrepresented in our framework, such as aspects of production driven by speaker-side constraints. Furthermore, our account is formulated primarily around concrete noun learning and future work must address its viability in other language learning problems. We chose to focus on ostensive labeling as a case-study phenomenon because it is an undeniably information-rich cue for young language learners, however ostensive labeling varies substantially across socio-economic status and cross-linguistically (citation for SES + lang ostensive labeling). This is to be expected to the extent that parent-child interaction is driven by different goals (or goals given different weights) across these populations-- variability in goals could give rise to variability in the degree of modification. Nonetheless, the generalizability of our account across populations remains unknown. Indeed, child-directed speech itself varies cross-linguistically, both in its features (citation) and quantity (citation). There is some evidence that CDS predicts learning even in cultures where CDS is qualitatively different and less prevalent than in American samples (Schneidman & Goldin-Meadow, 2012).  Future work is needed to establish the generalizability of our account beyond the western samples studied here.

We see this account as building on established, crucial statistical learning skills-- distributional information writ large and (unmodified) language data from overheard speech are undoubtedly helpful for some learning problems (e.g., phoneme learning). There is likely large variability in the extent to which statistical learning skills drive the learning for a given learning problem. The current framework is limited by its inability to account for such differences across learning problems, which could derive from domain or cultural differences. Understanding generalizability of this sort and the limits of statistical learning will likely require a full account spanning both parent production and child learning. 

A full account that explains variability in modification across aspects of language will rely on a fully specified model of optimal communication. Such a model will allow us to determine both which structures are predictably unmodified, and which structures must be modified for other reasons. Nonetheless, this work is an important first step in validating the hypothesis that language input that is structured to support language learning could arise from a single unifying goal: The desire to communicate effectively.



# Conclusion

# Acknowledgement

The authors are grateful to XX and YY for their thoughtful feedback on this manuscript. The authors are grateful to Madeline Meyers for her work coding referential communication in the coprus data. This research was supported by a James S MacDonnel Foundation Scholars Award to DY.

\newpage

# References


\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
