---
title             : "A communicative framework for early word learning"
shorttitle        : "Communicative Word Learning"

author: 
  - name          : "Benjamin C. Morris"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Department of Psychology, University of Chicago, 5848 S University Ave, Chicago, IL 60637"
    email         : "yurovsky@uchicago.edu"
  - name          : "Daniel Yurovsky"
    affiliation   : "1,2"
    corresponding : no    # Define only one corresponding author


affiliation:
  - id            : "1"
    institution   : "University of Chicago"
  - id            : "1"
    institution   : "Carnegie Mellon University"

author_note: >

abstract: >
  Enter abstract here. Each new line herein must be indented, like this line.
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["reference-game.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
---

```{r load_packages, include = FALSE}
library(papaja)
library(tidyverse)
```

```{r analysis_preferences}
# Seed for random number generation
set.seed(42)
```


# Introduction

Word learning as a statistical inference problem. 

  From Quine on. [@quine1960]

three kinds of uncertainty -- over statistical time and in the moment

constraints, pragmatics, etc deal with uncertainty in the moment

uncertainty over consistent meanings -- priors of some kind to deal with this tenenbaum & xu
[@tenenbaum1999,@xu2007]

statistical co-occurrence structure deals with uncerainty reduction over time
[@siskind1996,@yu2008,@blythe2010,@blythe2016]

these two scales are linked [@frank2009]

linking priors and in the moment scales [@frank2012,@frank2014]

All of the arguments in these domains are about the relative difficulty of these different kinds of problems [@trueswell2013,@smith2014,@yurovsky2014,@yurovsky2015]

but all of this stuff is still about speakers talking to no one! [@tomasello2000, @tomasello2001]

Indeed, it looks like it matters whether speech is to children
  - structural reasons [@aslin1996,]
  - evidence from weisleder, hoff, etc. [@weisleder2013]
  - argument from ruthee about structure of contra evidence from Akhtar [@akhtar2001,@akhtar2005,foushee2016]

In contrast, pedagogical inference -- shafto, bonawitz, etc. [@bonawitz2011,@shafto2012]
  - evidence for some of this kind of stuff from follow-in labeling. tomasello, baldwin, yu
  - but this is probably not what parents are doing most of the time (although c.f. tamis-lemonda) [@tamis-lemonda2017]
  - old arguments from newport, etc. [@newport1977]

An intermediate position: Speakers goal is to communicate
  - @grice1969

reference games and transmission of language
  - @kirby2015
  - @gibson2017
  - @baddeley2009
  
Critically, reference games and information theory (in general) assume that speaker and receiver share the same code

But what if only one person knows the code? In this case, in order to communicate successfully, speakers need to take into account the listener's knowledge of the language
  - evidence for some speaker design
  - brown-schmidt and tanenhaus [@brown-schmidt2008]

In this case, ambiguity will be controlled in part by the speaker's communicative goals, and scale with the listener. 

We show that without any explicit pedagogical goal, can get speaker design in reference games that leads to better learning

A spectrum of models from pedagogical to adversarial. Figure?

# A model of learning and production  

# Brief explanation of the general reference game framework

# Experiments 1 and 2
  speakers adapt to beliefs about points and also speaker knowledge

# Method

### Participants

### Material

### Procedure

### Data analysis

## Results

## Discussion

# Experiments 3 and 4
  this leads to better learning, but not as good as ostension (obviously)

# A model of teaching

# Experiment 5
  teaching!

# Consequences for Learning

In the model and experiments above, we asked whether the pressure to communicate successfully with a linguistically-naive partner would lead to pedagogically supportive input. These results confirmed its' sufficiency: As long as linguistic communication is less costly than deictic gesture, speakers should be motivated to teach in order to reduce future communicative costs. Further, the strength of this motivation is modulated by predictable factors (speaker's linguistic knowledge, listener's linguistic knowledge, relative cost of speech and gesture, learning rate, etc.), and the strength of this modulation is well predicted by a rational model of planning under uncertainty about listner's vocabulary.

In this final section, we take up the consequences of communicatively-motivated teaching for the listener. To do this, we adapt a framework used by @blythe2010 and colleagues to estimate the learning times for an idealized child learning language under a variety of models of both the child and their parent. We come to these estimates by simulating exposure to successive communicative events, and measuring the probability that successful learning happens after each event. The question of how different models of the parent impact the learner can then be formalized as a question of how much more quickly learning happens in the context of one model than another.

We consider three parent models:

1. *Teacher* - under this model, we take the parents' goal to be maximizing the child's linguistic development. Each communicative event in this model consists of an ostensive labelling event (Note: this model is equivalent to a *Communicator* that ignores communicative cost).

2. *Communicator* - under this model, we take the parents' goal to be maximizing communicative success while minimizing communicative cost. This is the model we explored in the previous section.

3. *Indifferent* - under this model, the parent produces a linguistic label in each communicative event regardless of the child's vocabulary state. (Note: this model is equivalent to a *Communicator* who ignores communicative cost).

SOME STUFF ABOUT CROSS SITUATIONAL LEARNING

One important point to note is that we are modeling the learning of a single word rather than the entirety of a multi-word lexicon [as in @blythe2010]. Although learning times for each word could be independent, an important feature of many models of word learning is that they are not [@yurovsky2014;@yu2008;@frank2009; although c.f. @mcmurray2007]. Indeed, positive synergies across words are predicted by the majority of models and the impact of these synergies can be quite large under some assumptions about the frequency with which different words are encountered [@reisenauer2013]. We assume independence primarily for pragmatic reasons here--it makes the simulations significantly more tractable (although it is what our experimental participants appear to assume about learners). Nonetheless, it is an important issue for future consideration. Of course, synergies that support learning under a cross-situational scheme must also support learning from communcators and teachers [@markman1988, @frank2009, @yurovsky2013]. Thus, the ordering across conditions should remain unchanged. However, the magnitude of the difference sacross teacher conditions could potentially increase or decrease.

Method

### Teaching. 
Because the teaching model is indifferent to communicative cost, it engages in ostensive an ostensive labeling (pointing + speaking) on each communicative event. Consequently, learning on each trial occurs with a probability that depends entirely on the learner's learning rate ($P_{k}=p$). Because we do not allow forgetting, the probability that a learner has failed to successfully learn after $n$ trials is equal to the probability that they have failed to learn on each of $n$ successive independent trials (The probabiliy of zero successess on $n$ trials of a Binomial random variable with parameter $p$). The probability of learning after $n$ trials is thus: 

$$ P_k(n) = 1 - \left(1-p\right)^{n}  $$

The expected probability of learning after $n$ trials was thus defined analytically and required no simulation. For comparison to the other models, we computed $P_{k}$ for values of $p$ that ranged from $.1$ to $1$ in increments of $.1$.

### Communication.

To test learner under the communication model, we implemented the same model described in the paper above. However, because our interest was in understanding the relationship between parameter values and learning outcomes rather than inferring the parameters that best describe people's behavior, we made a few simplifying assumptions to allow many runs of the model to complete in a more practical amount of time. First, in the full model above, speakers begin by inferring their own learning parameters ($P_{s}$) from their observations of their own learning, and subsequently use their maximum likelihood estimate as a standin for their listener's learning parameter ($P_{l}$). Because this estimate will converge to the true value in expectation, we omit these steps and simply stipulate that the speaker correctly estimates the listener's learning parameter. 

Second, unless the speaker knows apriori how many times they will need to refer to a particular referent, the planning process is an infinite recursion. However, each future step in the plan is less impactful than the previous step (because of exponential discounting), this infinite process is in practice well approximated by a relatively small number of recursive steps. In our explorations we found that predictions made from models which planned over 3 future events were indistinguishable from models that planned over four or more, so we simulated 3 steps of recursion[^1]. Finally, to increase the speed of the simulations we re-implemented them in the R programming language. All other aspects of the model were identical.

[^1]: It is an intersting empirical question to determine how the level of depth to which that people plan in this and similar games [see e.g. bounded rationality in @simon1991, resource-ratinoality in @griffiths2015]. This future work is outside the scope of the current project.


### Hypothesis Testing.

The literature on cross-situational learning is rich with a variety of models that could broadly be considered to be "hypothesis testers." In an eliminative hypothesis testing model, the learner begins with all possible mappings between words and objects and prunes potential mappings when they are inconsistent with the data according to some principe. A maximal version of this model relies on the principle that every time a word is heard its referent must be present, and thus prunes any word-object mappings that do not appear on the current trial. This model converges when only one hypothesis remains and is provably the fastest learner when its assumed principle is a correct assumption [@smith2011].

A positive hypothesis tester begins with no hypotheses, and on each trial stores one ore more hypotheses that are consistent with the data, or alternatively strengthens one or more hypotheses that it has already stored that are consistent with the new data. A number of such models have appeared in the literature, with different assumptions about (1) how many hypotheses a learner can store, (2) existing hypotheses are strengthened, (3) how existing hypotheses are pruned, and (4) when the model converges [@siskind1996; @smith2011; @stevens2017; @trueswell2013; @yu2012].

Finally, Bayesian models have been proposed that leverage some of the strengths of both of these different kinds of model, both increasing their confidence in hypotheses consisten with the data on a given learning event and decreasing their confidence in hypotheses inconsistent with the event [@frank2009]. 

Because of its more natural alignment with the learning models we use Teaching and Communication simulations, we implemented a positive hypothesis testing model[^2]. In this model, learners begin with no hypotheses and add new ones to their store as they encounter data. Upon first encountering a word and a set of objects, the model encodes up to $h$ hypothesized word-object pairs each with probability $p$. On subsequent trials, the model checks whether any of the existing hypotheses are consistent with the current data, and prunes any that are not. If no current hypotheses are consistent, it adds up to $h$ new hypotheses each with probability $p$. The model has converged when it has pruned all but the one correct hypothesis for the meaning of a word. This model is most similar to the Propose but Verify model proposed in @trueswell2013, with the exception that it allows for multiple hypotheses. Because of the data generating process, storing prior disconfirmed hypotheses [as in @stevens2017], or incrementing hypotheses consistent with some but not all of the data [as in @yu2012] has no impact on learner and so we do not implement it here. We note also that, as described in @yu2012, hypothesis testing models can mimic the behavior of associative learning models given the right parameter settings [@townsend1990]. 

In contrast to the Teaching and Communication simulations, the behavior of the Hypothesis Testing model depends on which particular non-target objects are present on each naming event. We thus began each simulation by generating a copus of 100 naming events, on each sampling the correct target as well as ($C$-1) competitors from a total set of $M$ objects. We then simulated a hypothesis tester learning over this set of events as described above, and recorded the first trial on which the learner converged (having only the single correct hypothesized mapping between the target word and target object). We repeated this process 1000 times for each simulated combination of $M = (16, 32, 64, 128)$  total objects, $C = (1,2,4,8)$ objects per trial, $h = (1, 2, 3, 4)$ concurrent hypotheses, as the learning rate $p$ varied from $.1$ to $1$ in increments of $.1$. 
 
[^2]: Our choice to focus on hypothesis testing rather than other learning frameworks is purely a pragmatic choice--the learning parameter $p$ in this models maps cleanly onto the learnin parameter in our other models. We encourage other researchers to adapt the code we have provided to estimate the long-term learning for other models.

# General Discussion

# Conclusion

# Acknowledgement

The authors are grateful to XX and YY for their thoughtful feedback on this manuscript. This research was supported by a James S MacDonnel Foundation Scholars Award to DY.

\newpage

# References


\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
