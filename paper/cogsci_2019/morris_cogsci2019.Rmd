---
title: "Pressure to communicate across knowledge asymmetries leads to pedagogically supportive language input"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
 \author{Benjamin C. Morris \and Daniel Yurovsky \\
         \texttt{\{benmorris, yurovsky\}@uchicago.edu} \\
        Department of Psychology \\ University of Chicago}
abstract: "Children do not learn language from passive observation of the world, but from interaction with caregivers who want to communicate with them. These communicative exchanges are structured at multiple levels in ways that support support language learning. We argue this pedagogically supportive structure can result from pressure to communicate successfully with a linguistically immature partner. We first characterize one kind of pedagogically supportive structure in a corpus analysis: caregivers provide more information-rich referential communication, using both gesture and speech to refer to a single object, when that object is rare and when their child is young. Then, in an iterated reference game experiment on Mechanical Turk (n = 480), we show how this behavior can arise from pressure to communicate successfully with a less knowledgeable partner. Lastly, we show that speaker behavior in our experiment can be explained by a rational planning model, without any explicit teaching goal. We suggest that caregivers’ desire to communicate successfully may play a powerful role in structuring children’s input in order to support language learning."

#We suggest that caregivers’ desire to communicate successfully may play a powerful role in structuring their language leading to the supportive language input we see in naturalistic data

# "Infants and young children learn better from child-directed speech. Child-directed speech might support learning in part due to communicative pressure: parents must use language that their children understand. Even in the absence of a goal to teach, parents might align their communicative behavior to their child's developing linguistic knowledge. We present longitudinal corpus data of parent-child interaction to demonstrate that parents provide information-rich referential communication, using both gesture and speech during the same reference, when their children are younger and when discussing infrequent objects. To experimentally validate the role of communicative pressure in generating supportive input, we developed a reference game on Mechanical Turk in which participants (n=480) communicated with listeners who had varying levels of expertise in a novel language. Rich, teaching-like communication emerged when the speaker had substantially more knowledge of the lexicon than the listener. Lastly, we show that speaker behavior in this paradigm can be explained by a rational planning model by which speakers attempt to maximize total expected utility over the course of the game. While language is more than reference games, this work validates the hypothesis that communicative pressure alone can lead to the supportive language input we see in naturalistic data."
#     "The abstract should be one paragraph, indented 1/8 inch on both sides,
# in 9 point font with single spacing. The heading Abstract should
# be 10 point, bold, centered, with one line space below it. This
# one-paragraph abstract section is required only for standard spoken
# papers and standard posters (i.e., those presentations that will be
# represented by six page papers in the Proceedings)."
    
keywords:
    "language learning; communication; computational modeling."
    
output: cogsci2016::cogsci_paper
final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb",
                      fig.path='figs/', echo=F, warning=F, cache=F, message=F,
                      sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(feather)
library(tidyverse)
library(lme4)
library(here)

theme_set(theme_classic(base_size = 10))
options(digits=2)
```

# Introduction

One of the most striking aspects of children's language learning is just how quickly they master the complex system of their natural language [@bloom2000]. In just a few short years, children go from complete ignorance to conversational fluency in a way that is the envy of second-language learners attempting the same feat later in life [@newport1990]. What accounts for this remarkable transition?

Distributional learning presents a unifying account of early language learning: where infants come to language acquisition with a powerful ability to learn the latent structure of language from the statistical properties of speech in their ambient environment [@saffran2003]. A number of experiments clearly demonstrate the early availability of such mechanisms and their utility across a range of language phenomena [@saffran2003;@smith2008]. However, there is reason to be suspicious about just how precocious young learners are early in development. For example, infants’ ability to track the co-occurrence information connecting words to their referents appears to be highly constrained by their developing memory and attention systems [@vlach2013; @smith2013]. Further, computational models of these processes show that the rate of acquisition is highly sensitive to variation in environmental statistics [e.g., @vogt2012]. Thus, precocious unsupervised statistical learning appears to fall short of a complete explanation for rapid early language learning.

Even relatively constrained statistical learning could be rescued, however, if caregivers structured their language in a way that simplified the learning problem. Indeed, evidence at a variety of levels-- from speech segmentation to word learning-- suggests that caregivers’ naturalistic communication provides exactly this kind of supportive structure [@thiessen2005; @gogate2000; @tomasello1986]. Under distributional learning accounts, the existence of this kind of structure is a theory-external feature of the world that does not have an independently motivated explanation. Indeed, because of widespread agreement that parental speech is not usually motivated by explicit pedagogical goals, the calibration of speech to learning mechanisms seems a happy accident; parental speech just happens to be calibrated to children’s learning needs. In this work, we take the first steps toward a unifying account of both the child’s learning and the parents’ production: Both are driven by a pressure to communicate successfully [@brown1977].

Early, influential functionalist accounts of language learning focused on the importance of communicative goals [e.g., @brown1977]. Our goal in this work is to formalize the intuitions in these accounts in a computational model, and to test this model against experimental data. We take as the caregiver’s goal the desire to communicate with the child, not about language itself, but instead about the world in front of them. To succeed, the caregiver must produce the kinds of communicative signals that the child can understand and respond contingently, potentially leading caregivers to tune the complexity of their speech as a byproduct of in-the-moment pressure to communicate successfully [@yurovsky2017]. 

To examine this hypothesis, we first analyze parent communicative behavior in a longitudinal corpus of parent-child interaction in the home [@goldin-meadow2014]. We investigate the extent to which parents tune their communicative behavior (focusing on modality– i.e. gesture vs. speech) across their child’s development to align to their child’s developing linguistic knowledge [@yurovsky2016]. We take this phenomenon to be a case study of pedagogically supportive structure in the language environment.

We then experimentally induce this form of structured language input in a simple model system: an iterated reference game in which two players earn points for communicating successfully with each other. Modeled after our corpus data, participants are asked to make choices about which communicative strategy to use (akin to modality choice). In an experiment on Mechanical Turk using this model system, we show that tuned, structured language input can arise from a pressure to communicate. We then show that participant behavior in our game can be explained by a rational planning model that seeks to optimize its total expected utility over the course of the game.


<!-- Specifically, we look at *communicative modality* to analyze parent's use of multi-modal reference-- using *both* gesture and a verbal label to indicate the same referent, in the same instance. These instances might be particularly powerful learning opportunities for young children because they provide a verbal label in the presence of a highly disambuiguting gestural cue (e.g., pointing), which might greatly reduce referential uncertainty (**gesture-speech citation**). -->


# Corpus Analysis
```{r}
# Plot of Referential Communication from LDP Corpus Data
corpus_data <- read_feather(("data/coded_responses.feather"))

modality_data <- corpus_data %>%
  group_by(person, age, subj, freq_cut, rank, freq, referent, modality, chat) %>%
  summarise(n = n()) %>%
  spread(modality, n, fill = 0)


plot_data <- modality_data %>%
  group_by(person, age, subj, freq, referent) %>%
  gather(modality, n, both, gesture, speech) %>%
  mutate(prop = n/sum(n)) %>%
  filter(modality == "both")

#set gradient
cc <- scales::seq_gradient_pal("#87d868", "#124200", "Lab")(seq(0,1,length.out=6))
cc[[6]] <- "#000000"
teaching_ribbons <- scales::seq_gradient_pal("#cbffb7", "#124200", "Lab")(seq(0,1,length.out=6))
teaching_ribbons[[6]] <- "#000000"
```

We first investigate parent referential communication in a longitudinal corpus of parent-child interaction. We analyze the production of multi-modal cues (i.e. using both gesture and speech) to refer to the same object, in the same instance-- an information-rich cue that we take as one instance of pedagogically supportive language input.  While many aspects of CDS support learning, multi-modal cues (e.g., speaking while pointing or looking) are uniquely powerful sources of data for young children [e.g., @baldwin2000]. Multi-modal reference may be especially pedagogically supportive if usage patterns reflect adaptive linguistic tuning, with caregivers using this information-rich cue more for young children and infrequent objects. The amount of multi-modal reference should be sensitive to the child’s age, such that caregivers will be more likely to provide richer communicative information when their child is younger (and has less linguistic knowledge) than as she gets older [@yurovsky2016]. 

 <!-- We focus on parent  to examine how parents might be aligning to their children. Parental alignment should also be stronger for infrequent objects, where again children are likely to have less knowledge of the relevant label. -->

## Methods
We used data from the Language Development Project-- a large-scale, longitudinal corpus of parent child-interaction in the home with families who are representative of the Chicago community in socio-economic and racial diversity [@goldin-meadow2014]. These data are drawn from a subsample of 10 families from the larger corpus. Recordings were taken in the home every 4-months from when the child was 14-months-old until they were 34-months-old, resulting in 6 timepoints (missing one family at the 30-month timepoint). Recordings were 90 minute sessions, and participants were given no instructions.

The Language Development Project corpus contains transcription of all speech and communicative gestures produced by children and their caregivers over the course of the 90-minute home recordings. An independent coder analyzed each of these communicative instances and identified each time a concrete noun was referenced using speech (in specific noun form), gesture (only deictic gestures were coded for ease of coding and interpretation-- e.g., pointing) or both simultaneously. 

## Results

```{r corpus_plot_lm, cache=TRUE}
parent_both_lm <- glmer(cbind(both, speech + gesture) ~ age + log(freq) + 
                    (1|referent) + (1|subj), 
      data = filter(modality_data, person == "parent"),
      family = "binomial")

age_beta <- round(fixef(parent_both_lm)["age"], digits=3)
freq_beta <- round(fixef(parent_both_lm)["log(freq)"], digits=3)

```
These corpus data were analyzed using a mixed effects regression to predict parent use of multi-modal reference for a given referent. Random effects of subject and referent were included in the model. Our key predictors were child age and logged referent frequency (i.e. how often a given object was referred to overall across our data).

We find a significant negative effect of child age (in months) on multi-modal reference, such that parents are significantly less likely to produce the multi-modal cue as their child gets older (*B =* `r age_beta`, *p < 0.0001*). We also find a significant negative effect of referent frequency on multi-modal reference as well, such that parents are significantly less likely to provide the multi-modal cue for frequent referents than infrequent ones (*B =* `r freq_beta`, *p < 0.0001*). Thus, in these data, we see early evidence that parents are providing richer, structured input about rarer things in the world for their younger children.

```{r corpus_plot, cache=TRUE,  fig.height=2, fig.align = "center", set.cap.width=T, num.cols.cap=1, fig.cap = "Proportion of parent multi-modal referential talk across development. The log of a referent's frequency is given on the x-axis, with less frequent items closer to zero."}
# theme_set(theme_bw(base_size = 14) + theme(panel.grid = element_blank(),
#                   strip.background = element_blank()))

#main corpus plot
ggplot(plot_data, aes(x = log(freq), y = prop, 
                      color = as.factor(age),
                      fill = as.factor(age),
                      label = as.factor(age))) + 
  geom_smooth(method = "loess") + 
  labs(y="Proportion of References\nusing Gesture + Speech", x='Log Referent Frequency') +
  coord_cartesian(ylim=c(0,.35), xlim=c(0,6)) +
  scale_fill_manual(values=teaching_ribbons, name='Age of Child', labels=c('14 months', '18 months','22 months','26 months', "30 months", "34 months")) +
  scale_color_manual(values=cc, name='Age of Child', labels=c('14 months', '18 months','22 months','26 months', "30 months", "34 months")) +
  theme(legend.position = c(.8,.67),
        legend.key.size = unit(.8, 'lines'),
        legend.text=element_text(size=7),
        legend.title=element_text(size=8))
```

## Discussion

Caregivers are not indiscriminate in their use of multi-modal reference; in these data, they provided more of this support when their child was younger and when discussing less familiar objects. These longitudinal corpus findings are consistent with an account of parental alignment: parents are sensitive to their child’s linguistic knowledge and adjust their communication accordingly (Yurovsky et al., 2016). Ostensive labeling is perhaps the most explicit form of pedagogical support, so we chose to focus on it for our first case study. We argue that these data could be explained by a simple, potentially-selfish pressure: to communicate successfully. The influence of communicative pressure is difficult to draw in naturalistic data, so we developed a paradigm to try to experimentally induce richly-structured, aligned input from a pressure to communicate in the moment.

# Experimental Framework

We developed a simple reference game in which participants would be motivated to communicate successfully on a trial-by-trial basis. In all conditions, participants were placed in the role of speaker and asked to communicate with a computerized listener whose responses were programmed to be contingent on speaker behavior.  We manipulated the relative costs of the communicative methods (gesture and speech) across conditions, as we did not have a direct way of assessing these costs in our naturalistic data, and they may vary across communicative contexts.  In all cases, we assumed that gesture was more costly than speech.  Though this need not be the case for all gestures and contexts, our framework compares simple lexical labeling and unambiguous deictic gestures, which likely are more costly and slower to produce [see @yurovsky2018]. We also established knowledge asymmetries by pre-training participants and manipulating how much training they thought their partner received. Using these manipulations, we aimed to experimentally determine the circumstances under which richly-structured input emerges, without an explicit pedagogical goal.

<!-- If people are motivated to communicate successfully, their choice of referential modality should reflect the tradeoff between the cost of producing the communicative signal with the likelihood that the communication would succeed. -->
<!-- After giving particpants varying amounts of training on novel names for 9 novel objects, we asked them to play a communicative game in which they were given one of the objects as their referential goal, and they were rewarded if their partner successfully selected this referent from among the set of competitors (see Figure \ref{fig:exp_screenshot}). Participants could choose to refer either using the novel labels they had been exposed to, or they could click an object to directly indicate the referent to their partner (analogous to a deictic gesture). Deixis was unambiguous, and thus would always succeed. However, in order for a labels to be effective for a target object, the participant and their partner would both have to know the correct novel label for the referent. Across participants, we varied the relative cost of using these two communicative signals. -->

<!-- If people are motivated to communicate successfully, their choice of referential modality should reflect the tradeoff between the cost of producing the communicative signal with the likelihood that the communication would succeed. We thus predicted that peoples' choice of referential modality would reflect this calculus: People should be more likely to use language if they have had more exposures to the novel object's correct label, and they should be more likely to use language as gesture becomes relatively more costly.  -->

## Method
```{r, include = FALSE}
all_data <- read.csv("data/1.30_turk_exp.csv") %>% mutate(exposureRate = as.factor(exposureRate))

n_Ps <- all_data %>% distinct(ldf_num) %>% nrow()
n_manipFail <- all_data %>% filter(manipFail==1) %>% distinct(ldf_num) %>% nrow()
n_engLabels <- all_data %>% filter(manipFail==0) %>% filter(usedEnglishLabels==1) %>% distinct(ldf_num) %>% nrow()
```

### Participants
`r n_Ps` participants were recruited though Amazon Mechanical Turk and received $1 for their participation. Data from `r n_manipFail` participants were excluded from subsequent analysis for failing the critical manipulation check and a further `r n_engLabels` for producing pseudo-English labels (e.g., 'pricklyyone'). The analyses reported exclude the data from those participants, but all analyses were also conducted without excluding any participants and all patterns hold (*ps < 0.05*).

### Design and Procedure

Participants were exposed to nine novel objects, each with a randomly assigned pseudo-word label. We manipulated the exposure rate within-subjects: during training participants saw three of the nine object-label mappings four times, two times, or one time. Participants were then given a recall task to establish their knowledge of the novel lexicon (pretest). 

```{r exp_screenshot, fig.align='center', set.cap.width=T, num.cols.cap=1, fig.cap = "Screenshot of speaker view during gameplay."}
img <- png::readPNG(here("figs/exp_screenshot.png"))
grid::grid.raster(img)
```

Prior to beginning the game, participants are told how much exposure their partner has had to the lexicon and also that they will be asked to discuss each object three times. As a manipulation check, participants are then asked to report their partner's level of exposure, and are corrected if they answer wrongly. Then during gameplay, speakers saw a target object in addition to an array of all nine objects (see Figure \ref{fig:exp_screenshot} for the speaker's perspective). Speakers had the option of either directly click on the target object in the array (gesture)- a higher cost cue but without ambiguity- or typing a label for the object (speech)- a lower cost cue but contingent on the listener's shared linguistic knowledge.  After sending the message, speakers are shown which object the listener selected.  

Speakers could win up to 100 points per trial if the listener correctly selected the target referent.  We manipulated the relative utility of the speech cue between-subjects across two conditions: low relative cost for speech (‘Low Relative Cost’) and higher relative cost for speech (‘Higher Relative Cost’). In the ‘Low Relative Cost’ condition, speakers were charged 70 points for gesturing and 0 points for labeling, yielding 30 points and 100 points respectively if the listener selected the target object. In the ‘Higher Relative Cost’ condition, speakers were charged 50 points for gesturing and 20 points for labeling, yielding up to 50 points and 80 points respectively. If the listener failed to identify the target object, the speaker nevertheless paid the relevant cost for that message in that condition. As a result of this manipulation, there was a higher relative expected utility for labeling in the ‘Low Relative Cost’ condition than the ‘Higher Relative Cost’ condition. 

Critically, participants were told about a third type of possible message using both gesture and speech within a single trial to effectively teach the listener an object-label mapping. This action directly mirrors the multi-modal reference behavior from our corpus data-- it presents the listener with an information-rich, potentially pedagogical learning moment. In order to produce this teaching behavior, speakers had to pay the cost of producing both cues (i.e. both gesture and speech). Note that, in all utility conditions, teaching yielded participants 30 points (compared with the much more beneficial strategy of speaking which yielded 100 points or 80 points across our two utility manipulations). 

<!-- Our communicative game was designed to reward in-the-moment communication, and thus teaching required the speaker pay a high cost upfront. However, rational communicators may understand that if one is accounting for future trials, paying the cost upfront to teach the listener allows a speaker to use a less costly message strategy on subsequent trials (namely, speech). -->
<!-- Incorporating teaching means that our speaker must also reason about their interlocutor’s knowledge state more explicitly, in order to make rational decisions about what to teach and when. To address this added dimension, -->

To explore the role of listener knowledge, we also manipulated participants' expectations about their partner's knowledge across 3 conditions. Participants were told that their partner had either no experience with the lexicon, had the same experience as the speaker, or had twice the experience of the speaker. 

Listeners were programmed with starting knowledge states initialized accordingly. Listeners with no exposure began the game with knowledge of 0 object-label pairs. Listeners with the same exposure of the speaker began with knowledge of five object-label pairs (3 high frequency, 1 mid frequency, 1 low frequency), based the average retention rates found previously. Lastly, the listener with twice as much exposure as the speaker began with knowledge of all nine object-label pairs. If the speaker produced a label, the listener was programmed to consult their own knowledge of the lexicon and check for similar labels (selecting a known label with a Levenshtein edit distance of two or fewer from the speaker's production), or select among unknown objects if no similar labels are found. Listeners could integrate new words into their knowledge of the lexicon if taught.

<!-- If the speaker produced a label, the listener was programmed to consult their own knowledge of the lexicon and select the closest known reference (selecting a label with a levenshtein edit distance of two or fewer from the speaker's production). If there was not a similar known label (i.e. edit distance > 2), the lsitener was programmed to select randomly among unknown objects. If the speaker gestured (or taught by producing gesture and a label), the listener was programmed to always select the gestured object. After a teaching trial, listeners integrated the taught object-label mapping into their set of candidate word-meanings when evaluating subesquent messages.  -->

Crossing our 2 between-subjects manipulations yielded 6 conditions (2 utility manipulations: ‘Low Relative Cost’ and ‘Higher Relative Cost’; and 3 levels of partner’s exposure: None, Same, Double), with 80 participants in each condition. We expected to find results that mirrored our corpus findings such that rates of teaching would be higher when there was an asymmetry in knowledge where the speaker knew more (None manipulation) compared with when there was equal knowledge (Same manipulation) or when the listener was more familiar with the language (Double manipulation). We expected that participants would also be sensitive to our utility manipulation, such that rates of labeling and teaching would be higher in the ‘Low Relative Cost’ conditions than the other conditions.

## Results
```{r}
learningByExposure <- all_data %>% 
  filter(toBeDropped!=1) %>%
  group_by(ldf_num, condition, partnersExposure, realLabel, exposureRate) %>%
  summarize(testCorrect=first(testCorrect))


descriptives <- all_data %>% 
  filter(toBeDropped!=1) %>%
  group_by(ldf_num, condition, partnersExposure, realLabel, exposureRate) %>%
  summarize(testCorrect=first(testCorrect)) %>%
  group_by(ldf_num) %>%
  summarize(sum=sum(testCorrect)) %>%
  summarize(meanK = round(mean(sum), digits=2), sdK= round(sd(sum), digits=2))

gm_known <- glmer((testCorrect==1) ~ exposureRate +
               (1|ldf_num),
               control = glmerControl(optimizer = "bobyqa"),
            data=all_data %>% filter(toBeDropped==0),
      family=binomial)

learning_beta_2 <- round(fixef(gm_known)["exposureRate2"], digits=2)
learning_beta_4 <- round(fixef(gm_known)["exposureRate4"], digits=2)

```

As an initial check of our exposure manipulation, a logistic regression showed that participants were significantly more likely to recall the label for objects with two exposures (*B =* `r learning_beta_2`, *p < 0.0001*) or with four exposures (*B =* `r learning_beta_4`, *p < 0.0001*), compared with objects they saw only once. On average, participants knew at least 6 of the 9 words in the lexicon (mean = `r descriptives$meanK`, sd = `r descriptives$sdK`).


### Gesture-Speech Tradeoff.

```{r}
gm_label <- glmer((method=='label') ~  appearance +
                partnersExposure +
                condition +
                exposureRate +
               (1|ldf_num),
               control = glmerControl(optimizer = "bobyqa"),
            data=all_data %>% filter(toBeDropped==0),
      family=binomial)

gm_label_known <- glmer((method=='label') ~  appearance +
                partnersExposure +
                condition +
                exposureRate +
               (1|ldf_num),
               control = glmerControl(optimizer = "bobyqa"),
            data=all_data %>% filter(toBeDropped==0, testCorrect==1),
      family=binomial)

label_2nd_beta <- round(fixef(gm_label)["appearanceSecond"], digits=2)
label_3rd_beta <- round(fixef(gm_label)["appearanceThird"], digits=2)
label_pExpPerf_beta <- round(fixef(gm_label)["partnersExposurePerfect"], digits=2)
label_pExpSame_beta <- round(fixef(gm_label)["partnersExposureSame"], digits=2)
label_freq2_beta <- round(fixef(gm_label)["exposureRate2"], digits=2)
label_freq4_beta <- round(fixef(gm_label)["exposureRate4"], digits=2)
label_cond80_beta <- round(fixef(gm_label)["condition80_50"], digits=2)


gm_click <- glmer((method=='click') ~ appearance +
                partnersExposure +
                condition +
                exposureRate +
               (1|ldf_num),
               control = glmerControl(optimizer = "bobyqa"),
            data=all_data %>% filter(toBeDropped==0),
      family=binomial)

click_2nd_beta <- round(fixef(gm_click)["appearanceSecond"], digits=2)
click_3rd_beta <- round(fixef(gm_click)["appearanceThird"], digits=2)
click_pExpPerf_beta <- round(fixef(gm_click)["partnersExposurePerfect"], digits=2)
click_pExpSame_beta <- round(fixef(gm_click)["partnersExposureSame"], digits=2)
click_freq2_beta <- round(fixef(gm_click)["exposureRate2"], digits=2)
click_freq4_beta <- round(fixef(gm_click)["exposureRate4"], digits=2)
click_cond80_beta <- round(fixef(gm_click)["condition80_50"], digits=2)
```

To determine how gesture and speech are trading off across conditions, we  looked at a mixed effects logistic regression to predict whether speakers chose to produce a label during a given trial as a function of the exposure rate, object instance in the game (first, second, or third), utility manipulation, and partner manipulation. A random subjects effects term was included in the model. There was a significant effect of exposure rate such that there was more labeling for objects with two exposures (*B =* `r label_freq2_beta`, *p < 0.0001*) or with four exposures (*B =* `r label_freq4_beta`, *p < 0.0001*), compared with objects seen only once at training. Compared with the first instance of an object, speakers were significantly more likely to produce a label on the second appearance (*B =* `r label_2nd_beta`, *p < 0.01*) or third instance of a given object (*B =* `r label_3rd_beta`, *p < 0.0001*). Participants also modulated their communicative behavior on the basis of the utility manipulation and our partner exposure manipulation.  Speakers in the Low Relative Cost condition produced significantly more labels than participants in the Higher Relative Cost condition (*B =* `r label_cond80_beta`, *p < 0.001*). Speakers did more labeling with more knowledgeable partners; compared with the listener with no exposure, there were significantly higher rates of labeling in the same exposure  (*B =* `r label_pExpSame_beta`, *p < 0.0001*) and double exposure conditions (*B =* `r label_pExpPerf_beta`, *p < 0.001*).  

```{r exp_speech_gesture, fig.height = 2.25, fig.width = 3.25, fig.align='center', set.cap.width=T, num.cols.cap=1, fig.cap = "Speaker communicative method choice as a function of exposure and the utility manipulation. Data are taken from the Double Exposure manipulation. Rates of teaching were minimal and are not shown."}

modality.colors <- c(speak = "#e8250b", point = "#1f11e0", teach ="#54a832", speech = "#e8250b", gesture = "#1f11e0", both ="#54a832", label = "#e8250b", click = "#1f11e0", label_click ="#54a832")

seprop <- function(props) {
  mean_prop = mean(props)
  sqrt( (mean_prop*(1 - mean_prop)) / length(props))
}

###wrangling data for plots by exposure
prop_methods_exposures <- all_data %>%
  filter(toBeDropped != 1) %>%
  ungroup() %>%
  select(condition,ldf_num, partnersExposure, exposureRate,method) %>%
  group_by(condition, ldf_num,partnersExposure, exposureRate, method) %>%
  summarise(n = n()) %>%
  group_by(condition, ldf_num,partnersExposure, exposureRate) %>%
  mutate(n = n/sum(n)) %>%
  ungroup() %>%
  mutate(method = as.factor(method)) %>%
  tidyr::complete(nesting(condition, ldf_num, partnersExposure), exposureRate, method, fill = list(n = 0)) %>%
  group_by(condition, partnersExposure, exposureRate, method) %>%
  summarise(mean = mean(n), se = seprop(n)) %>%
  ungroup()


label_data <- tibble(exposureRate = 1.25, condition = "Low Relative Cost",
                     method = c("gesture", "speech"), mean = c(.2, .8))

label_data <- label_data %>% mutate(condition = factor(condition, 
                            levels=c("Low Relative Cost","Higher Relative Cost"), ordered=TRUE))

#Exp., Plot 1., gesture speech tradeoff
plot_prop_methods_exposures <- prop_methods_exposures %>% 
  mutate(condition = ifelse(condition=="100_30", "Low Relative Cost", "Higher Relative Cost"),
      condition = as.factor(condition),
      condition = factor(condition, 
                            levels=c("Low Relative Cost","Higher Relative Cost"), ordered=TRUE),
         exposureRate = as.factor(exposureRate),
         method = factor(method, levels = c("click", "label"),
                         labels = c("gesture", "speech"))) %>%
  filter(partnersExposure == "Perfect",
         method != "label_click") 

plot_prop_methods_exposures %>%
  ggplot(aes(x=exposureRate, y=mean, color = method, label = method)) +
  geom_point(size=1.5,position=position_dodge(.25)) +
  geom_line(aes(x=(exposureRate), y=mean, group=interaction(method,condition), color=method),  size=.9,
          position=position_dodge(.25)) +
  geom_linerange(aes(ymax = mean + se,
    ymin = mean - se),position=position_dodge(.25)) +
  coord_cartesian(ylim=c(0,1)) +
  facet_grid(cols = vars(condition)) +
  labs(y="Proportion of Trials", x="Exposure Rate During Training") +
  scale_color_manual(values=c(modality.colors),  name = "Method")+
    theme(legend.position = "none",
        legend.key.size = unit(.8, 'lines'),
        legend.text=element_text(size=7),
        legend.title=element_text(size=8)) +
    geom_text(data = label_data)
```

Figure \ref{fig:exp_speech_gesture} illustrates the gesture-speech tradeoff pattern in the Double Exposure condition (as there was minimal teaching in that condition, so the speech-gesture trade-off is most interpretable). The effects on gesture mirror those found for labeling and are thus not included for brevity (*ps < 0.01*). Note that these effects cannot be explained by participant knowledge; all patterns above hold when looking *only* at words known by the speaker at pretest (*ps < 0.01*). Further, these patterns directly mirror previous corpus analyses demonstrating the gesture-speech tradeoff in naturalistic parental communicative behaviors, where lexical knowledge is likely for even the least frequent referent [see @yurovsky2018].  
\newline
    

### Emergence of Teaching.

<!-- Thus far, we have focused on relatively straightforward scenarios to demonstrate that a pressure to communicate successfully in the moment can lead speakers to trade-off between gesture and speech sensibly. Next, we turn to the emergence of teaching behavior.  -->

```{r}

gm_teach <- glmer((method=='label_click') ~ appearance +
                partnersExposure +
                condition +
                exposureRate +
               (1|ldf_num),
               control = glmerControl(optimizer = "bobyqa"),
            data=all_data %>% filter(toBeDropped==0),
      family=binomial)

teach_pExpPerf_beta <- round(fixef(gm_teach)["partnersExposurePerfect"], digits=2)
teach_pExpSame_beta <- round(fixef(gm_teach)["partnersExposureSame"], digits=2)
teach_freq2_beta <- round(fixef(gm_teach)["exposureRate2"], digits=2)
teach_freq4_beta <- round(fixef(gm_teach)["exposureRate4"], digits=2)
teach_cond80_beta <- round(fixef(gm_teach)["condition80_50"], digits=2)
teach_2nd_beta <- round(fixef(gm_teach)["appearanceSecond"], digits=2)
teach_3rd_beta <- round(fixef(gm_teach)["appearanceThird"], digits=2)

```


In line with our hypotheses, a mixed effects logistic regression predicting whether or not teaching occurred on a given trial revealed that teaching rates across conditions depend on all of the same factors that predict speech and gesture (see Figure \ref{fig:exp_teach}). There was a significant positive effect of initial training on the rates of teaching, such that participants were more likely to teach words with two exposures (*B =* `r teach_freq2_beta`, *p < 0.05*) and four exposures (*B =* `r teach_freq4_beta`, *p < 0.05*), compared with words seen only once at training. There was also a significant effect of the utility manipulation such that being in the Low Relative Cost condition predicted higher rates of teaching than being in the Higher Relative Cost condition (*B =* `r teach_cond80_beta`, *p < 0.001*), a rational response considering teaching allows one to use a less costly strategy in the future and that strategy is especially superior in the Low Relative Cost condition. 

We found an effect of partner exposure on rates of teaching as well: participants were significantly more likely to teach a partner with no prior exposure to the language than a partner with the same amount of exposure as the speaker (*B =* `r teach_pExpSame_beta`, *p < 0.0001*) or double their exposure (*B =* `r teach_pExpPerf_beta`, *p < 0.0001*). The planned utility of teaching comes from using another, cheaper strategy (speech) on later trials, thus the expected utility of teaching should decrease when there are fewer subsequent trials for that object, predicting that teaching rates should drop dramatically across trials for a given object. Compared with the first trial for an object, speakers were significantly less likely to teach on the second trial (*B =* `r teach_2nd_beta`, *p < 0.0001*) or third trial (*B =* `r teach_3rd_beta`, *p < 0.0001*).

## Discussion 

As predicted, the data from our paradigm corroborate our findings from the corpus analysis, demonstrating that pedagogically supportive behavior emerges despite the initial cost when there is an asymmetry in knowledge and when speech is less costly than other modes of communication. While this paradigm has stripped away much of the interactive environment of the naturalistic corpus data, it provides important proof of concept that the structured and tuned language input we see in those data could arise from a pressure to communicate. The paradigm’s clear, quantitative predictions also allow us to build a formal model to predict our empirical results.

```{r exp_teach, cache=TRUE, fig.height = 2.25, fig.width = 3.25, set.cap.width=T, num.cols.cap=1, fig.cap = "Rates of teaching across the 6 conditions, plotted by how many times an object had been the target object."}

prop_methods_teaching <- all_data %>%
  filter(toBeDropped != 1) %>%
  select(condition,ldf_num, partnersExposure, appearance,method) %>%
  group_by(condition, ldf_num,partnersExposure, appearance, method) %>%
  summarise(n = n()) %>%
  group_by(condition, ldf_num,partnersExposure, appearance) %>%
  mutate(n = n/sum(n)) %>%
  ungroup() %>%
  mutate(method = as.factor(method)) %>%
  tidyr::complete(nesting(condition, ldf_num, partnersExposure), appearance, method, fill = list(n = 0)) %>%
  group_by(condition, partnersExposure, appearance, method) %>%
  summarise(mean = mean(n), se = seprop(n)) 

plot_prop_methods_teaching <- prop_methods_teaching %>%
  ungroup() %>%
  mutate(condition = factor(condition, labels=c('Low Relative Cost', 'Higher Relative Cost')),
         partnersExposure = factor(partnersExposure, levels = c("None", "Same", "Perfect"),
                                   labels = c("None", "Same Amt", "Twice Amt"))) %>%
  filter(method=="label_click") 



teach_label_data <- tibble(appearance = c(1.5, 1, 1), condition = "Low Relative Cost", 
                     partnersExposure = c("None", "Same Amt", "Twice Amt"),
                     mean = c(.3, .165, .005))

teach_label_data <- teach_label_data %>% mutate(condition = factor(condition, 
                            levels=c("Low Relative Cost","Higher Relative Cost"), ordered=TRUE))

plot_prop_methods_teaching %>%
  ggplot(aes(x=appearance, y=mean, label=partnersExposure, color = partnersExposure)) +
  geom_point(aes(y=mean), size=2.25, position=position_dodge(.25)) +
  geom_line(aes(x=appearance, y=mean, group=partnersExposure, color=partnersExposure), 
            size=1, position=position_dodge(.25)) +
  geom_linerange(aes(ymax = mean + se,
    ymin = mean - se, color=partnersExposure), position=position_dodge(.25)) +
  facet_grid(. ~ condition) +
  labs(y="Proportion of Teaching Trials", x="Object Instance during Game") +
  coord_cartesian(ylim=c(0,.4)) +
  scale_color_manual(values=c("#87d868", "#54a832", "#2c6d12"), name = "Partner's Exposure") +
  theme(legend.position = "none",
        legend.key.size = unit(.8, 'lines'),
        legend.text=element_text(size=7),
        legend.title=element_text(size=8),
        axis.text.x = element_text(angle = 35, hjust = 1)) + 
  geom_text(data = teach_label_data, size = 2.5)


# grid::grid.raster(img)
```

# Model: Communication as planning
```{r, include=FALSE}
empiricalVocabs_1.30 <- all_data %>%
  group_by(ldf_num, condition, partnersExposure, realLabel, exposureRate, trueClickPoints, trueLabelPoints) %>%
  summarize(testCorrect=first(testCorrect)) %>%
  rename(exposures=exposureRate,
         label= realLabel) %>%
  ungroup() %>%
  mutate(known = ifelse(testCorrect == 1, TRUE, FALSE),
         partnersExposure = ifelse(partnersExposure=='Perfect', 2,
                                   ifelse(partnersExposure=='None', 0, 1)),
         condition=as.factor(condition))  %>%
  mutate(speechCost = 100-trueLabelPoints,
          pointCost = 100-trueClickPoints)%>%
  as.data.frame()

#lines below will run the model and clean its outputted predictions
  #for ease+efficiency, we will load a stored copy of the models predictions instead.
#library(rwebppl)
#outcomes_exp_1.30<-webppl(program_file = "wppl/speaker.wppl", data=empiricalVocabs_1.30, data_var = "empiricalVocabs")

# gamePredictions_1.30 <-outcomes_exp_1.30 %>%
#   unnest(predictions,  gameTrials) %>%
#     cbind(appearance= rep.int(c(1,2,3), nrow(.)/3))
# all_data_model <- all_data %>%
#   filter(ldf_num %in% gamePredictions_1.30$me) %>%
#   select(-partnersExposure) %>%
#   filter(toBeDropped != 1) %>%
#   group_by(ldf_num, exposureRate, targetObjectName) %>%
#   mutate(appearance = if_else(trialnum == min(trialnum), 1,
#                               if_else(trialnum == max(trialnum), 3, 2))) %>%
#   left_join(gamePredictions_1.30, by=c('ldf_num'='me', 'realLabel'='gameTrials', 'appearance')) %>%
#   mutate(method=ifelse(method=='click', "point",
#                         ifelse(method=="label","speak", "teach"))) %>%
#   as.data.frame(.)

all_data_model <- read.csv("wppl/1.30.2019_explicit_model.csv") %>% select(-X)

```

The results from this experiment are qualitatively consistent with a model in which participants make their communicative choices to maximize their expected utility from the reference game. We next formalize this model to determine if these results are predicted quantitatively as well.

\newcommand{\E}[1]{\mathbb{E}\left[ #1 \right]}

We take as inspiration the idea that communication is a kind of action--e.g. talking is a speech act [@austin1975]. Consequently, we can understand the choice of *which communicative act* a speaker should take as a question of which act would maximize their utility: achieving successful communication while minimizing their cost [@frank2012]. In this game, speakers can take three actions: talking, pointing, or teaching. In this reference game, these Utilities ($U$) are given directly by the rules. Because communication is a repeated game, people should take actions that maximize their Expected Utility ($EU$) over the course of not just this act, but all future communicative acts with the same conversational partner. We can think of communication, then as a case of recursive planning. However, people do not have perfect knowledge of each-other's vocabularies ($v$). Instead, they only have uncertain beliefs ($b$) about these vocabularies that combine their expectations about what kinds of words people with as much linguistic experience as their partner are likely to know with their observations of their partner's behavior in past communicative interactions. This makes communication a kind of planning under uncertainty well modeled as a Partially Observable Markov Decision Process [POMDP, @kaelbling1998].

Optimal planning in a POMDP involves a cycle of four phases: (1) Plan, (2) Act, (3) Observe, (4) Update beliefs. When people plan, they compute the Expected Utility of each possible action ($a$) by combining the Expected Utility of that action now with the Discounted Expected Utility they will get in all future actions. The amount of discounting ($\gamma$) reflects how people care about success now compared to success in the future. In our simulations, we set $\gamma=.5$ in line with prior work. Because Utilities depend on the communicative partner's vocabulary, people should integrate over all possible vocabularies in proportion to the probability that their belief assigns to that ($\mathbb{E}_{v \sim b}$).
$$
EU\left[a\right | b] = \mathbb{E}_{v \sim b} \left(U(a|v) + \gamma \,\mathbb{E}_{v',o',a'} \,\left( EU\left[a' | b'\right]\right)\right)
$$
Next, people take an action as a function of its Expected Utility. Following other models in the Rational Speech Act framework, we use the Luce Choice Axiom, in which each choice is taken in probability proportional to its exponentiated utility [@frank2012; @luce1959]. This choice rule has a single parameter $\alpha$ that controls the noise in this choice--as $\alpha$ approaches 0, choice is random and as $\alpha$ approaches infinity choice is optimal. For the results reported here, we set $\alpha = 2$ based on hand-tuning, but other values produce similar results.
$$
P\left(a|b\right) \propto \alpha \, e^{EU[a|b]}
$$

After taking an action, people observe ($o$) their partner's choice--sometimes they pick the intended object, and sometimes they don't. They then update their beliefs about the partner's vocabulary based on this observation. For simplicity, we assume that people think their partner should always select the correct target if they point to it, or if they teach, and similarly should always select the correct target if they produce its label and the label is in their partner's vocabulary. Otherwise, they assume that their partner will select the wrong object. People could of course have more complex inferential rules, e.g. assuming that if their partner does know a word they will choose among the set of objects whose labels they do not know [mutual exclusivity, @markman1988]. Empirically, however, our simple model appears to accord well with people's behavior.
$$
b'(v') \propto P\left(o|v',a\right) \sum_{v \in V}P\left(v'|v,a\right)b\left(v\right)
$$

The critical feature of a repeated communication game is that people can change their partner’s vocabulary. In teaching, people pay the cost of both talking and pointing together, but can leverage their partner’s new knowledge on future trials. Note here that teaching has an upfront cost and the only benefit to be gained comes from using less costly communication modes later. There is no pedagogical goal-- the model treats speakers as selfish agents aiming to maximize their own utilities by communicating successfully.  We assume for simplicity that learning is approximated by a simple Binomial learning model. If someone encounters a word $w$ in an unambiguous context (e.g. teaching), they add it to their vocabulary with probability $p$. We also assume that over the course of this short game that people do not forget--words that enter the vocabulary never leave, and that no learning happens by inference from mutual exclusivity.

$$
P\left(v'|v,a\right)= \begin{cases} 
1 & \text{if } v_{w} \in v \& v'\\ 
p & \text{if } v_{w} \notin v \& a = \text{point+talk}\\ 
0 & otherwise\end{cases}
$$

The final detail is to specify how people estimate their partner's learning rate ($p$) and initial vocabulary ($v$). We propose that people begin by estimating their own learning rate by reasoning about the words they learned at the start of the task: Their $p$ is the rate that maximizes the probability of them having learned their initial vocabularies from the trials they observed. People can then expect their partner to have a similar $p$ [per the "like me" hypothesis, @meltzoff2005]. Having an estimate of their partner's $p$, they can estimate their vocabulary by simulating their learning from the amount of training we told them their partner had before the start of the game. 

```{r include=FALSE}
prop_methods <- all_data_model %>%
  filter(toBeDropped != 1) %>%
  ungroup() %>%
  select(condition,ldf_num, partnersExposure, appearance,method,predictions) %>%
  # select(condition,ldf_num, partnersExposure, appearance,method) %>%
  gather(isEmpirical, method, -condition, -partnersExposure, -ldf_num, -appearance) %>%
  group_by(condition, ldf_num,partnersExposure, appearance,isEmpirical, method) %>%
  summarise(n = n()) %>%
  group_by(condition, ldf_num,partnersExposure, isEmpirical, appearance) %>%
  mutate(n = n/sum(n)) %>%
  ungroup() %>%
  mutate(method = as.factor(method)) %>%
  tidyr::complete(nesting(condition, ldf_num, partnersExposure), isEmpirical, appearance, method, fill = list(n = 0)) %>%
  group_by(condition, partnersExposure, method, isEmpirical, appearance) %>%
  summarise(mean = mean(n), se = seprop(n))


###direct plot of model fit
model_fit <- prop_methods %>%
  ungroup() %>%
  mutate(isEmpirical= ifelse(isEmpirical=='method', 'empirical', isEmpirical)) %>%
  unite(mean_new, mean, se) %>%
  spread(isEmpirical, mean_new) %>% 
  separate(empirical, c('mean_emp', 'se_emp'), '_') %>%
  separate(predictions, c('mean_pred', 'se_pred'), '_') %>%
  mutate_if(grepl('_', names(.)), as.numeric) %>%
  rename(mean_pred_plot=mean_pred,
         se_pred_plot=se_pred)


model_corr <- model_fit %>%
  summarise(cor = round(cor(mean_emp, mean_pred_plot), digits=2))

model_corr_test <- cor.test(model_fit$mean_emp, model_fit$mean_pred_plot)
```

## Model Results
The fit between our model’s predictions and our empirical data from our reference game study on Amazon Turk can be seen in Figure \ref{fig:model_fit}. The model outputs trial-level action predictions (e.g., “speak”) for every speaker in our empirical data. These model outputs were aggregated across the same factors as the empirical data: modality, appearance, partner’s exposure, and utility condition. We see a significant correlation of our model predictions and our empirical data (*r = `r model_corr`, p<0.0001*). Our model provides a strong fit for these data, supporting our conclusion that richly-structured language input could emerge from in-the-moment pressure to communicate, without a goal to teach.

```{r model_fit, fig.env = "figure", fig.pos = "H", fig.align='center',  fig.height=2.5,  fig.width=3.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Fit between model predictions and empirical data."}
#plot of model predictions vs empirical game data
model_fit %>%
  ggplot(aes(x=mean_pred_plot, y=mean_emp, color=method,
             shape=as.factor(partnersExposure),group=appearance,
             alpha=condition,
             label = appearance)) +
  geom_point(aes(size=as.factor(appearance))) +
  geom_abline(intercept =0, slope=1) +
  geom_linerange(aes(ymax = mean_emp + se_emp,
                     ymin = mean_emp - se_emp),
                 alpha=.4) +
  geom_errorbarh(aes(xmin = mean_pred_plot - se_pred_plot,
                     xmax = mean_pred_plot + se_pred_plot),
                 alpha = .4)+
  # geom_text(aes(label=appearance),hjust=0, vjust=0) + 
  # facet_grid(.~condition)+
  coord_cartesian(xlim=c(0,.8), ylim=c(0,.8)) + 
  labs(y="Empirical Proportion of Trials", x="Model Predictions") +
  scale_color_manual(values=modality.colors, name='Modality', labels=c("Gesture", "Speech", "Teach")) +
  scale_shape_manual(values=c(15,17,16),name="Partner's Exposure", labels=c("None", "Same Amount", "Twice as Much")) + 
  scale_alpha_manual(values=c(.4, 1), name="Utility Condition", labels=c("Low Relative Cost", "Higher Relative Cost")) +
  scale_size_manual(values=c(1,2,3), name="Appearance", labels=c("First", "Second", "Third"))+
  theme(legend.direction = "vertical", 
        legend.position = "right",
        legend.key.size = unit(.15, 'lines'),
        legend.title=element_text(size=7),
        legend.text=element_text(size=6),
        legend.spacing = unit(0, "in"),
        legend.box.spacing = unit(0, "in"))
```


# General Discussion

We showed that people tune their communicative choices to varying cost and reward structures, and also critically to their partner’s linguistic knowledge–providing richer cues when partners are unlikely to know language and many more rounds remain. These data are consistent with the patterns shown in our corpus analysis of parent referential communication and demonstrate that such pedagogically supportive input could arise from a motivation to maximize communicative success while minimizing communicative cost-- no additional motivation to teach is necessary. Our account is not specific to any particular language phenomenon, though we have focused on multi-modal reference here. Given the right data or paradigm, our account should hold equally well when explaining how other information-rich language input could arise. 

Of course, many aspects of language do not differ in speech to children [e.g., syntax, see @newport1977]. On our account, not all aspects of language should be calibrated to child’s language development--only those that support communication. A full account that explains variability in modification across aspects of language will rely on a fully specified model of optimal communication. Such a model will allow us to determine both which structures are predictably unmodified, and which structures must be modified for other reasons. Nonetheless, this work is an important first step in validating the hypothesis that language input that is structured to support language learning could arise from a single unifying goal: The desire to communicate effectively. 

<!-- Of course, this work is limited in a number of important ways. While our task allows us to distill key pressures and manipulations, there are signficant divergences from the corpus data-collecting environment as a result. For one, there are fewer and less dynamic, reciporical interactions between interlocutors and no time-constraints are imposed on communication. Also, parents are likely aware of not just how much language their children know, but *what* their children know (at least at a lexical level). Critical next steps in this line of work will return back to the level of parent-child interaction. -->

\vspace{1em}\fbox{ \parbox[b][][c]{7.5cm}{\centering The Mechanical Turk experiment was preregistered on Open Science Framework at https://osf.io/63qdg All data and code for analyses are available at https://github.com/benjamincmorris/reference-game \ }}

# Acknowledgements
This research was funded by a James S. McDonnell Foundation Scholar Award to DY.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}

\noindent



