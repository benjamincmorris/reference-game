% Template for Cogsci submission with R Markdown

% Stuff changed from original Markdown PLOS Template
\documentclass[10pt, letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{float}
\usepackage{caption}

% amsmath package, useful for mathematical formulas
\usepackage{amsmath}

% amssymb package, useful for mathematical symbols
\usepackage{amssymb}

% hyperref package, useful for hyperlinks
\usepackage{hyperref}

% graphicx package, useful for including eps and pdf graphics
% include graphics with the command \includegraphics
\usepackage{graphicx}

% Sweave(-like)
\usepackage{fancyvrb}
\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{}
\DefineVerbatimEnvironment{Scode}{Verbatim}{fontshape=sl}
\newenvironment{Schunk}{}{}
\DefineVerbatimEnvironment{Code}{Verbatim}{}
\DefineVerbatimEnvironment{CodeInput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{CodeOutput}{Verbatim}{}
\newenvironment{CodeChunk}{}{}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{apacite}

% KM added 1/4/18 to allow control of blind submission


\usepackage{color}

% Use doublespacing - comment out for single spacing
%\usepackage{setspace}
%\doublespacing


% % Text layout
% \topmargin 0.0cm
% \oddsidemargin 0.5cm
% \evensidemargin 0.5cm
% \textwidth 16cm
% \textheight 21cm

\title{Pressure to communicate across knowledge asymmetries leads to
pedagogically supportive language input}


\author{Benjamin C. Morris \and Daniel Yurovsky \\
        \texttt{\{benmorris, yurovsky\}@uchicago.edu} \\
       Department of Psychology \\ University of Chicago}

\begin{document}

\maketitle

\begin{abstract}
Children do not learn language from passive observation of the world,
but from interaction with caregivers who want to communicate with them.
Children's language input from these caregivers has rich structure.
Though this structure might appear intentionally pedagogical, we argue a
simpler account: pressure to communicate successfully with a
linguistically immature partner could give rise to the kind of
pedagogically supportive structure observed in children's language
input. We first show in a corpus analysis that caregivers provide more
information-rich referential communication, using both gesture and
speech to refer to a single object, when that object is rare and when
their child is young. Then, in an iterated reference game experiment on
Mechanical Turk (n = 480), we show how this behavior can arise from
communicative pressure alone when communicating with a less
knowledgeable partner. Lastly, we show that speaker behavior in our
experiment can be explained by a rational planning model, without any
explicit teaching goal. We suggest that caregivers' desire to
communicate successfully may play a powerful role in structuring
children's input in order to support language learning.

\textbf{Keywords:}
language learning; communication; computational modeling.
\end{abstract}

\section{Introduction}\label{introduction}

One of the most striking aspects of children's language learning is just
how quickly they master the complex system of their natural language
(Bloom, 2000). In just a few short years, children go from complete
ignorance to conversational fluency in a way that is the envy of
second-language learners attempting the same feat later in life
(Newport, 1990). What accounts for this remarkable transition?

One possibility is that children's caregivers deserve most of the
credit; that the language parents produce to their children is optimized
for teaching. Although there is some evidence that aspects of
child-directed speech support learning, other aspects--even in the same
subproblem, e.g.~phoneme discrimination--appear to make learning more
difficult (Eaves Jr, Feldman, Griffiths, \& Shafto, 2016; McMurray,
Kovack-Lesh, Goodwin, \& McEchron, 2013). In general, parents rarely
explicitly correct their children, and children are resistant to the
rare explicit language correction they do get (Newport, Gleitman, \&
Gleitman, 1977). Thus while parents may occasionally offer a supervisory
signal, the bulk of the evidence suggests that parental supervision is
unlikely to explain rapid early language acquisition.

Alternatively, even infants may already come to language acquisition
with a precocious ability to learn the latent structure of language from
the statistical properties of speech in their ambient environment
(Saffran, 2003; Smith \& Yu, 2008). While a number of experiments
clearly demonstrate the early availability of such mechanisms, there is
reason to be suspicious about just how precocious they are early in
development. For example, infants' ability to track the co-occurrence
information connecting words to their referents appears to be highly
constrained by both their developing memory and attention systems (Smith
\& Yu, 2013; Vlach \& Johnson, 2013). Further, computational models of
these processes show that the rate of acquisition is highly sensitive to
variation in environmental statistics (e.g., Vogt, 2012). Thus
precocious unsupervised statistical learning also appears to fall short
of an explanation for rapid early language learning.

We explore the consequences of a third possibility: The language that
children hear is neither designed for pedagogy, nor is it random; it is
designed for communication (Brown, 1977). We take as the caregiver's
goal the desire to communicate with the child, not about language
itself, but instead about the world in front of them. To succeed, the
caregiver must produce the kinds of communicative signals that the child
can understand, and thus might tune the complexity of their speech not
for the sake of learning itself, but as a byproduct of in-the-moment
pressure to communicate successfully (Yurovsky, 2017). Thus, even
without a pedagogical goal, this communicative pressure could lead to
pedagogically-supportive language input that may help to explain rapid
early language learning.

To examine this hypothesis, we first analyze parent communicative
behavior in a longitudinal corpus of parent-child interaction in the
home (Goldin-Meadow et al., 2014). We investigate the extent to which
parents tune their communicative behavior (focusing on modality--
i.e.~gesture vs.~speech) across their child's development to align to
their child's developing linguistic knowledge (Yurovsky, Doyle, \&
Frank, 2016).

We then turn to the emergence of tuned, structured input in a simple
model system: an iterated reference game in which two players earn
points for communicating successfully with each other. Modeled after our
corpus data, participants are asked to make choices about which
communicative strategy to use (akin to modality choice). In an
experiment on Mechanical Turk using this model system, we show that
tuned, structured language input can arise from a pressure to
communicate. We then show that participant behavior in our game can be
explained by a rational planning model that seeks to optimize its total
expected utility over the course of the game.

\section{Corpus Analysis}\label{corpus-analysis}

We first investigate parent referential communication in a longitudinal
corpus of parent child interaction. We analyze the production of
multi-modal cues (i.e.~using both gesture and speech) to refer to the
same object, in the same instance. This multi-modal reference may
reflect linguistic tuning if parents are adaptively using this
information-rich cue more for young children and infrequent objects. The
amount of multi-modal reference should be sensitive to the child's age,
such that parents will be more likely to provide richer communicative
information when their child is younger (and has less linguistic
knowledge) than as she gets older (Yurovsky et al., 2016).

\subsection{Methods}\label{methods}

We used data from the Language Development Project-- a large-scale,
longitudinal corpus of parent child-interaction in the home with
families who are representative of the Chicago community in
socio-economic and racial diversity (Goldin-Meadow et al., 2014). These
data are drawn from a subsample of 10 families from the larger corpus.
Recordings were taken in the home every 4-months from when the child was
14-months-old until they were 34-months-old, resulting in 6 timepoints
(missing one family at the 30-month timepoint). Recordings were 90
minute sessions, and participants were given no instructions.

The Language Development Project corpus contains transcription of all
speech and communicative gestures produced by children and their
caregivers over the course of the 90-minute home recordings. An
independent coder analyzed each of these communicative instances and
identified each time a concrete noun was referenced using speech (in
specific noun form), gesture (only deictic gestures were coded for ease
of coding and interpretation-- e.g., pointing) or both simultaneously.

\subsection{Results}\label{results}

These corpus data were analyzed using a mixed effects regression to
predict parent use of multi-modal reference for a given referent. Random
effects of subject and referent were included in the model. Our key
predictors were child age and logged referent frequency (i.e.~how often
a given object was referred to overall across our data).

We find a significant negative effect of child age on multi-modal
reference, such that parents are significantly less likely to produce
the multi-modal cue as their child gets older (\emph{B \textless{}}
-0.04, \emph{p \textless{} 0.0001}). We also find a significant negative
effect of referent frequency on multi-modal reference as well, such that
parents are significantly less likely to provide the multi-modal cue for
frequent referents than infrequent ones (\emph{B \textless{}} -0.13,
\emph{p \textless{} 0.0001}). Thus, in these data, we see early evidence
that parents are providing richer, structured input about rarer things
in the world for their younger children.

\begin{CodeChunk}
\begin{figure}[tb]

{\centering \includegraphics{figs/corpus_plot-1} 

}

\caption[Proportion of parent multi-modal referential talk across development]{Proportion of parent multi-modal referential talk across development. The log of a referent's frequency is given on the x-axis, with less frequent items closer to zero.}\label{fig:corpus_plot}
\end{figure}
\end{CodeChunk}

\subsection{Discussion}\label{discussion}

We have shown that parents provided more multi-modal reference when
their child was younger and for less familiar objects. These
longitudinal corpus findings are consistent with an account of parental
alignment: parents are sensitive to their child's linguistic knowledge
and adjust their communication accordingly. These alignment data could
be argued to derive from a pedagogical pressure to teach younger
children, however we will argue that these data could be explained by a
simpler, potentially-selfish pressure: to communicate successfully. The
distinction between these pressures is difficult to draw in naturalistic
data, so we developed a paradigm to try to experimentally induce
richly-structured, aligned input from a pressure to communicate in the
moment.

\section{Experimental Framework}\label{experimental-framework}

We developed a simple reference game in which participants would be
motivated to communicate successfully on a trial-by-trial basis. By
manipulating the relative costs of the communicative methods and
examining the role of asymmetric knowledge states, we aimed to
experimentally determine the circumstances under which richly-structured
input emerges, without an explicit pedagogical goal. In all conditions,
participants were placed in the role of speaker and asked to communicate
with a computerized listener whose responses were programmed to be
contingent on speaker behavior.

\subsection{Method}\label{method}

\subsubsection{Participants}\label{participants}

480 participants were recruited though Amazon Mechanical Turk and
received \$1 for their participation. Data from 51 participants were
excluded from subsequent analysis for failing the critical manipulation
check and a further 28 for producing pseudo-English labels (e.g.,
`pricklyyone'). The analyses reported exclude the data from those
participants, but all analyses were also conducted without excluding any
participants and all patterns hold (\emph{ps \textless{} 0.05}).

\subsubsection{Design and Procedure}\label{design-and-procedure}

Participants were exposed to nine novel objects, each with a randomly
assigned pseudo-word label. We manipulated the exposure rate
within-subjects: during training participants saw three of the nine
object-label mappings four times, two times, or one time. Participants
were then given a recall task to establish their knowledge of the novel
lexicon (pretest).

\begin{CodeChunk}
\begin{figure}[tb]

{\centering \includegraphics{figs/exp_screenshot-1} 

}

\caption[Screenshot of speaker view during gameplay]{Screenshot of speaker view during gameplay.}\label{fig:exp_screenshot}
\end{figure}
\end{CodeChunk}

Prior to beginning the game, participants are told how much exposure
their partner has had to the lexicon and also that they will be asked to
discuss each object three times. As a manipulation check, participants
are then asked to report their partner's level of exposure, and are
corrected if they answer wrongly. Then during gameplay, speakers saw a
target object in addition to an array of all nine objects (see Figure
\ref{fig:exp_screenshot} for the speaker's perspective). Speakers had
the option of either directly click on the target object in the array
(gesture)- a higher cost cue but without ambiguity- or typing a label
for the object (speech)- a lower cost cue but contingent on the
listener's shared linguistic knowledge. After sending the message,
speakers are shown which object the listener selected.

Speakers could win up to 100 points per trial if the listener correctly
selected the target referent. We manipulated the relative utility of the
speech cue between-subjects across two conditions: low relative cost for
speech (`Low Relative Cost') and higher relative cost for speech
(`Higher Relative Cost'). In the `Low Relative Cost' condition, speakers
were charged 70 points for gesturing and 0 points for labeling, yielding
30 points and 100 points respectively if the listener selected the
target object. In the `Higher Relative Cost' condition, speakers were
charged 50 points for gesturing and 20 points for labeling, yielding up
to 50 points and 80 points respectively. If the listener failed to
identify the target object, the speaker nevertheless paid the relevant
cost for that message in that condition. As a result of this
manipulation, there was a higher relative expected utility for labeling
in the `Low Relative Cost' condition than the `Higher Relative Cost'
condition.

Critically, participants were told about a third type of possible
message using both gesture and speech within a single trial to
effectively teach the listener an object-label mapping. This action
directly mirrors the multi-modal reference behavior from our corpus
data-- it presents the listener with an information-rich, potentially
pedagogical learning moment. In order to produce this teaching behavior,
speakers had to pay the cost of producing both cues (i.e.~both gesture
and speech). Note that, in all utility conditions, teaching yielded
participants 30 points (compared with the much more beneficial strategy
of speaking which yielded 100 points or 80 points across our two utility
manipulations).

To explore the role of listener knowledge, we also manipulated
participants' expectations about their partner's knowledge across 3
conditions. Participants were told that their partner had either no
experience with the lexicon, had the same experience as the speaker, or
had twice the experience of the speaker.

Listeners were programmed with starting knowledge states initialized
accordingly. Listeners with no exposure began the game with knowledge of
0 object-label pairs. Listeners with the same exposure of the speaker
began with knowledge of five object-label pairs (3 high frequency, 1 mid
frequency, 1 low frequency), based the average retention rates found
previously. Lastly, the listener with twice as much exposure as the
speaker began with knowledge of all nine object-label pairs. If the
speaker produced a label, the listener was programmed to consult their
own knowledge of the lexicon and check for similar labels (selecting a
known label with a Levenshtein edit distance of two or fewer from the
speaker's production), or select among unknown objects if no similar
labels are found. Listeners could integrate new words into their
knowledge of the lexicon if taught.

Crossing our 2 between-subjects manipulations yielded 6 conditions (2
utility manipulations: `Low Relative Cost' and `Higher Relative Cost';
and 3 levels of partner's exposure: None, Same, Double), with 80
participants in each condition. We expected to find results that
mirrored our corpus findings such that rates of teaching would be higher
when there was an asymmetry in knowledge where the speaker knew more
(None manipulation) compared with when there was equal knowledge (Same
manipulation) or when the listener was more familiar with the language
(Double manipulation). We expected that participants would also be
sensitive to our utility manipulation, such that rates of labeling and
teaching would be higher in the `Low Relative Cost' conditions than the
other conditions.

\subsection{Results}\label{results-1}

As an initial check of our exposure manipulation, a logistic regression
showed that participants were significantly more likely to recall the
label for objects with two exposures (\emph{B =} 1.66, \emph{p
\textless{} 0.0001}) or with four exposures (\emph{B =} 3.07, \emph{p
\textless{} 0.0001}), compared with objects they saw only once. On
average, participants knew at least 6 of the 9 words in the lexicon
(mean = 6.28, sd = 2.26).

\subsubsection{Gesture-Speech Tradeoff.}\label{gesture-speech-tradeoff.}

To determine how gesture and speech are trading off across conditions,
we looked at a mixed effects logistic regression to predict whether
speakers chose to produce a label during a given trial as a function of
the exposure rate, object instance in the game (first, second, or
third), utility manipulation, and partner manipulation. A random
subjects effects term was included in the model. There was a significant
effect of exposure rate such that there was more labeling for for
objects with two exposures (\emph{B =} 0.91, \emph{p \textless{}
0.0001}) or with four exposures (\emph{B =} 1.83, \emph{p \textless{}
0.0001}), compared with objects seen only once at training. Compared
with the first instance of an object, speakers were significantly more
likely to produce a label on the second appearance (\emph{B =} 0.2,
\emph{p \textless{} 0.01}) or third instance of a given object (\emph{B
=} 0.46, \emph{p \textless{} 0.0001}). Participants also modulated their
communicative behavior on the basis of the utility manipulation and our
partner exposure manipulation. Speakers in the Low Relative Cost
condition produced significantly more labels than participants in the
Higher Relative Cost condition (\emph{B =} -0.84, \emph{p \textless{}
0.001}). Speakers did more labeling with more knowledgeable partners;
compared with the listener with no exposure, there were significantly
higher rates of labeling in the same exposure (\emph{B =} 1.74, \emph{p
\textless{} 0.0001}) and double exposure conditions (\emph{B =} 3.14,
\emph{p \textless{} 0.001}).

Figure \ref{fig:exp_speech_gesture} illustrates the gesture-speech
tradeoff pattern in the Double Exposure condition (as there was minimal
teaching in that condition, so the speech-gesture trade-off is most
interpretable). The effects on gesture mirror those found for labeling
and are thus not included for brevity (\emph{ps \textless{} 0.01}). Note
that these effects cannot be explained by the degree of participant
knowledge; all patterns above hold when looking \emph{only} at words
known by the speaker at pretest (\emph{ps \textless{} 0.01}).

\begin{CodeChunk}
\begin{figure}[tb]

{\centering \includegraphics{figs/exp_speech_gesture-1} 

}

\caption[Speaker communicative method choice as a function of exposure and the utility manipulation]{Speaker communicative method choice as a function of exposure and the utility manipulation. Data are taken from the Double Exposure manipulation. Rates of teaching were minimal and are not shown.}\label{fig:exp_speech_gesture}
\end{figure}
\end{CodeChunk}

\subsubsection{Emergence of Teaching.}\label{emergence-of-teaching.}

In line with our hypotheses, a mixed effects logistic regression
predicting whether or not teaching occurred on a given trial revealed
that teaching rates across conditions depend on all of the same factors
that predict speech and gesture (see Figure \ref{fig:exp_teach}). There
was a significant positive effect of initial training on the rates of
teaching, such that participants were more likely to teach words with
two exposures (\emph{B =} 0.26, \emph{p \textless{} 0.05}) and four
exposures (\emph{B =} 0.25, \emph{p \textless{} 0.05}), compared with
words seen only once at training. There was also a significant effect of
the utility manipulation such that being in the Low Relative Cost
condition predicted higher rates of teaching than being in the Higher
Relative Cost condition (\emph{B =} -0.96, \emph{p \textless{} 0.001}),
a rational response considering teaching allows one to use a less costly
strategy in the future and that strategy is especially superior in the
Low Relative Cost condition.

We found an effect of partner exposure on rates of teaching as well:
participants were significantly more likely to teach a partner with no
prior exposure to the language than a partner with the same amount of
exposure as the speaker (\emph{B =} -1.63, \emph{p \textless{} 0.0001})
or double their exposure (\emph{B =} -3.51, \emph{p \textless{}
0.0001}). The planned utility of teaching comes from using another,
cheaper strategy (speech) on later trials, thus the expected utility of
teaching should decrease when there are fewer subsequent trials for that
object, predicting that teaching rates should drop dramatically across
trials for a given object. Compared with the first trial for an object,
speakers were significantly less likely to teach on the second trial
(\emph{B =} -0.84, \emph{p \textless{} 0.0001}) or third trial (\emph{B
=} -1.67, \emph{p \textless{} 0.0001}).

\subsection{Discussion}\label{discussion-1}

In line with our predictions, the data from our paradigm corroborate our
findings from the corpus analysis, demonstrating that pedagogical-like
behavior emerges despite the initial cost when there is an asymmetry in
knowledge and when speech is less costly than other modes of
communication. While this paradigm has stripped away much of the
interactive environment of the naturalistic corpus data, it provides
important proof of concept that the structured and tuned language input
we see in those data could arise from a pressure to communicate. The
paradigm's clear, quantitative predictions also allow us to build a
formal model to predict our empirical results.

\begin{CodeChunk}
\begin{figure}[tb]
\includegraphics{figs/exp_teach-1} \caption[Rates of teaching across the 6 conditions, where the x-axis how many times an object had been the target object]{Rates of teaching across the 6 conditions, where the x-axis how many times an object had been the target object.}\label{fig:exp_teach}
\end{figure}
\end{CodeChunk}

\section{Model: Communication as
planning}\label{model-communication-as-planning}

The results from this experiment are qualitatively consistent with a
model in which participants make their communicative choices to maximize
their expected utility from the reference game. We next formalize this
model to determine if these results are predicted quantitatively as
well.

\newcommand{\E}[1]{\mathbb{E}\left[ #1 \right]}

We take as inspiration the idea that communication is a kind of
action--e.g.~talking is a speech act (Austin, 1975). Consequently, we
can understand the choice of \emph{which communicative act} a speaker
should take as a question of which act would maximize their utility:
achieving successful communication while minimizing their cost (Frank \&
Goodman, 2012). In this game, speakers can take three actions: talking,
pointing, or teaching. In this reference games, these Utilities (\(U\))
are given directly by the rules. Because communication is a repeated
game, people should take actions that maximize their Expected Utility
(\(EU\)) over the course of not just this act, but all future
communicative acts with the same conversational partner. We can think of
communication, then as a case of recursive planning. However, people do
not have perfect knowledge of each-other's vocabularies (\(v\)).
Instead, they only have uncertain beliefs (\(b\)) about these
vocabularies that combine their expectations about what kinds of words
people with as much linguistic experience as their partner are likely to
know with their observations of their partner's behavior in past
communicative interactions. This makes communication a kind of planning
under uncertainty well modeled as a Partially Observable Markov Decision
Process (POMDP, Kaelbling, Littman, \& Cassandra, 1998).

Optimal planning in a POMDP involves a cycle of four phases: (1) Plan,
(2) Act, (3) Observe, (4) Update beliefs. When people plan, they compute
the Expected Utility of each possible action (\(a\)) by combining the
Expected Utility of that action now with the Discounted Expected Utility
they will get in all future actions. The amount of discounting
(\(\gamma\)) reflects how people care about success now compared to
success in the future. In our simulations, we set \(\gamma=.5\) in line
with prior work. Because Utilities depend on the communicative partner's
vocabulary, people should integrate over all possible vocabularies in
proportion to the probability that their belief assigns to that
(\(\mathbb{E}_{v \sim b}\)). \[
EU\left[a\right | b] = \mathbb{E}_{v \sim b} \left(U(a|v) + \gamma \,\mathbb{E}_{v',o',a'} \,\left( EU\left[a' | b'\right]\right)\right)
\] Next, people take an action as a function of its Expected Utility.
Following other models in the Rational Speech Act framework, we use the
Luce Choice Axiom, in which each choice is taken in probability
proportional to its exponentiated utility (Frank \& Goodman, 2012; Luce,
1959). This choice rule has a single parameter \(\alpha\) that controls
the noise in this choice--as \(\alpha\) approaches 0, choice is random
and as \(\alpha\) approaches infinity choice is optimal. For the results
reported here, we set \(\alpha = 2\) based on hand-tuning, but other
values produce similar results. \[ 
P\left(a|b\right) \propto \alpha \, e^{EU[a|b]}
\]

After taking an action, people observe (\(o\)) their partner's
choice--sometimes they pick the intended object, and sometimes they
don't. They then update their beliefs about the partner's vocabulary
based on this observation. For simplicity, we assume that people think
their partner should always select the correct target if they point to
it, or if they teach, and similarly should always select the correct
target if they produce its label and the label is in their partner's
vocabulary. Otherwise, they assume that their partner will select the
wrong object. People could of course have more complex inferential
rules, e.g.~assuming that if their partner does know a word they will
choose among the set of objects whose labels they do not know (mutual
exclusivity, Markman \& Wachtel, 1988). Empirically, however, our simple
model appears to accord well with people's behavior. \[
b'(v') \propto P\left(o|v',a\right) \sum_{v \in V}P\left(v'|v,a\right)b\left(v\right)
\]

The critical feature of a repeated communication game is that people can
change their partner's vocabulary. In teaching, people pay the cost of
both talking and pointing together, but can leverage their partner's new
knowledge on future trials. Note here that teaching has an upfront cost
and the only benefit to be gained comes from using less costly
communication modes later. There is no pedagogical goal-- the model
treats speakers as selfish agents aiming to maximize their own utilities
by communicating successfully. We assume for simplicity that learning is
approximated by a simple Binomial learning model. If someone encounters
a word \(w\) in an unambiguous context (e.g.~teaching), they add it to
their vocabulary with probability \(p\). We also assume that over the
course of this short game that people do not forget--words that enter
the vocabulary never leave, and that no learning happens by inference
from mutual exclusivity.

\[
P\left(v'|v,a\right)= \begin{cases} 
1 & \text{if } v_{w} \in v \& v'\\ 
p & \text{if } v_{w} \notin v \& a = \text{point+talk}\\ 
0 & otherwise\end{cases}
\]

The final detail is to specify how people estimate their partner's
learning rate (\(p\)) and initial vocabulary (\(v\)). We propose that
people begin by estimating their own learning rate by reasoning about
the words they learned at the start of the task: Their \(p\) is the rate
that maximizes the probability of them having learned their initial
vocabularies from the trials they observed. People can then expect their
partner to have a similar \(p\) (per the ``like me'' hypothesis,
Meltzoff, 2005). Having an estimate of their partner's \(p\), they can
estimate their vocabulary by simulating their learning from the amount
of training we told them their partner had before the start of the game.

\subsection{Model Results}\label{model-results}

The fit between our model's predictions and our empirical data from our
reference game study on Amazon Turk can be seen in Figure
\ref{fig:model_fit}. The model outputs trial-level action predictions
(e.g. ``speak'') for every speaker in our empirical data. These model
outputs were aggregated across the same factors as the empirical data:
modality, appearance, partner's exposure, and utility condition. We see
a significant correlation of our model predictions and our empirical
data (\emph{r = 0.94, p\textless{}0.0001}). Our model provides a strong
fit for these data, supporting our conclusion that richly-structured
language input could emerge from in-the-moment pressure to communicate,
without a goal to teach.

\begin{CodeChunk}
\begin{figure}[H]

{\centering \includegraphics{figs/model_fit-1} 

}

\caption[Fit between model predictions and empirical data]{Fit between model predictions and empirical data.}\label{fig:model_fit}
\end{figure}
\end{CodeChunk}

\section{General Discussion}\label{general-discussion}

We showed that people tune their communicative choices to varying cost
and reward structures, and also critically to their partner's linguistic
knowledge--teaching when partners are unlikely to know language and many
more rounds remain. These data are consistent with the patterns shown in
our corpus analysis of parent referential communication and demonstrate
that such pedagogically-supportive input could arise from purely selfish
motives to maximize the utility of communicating successfully while
minimizing the cost of communication. Our account is not specific to any
particular language phenomenon, though we have focused on multi-modal
reference here. Given the right data or paradigm, our account should
hold equally well when explaining how other information-rich language
input could arise. Using corpus and experimental data, this work
validates the hypothesis that language input that is structured to
support language learning could arise from a single unifying goal: The
desire to communicative effectively.

\vspace{1em}\fbox{ \parbox[b][][c]{7.3cm}{\centering The Mechanical Turk experiment was preregistered on Open Science Framework, and all data and analyses are on GitHub and will be made available after unblinding. \ }}

\section{References}\label{references}

\setlength{\parindent}{-0.1in} \setlength{\leftskip}{0.125in}

\noindent

\hypertarget{refs}{}
\hypertarget{ref-austin1975}{}
Austin, J. L. (1975). \emph{How to do things with words} (Vol. 88).
Oxford university press.

\hypertarget{ref-bloom2000}{}
Bloom, P. (2000). \emph{How children learn the meanings of words}. MIT
press: Cambridge, MA.

\hypertarget{ref-brown1977}{}
Brown, R. (1977). Introduction. In C. E. Snow \& C. A. Ferguson (Eds.),
\emph{Talking to children: Language input and interaction}. Cambridge,
MA.: MIT Press.

\hypertarget{ref-eaves-jr2016}{}
Eaves Jr, B. S., Feldman, N. H., Griffiths, T. L., \& Shafto, P. (2016).
Infant-directed speech is consistent with teaching. \emph{Psychological
Review}, \emph{123}(6), 758.

\hypertarget{ref-frank2012}{}
Frank, M. C., \& Goodman, N. D. (2012). Predicting Pragmatic Reasoning
in Language Games. \emph{Science}, \emph{336}(6084), 998--998.

\hypertarget{ref-goldin-meadow2014}{}
Goldin-Meadow, S., Levine, S. C., Hedges, L. V., Huttenlocher, J.,
Raudenbush, S. W., \& Small, S. L. (2014). New evidence about language
and cognitive development based on a longitudinal study: Hypotheses for
intervention. \emph{American Psychologist}, \emph{69}(6), 588--599.

\hypertarget{ref-kaelbling1998}{}
Kaelbling, L. P., Littman, M. L., \& Cassandra, A. R. (1998). Planning
and acting in partially observable stochastic domains. \emph{Artificial
Intelligence}, \emph{101}, 99--134.

\hypertarget{ref-luce1959}{}
Luce, R. D. (1959). Individual choice behavior.

\hypertarget{ref-markman1988}{}
Markman, E. M., \& Wachtel, G. F. (1988). Children's use of mutual
exclusivity to constrain the meanings of words. \emph{Cognitive
Psychology}, \emph{20}(2), 121--157.

\hypertarget{ref-mcmurray2013}{}
McMurray, B., Kovack-Lesh, K. A., Goodwin, D., \& McEchron, W. (2013).
Infant directed speech and the development of speech perception:
Enhancing development or an unintended consequence? \emph{Cognition},
\emph{129}(2), 362--378.

\hypertarget{ref-meltzoff2005}{}
Meltzoff, A. N. (2005). Imitation and other minds: The ``like me''
hypothesis. \emph{Perspectives on Imitation: From Neuroscience to Social
Science}, \emph{2}, 55--77.

\hypertarget{ref-newport1990}{}
Newport, E. L. (1990). Maturational Constraints on Language Learning.
\emph{Cognitive Science}, \emph{14}(1), 11--28.

\hypertarget{ref-newport1977}{}
Newport, E. L., Gleitman, H., \& Gleitman, L. R. (1977). Mother, I'd
rather do it myself: Some effects and non-effects of maternal speech
style. In C. A. Ferguson (Ed.), \emph{Talking to children language input
and interaction} (pp. 109--149). Cambridge University Press.

\hypertarget{ref-saffran2003}{}
Saffran, J. R. (2003). Statistical language learning: Mechanisms and
constraints. \emph{Current Directions in Psychological Science},
\emph{12}(4), 110--114.

\hypertarget{ref-smith2008}{}
Smith, L. B., \& Yu, C. (2008). Infants rapidly learn word-referent
mappings via cross-situational statistics. \emph{Cognition}, \emph{106},
1558--1568.

\hypertarget{ref-smith2013}{}
Smith, L. B., \& Yu, C. (2013). Visual attention is not enough:
Individual differences in statistical word-referent learning in infants.
\emph{Language Learning and Development}, \emph{9}, 25--49.

\hypertarget{ref-vlach2013}{}
Vlach, H. A., \& Johnson, S. P. (2013). Memory constraints on infants
cross-situational statistical learning. \emph{Cognition}, \emph{127}(3),
375--382.

\hypertarget{ref-vogt2012}{}
Vogt, P. (2012). Exploring the robustness of cross-situational learning
under zipfian distributions. \emph{Cognitive Science}, \emph{36}(4),
726--739.

\hypertarget{ref-yurovsky2017}{}
Yurovsky, D. (2017). A communicative approach to early word learning.
\emph{New Ideas in Psychology}, 1--7.

\hypertarget{ref-yurovsky2016}{}
Yurovsky, D., Doyle, G., \& Frank, M. C. (2016). Linguistic input is
tuned to childrens developmental level. In \emph{Proceedings of the
annual meeting of the cognitive science society} (pp. 2093--2098).

\bibliographystyle{apacite}


\end{document}
